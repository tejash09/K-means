{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPk1IHvvIdxuiGJjbzsP0rU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejash09/K-means/blob/main/bottom_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 1st try\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "y2PhSg_57tw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTJiPzx77qYK",
        "outputId": "3ad8a3e3-b57c-4628-c8bc-8ca4a977dc6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 6.824 seconds\n",
            "Average execution time (OptimizedKMeans): 0.027 seconds\n",
            "Average execution time (SklearnKMeans): 0.037 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 4.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "@njit(parallel=True)\n",
        "def _fast_distances(X, centroids):\n",
        "    \"\"\"Compute squared Euclidean distances with Numba - highly optimized version.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids (constant for all data points)\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Pre-compute squared norms of X\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float64)\n",
        "    for i in range(n_samples):\n",
        "        for k in range(n_features):\n",
        "            x_norms[i] += X[i, k] * X[i, k]\n",
        "\n",
        "    # Use ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩ identity for faster computation\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit\n",
        "def _assign_labels_numba(X, centroids):\n",
        "    \"\"\"Assign labels to data points based on nearest centroid.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    labels = np.zeros(n_samples, dtype=np.int32)\n",
        "    min_distances = np.full(n_samples, np.inf)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            dist = 0.0\n",
        "            for k in range(X.shape[1]):\n",
        "                diff = X[i, k] - centroids[j, k]\n",
        "                dist += diff * diff\n",
        "            if dist < min_distances[i]:\n",
        "                min_distances[i] = dist\n",
        "                labels[i] = j\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "@njit\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels.\"\"\"\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Sum points in each cluster\n",
        "    for i in range(X.shape[0]):\n",
        "        cluster_id = labels[i]\n",
        "        counts[cluster_id] += 1\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Divide by counts to get means\n",
        "    for i in range(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "# =============================================================================\n",
        "# Optimized Bottom-Up KMeans Implementation\n",
        "# =============================================================================\n",
        "class BottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation with a bottom-up approach, gradually adding points\n",
        "    to the calculation until convergence.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor=0.1,\n",
        "                 batch_growth_factor=15, verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor  # For early convergence detection\n",
        "        self.n_init = n_init  # Run algorithm multiple times and select best\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Enhanced centroid initialization with multiple methods.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Stratified random selection for better coverage\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Divide the data range into strata for more uniform coverage\n",
        "            sample_indices = []\n",
        "            if n_samples >= self.n_clusters * 10:  # If we have enough samples\n",
        "                # Use approximate quantiles to divide data\n",
        "                from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "                # Use feature with highest variance for stratification\n",
        "                feature_var = np.var(X, axis=0)\n",
        "                stratify_feature = np.argmax(feature_var)\n",
        "\n",
        "                discretizer = KBinsDiscretizer(n_bins=min(self.n_clusters, 20),\n",
        "                                              encode='ordinal', strategy='quantile')\n",
        "                strata = discretizer.fit_transform(X[:, stratify_feature].reshape(-1, 1)).astype(int).flatten()\n",
        "                unique_strata = np.unique(strata)\n",
        "\n",
        "                # Select points from different strata\n",
        "                for stratum in unique_strata:\n",
        "                    stratum_indices = np.where(strata == stratum)[0]\n",
        "                    if len(stratum_indices) > 0:\n",
        "                        idx = random_state.choice(stratum_indices,\n",
        "                                                 size=min(max(1, self.n_clusters // len(unique_strata)),\n",
        "                                                         len(stratum_indices)),\n",
        "                                                 replace=False)\n",
        "                        sample_indices.extend(idx)\n",
        "\n",
        "            # If we couldn't get enough from stratification, add random ones\n",
        "            if len(sample_indices) < self.n_clusters:\n",
        "                remaining = self.n_clusters - len(sample_indices)\n",
        "                avail_indices = list(set(range(n_samples)) - set(sample_indices))\n",
        "                if avail_indices:\n",
        "                    additional = random_state.choice(avail_indices,\n",
        "                                                   size=min(remaining, len(avail_indices)),\n",
        "                                                   replace=False)\n",
        "                    sample_indices.extend(additional)\n",
        "\n",
        "            # If we still don't have enough, allow repeats\n",
        "            if len(sample_indices) < self.n_clusters:\n",
        "                remaining = self.n_clusters - len(sample_indices)\n",
        "                additional = random_state.choice(n_samples, size=remaining, replace=True)\n",
        "                sample_indices.extend(additional)\n",
        "\n",
        "            # Trim to exactly k clusters\n",
        "            sample_indices = sample_indices[:self.n_clusters]\n",
        "            return X[sample_indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ with vectorization\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # Use vectorized operations for faster distance calculations\n",
        "            for c in range(1, self.n_clusters):\n",
        "                # Calculate squared distances to closest centroid\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    closest_dist_sq = np.min(np.sum((X[:, np.newaxis, :] -\n",
        "                                              centroids[np.newaxis, :c, :])**2, axis=2), axis=1)\n",
        "\n",
        "                # Select next centroid with probability proportional to square distance\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    next_centroid_idx = random_state.choice(n_samples, p=probs)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "\n",
        "        elif self.init == 'k-means++-fast':\n",
        "            # Faster approximate k-means++ using subsampling for large datasets\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Subsample for very large datasets\n",
        "            subsample_limit = 10000\n",
        "            if n_samples > subsample_limit:\n",
        "                subsample_indices = random_state.choice(n_samples,\n",
        "                                                     size=subsample_limit,\n",
        "                                                     replace=False)\n",
        "                X_subset = X[subsample_indices]\n",
        "            else:\n",
        "                X_subset = X\n",
        "                subsample_indices = np.arange(n_samples)\n",
        "\n",
        "            # Choose first centroid randomly from subset\n",
        "            first_idx = random_state.randint(len(X_subset))\n",
        "            centroids[0] = X_subset[first_idx].copy()\n",
        "\n",
        "            # Initialize distances array once\n",
        "            subset_size = len(X_subset)\n",
        "            closest_dist_sq = np.zeros(subset_size)\n",
        "\n",
        "            for c in range(1, self.n_clusters):\n",
        "                # Update distances for new centroid\n",
        "                for i in range(subset_size):\n",
        "                    dist = np.sum((X_subset[i] - centroids[c-1])**2)\n",
        "                    if c == 1 or dist < closest_dist_sq[i]:\n",
        "                        closest_dist_sq[i] = dist\n",
        "\n",
        "                # Select next centroid\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    subset_idx = random_state.choice(subset_size, p=probs)\n",
        "                    centroids[c] = X_subset[subset_idx].copy()\n",
        "                else:\n",
        "                    # If all points are identical to centroids, pick randomly\n",
        "                    subset_idx = random_state.randint(subset_size)\n",
        "                    centroids[c] = X_subset[subset_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        \"\"\"Compute distances with vectorization and Numba.\"\"\"\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            # Vectorized implementation for non-Numba case\n",
        "            # Reshape to allow broadcasting\n",
        "            expanded_X = X[:, np.newaxis, :]\n",
        "            expanded_centroids = centroids[np.newaxis, :, :]\n",
        "\n",
        "            # Calculate squared distances\n",
        "            squared_diff = (expanded_X - expanded_centroids) ** 2\n",
        "            squared_distances = np.sum(squared_diff, axis=2)\n",
        "\n",
        "            return squared_distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, iteration):\n",
        "        \"\"\"\n",
        "        Enhanced batch selection strategy with adaptive sampling.\n",
        "        Selects points that are likely to improve centroid positions the most.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        inactive_mask = np.ones(n_samples, dtype=bool)\n",
        "        inactive_mask[current_active] = False\n",
        "        inactive_indices = np.where(inactive_mask)[0]\n",
        "\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([])\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        labels = np.argmin(inactive_distances, axis=1)\n",
        "\n",
        "        # Calculate multiple selection criteria\n",
        "        # 1. Uncertainty: difference between closest and second closest centroid\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            uncertainty = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            uncertainty = uncertainty / (np.max(uncertainty) + 1e-10)\n",
        "        else:\n",
        "            uncertainty = np.zeros(len(inactive_indices))\n",
        "\n",
        "        # 2. Representativeness: distance to closest centroid\n",
        "        closest_distances = np.min(inactive_distances, axis=1)\n",
        "        if np.max(closest_distances) > np.min(closest_distances):\n",
        "            representativeness = (closest_distances - np.min(closest_distances)) / (np.max(closest_distances) - np.min(closest_distances) + 1e-10)\n",
        "        else:\n",
        "            representativeness = np.zeros_like(closest_distances)\n",
        "\n",
        "        # 3. Cluster balance: prioritize points from underrepresented clusters\n",
        "        cluster_counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        balance_score = np.zeros(len(inactive_indices))\n",
        "        for i, label in enumerate(labels):\n",
        "            if cluster_counts[label] > 0:\n",
        "                balance_score[i] = 1.0 / cluster_counts[label]\n",
        "        balance_score = balance_score / (np.max(balance_score) + 1e-10)\n",
        "\n",
        "        # Adaptive weighting of criteria based on iteration\n",
        "        # Early: focus on representativeness (exploration)\n",
        "        # Late: focus on uncertainty (refinement)\n",
        "        exploration_weight = max(0, 1 - iteration / self.max_iterations)\n",
        "        refinement_weight = 1 - exploration_weight\n",
        "\n",
        "        # Combine criteria with adaptive weights\n",
        "        combined_score = (\n",
        "            exploration_weight * representativeness +\n",
        "            refinement_weight * uncertainty +\n",
        "            0.2 * balance_score  # Fixed weight for balance\n",
        "        )\n",
        "\n",
        "        # Select best points according to combined score\n",
        "        n_to_select = min(batch_size, len(inactive_indices))\n",
        "        selected_idx = np.argsort(-combined_score)[:n_to_select]  # Descending order\n",
        "\n",
        "        return inactive_indices[selected_idx]\n",
        "\n",
        "    def _run_single_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run a single instance of the bottom-up K-means algorithm.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        inertia = 0.0\n",
        "        n_iter = 0\n",
        "        iteration_table = []\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize batch size and active set\n",
        "        initial_batch_size = max(int(n_samples * self.batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        # Initial active set with points closest to centroids and some distant points\n",
        "        min_distances = np.min(distances, axis=1)\n",
        "        closest_points = np.argsort(min_distances)[:initial_batch_size // 2]\n",
        "        farthest_points = np.argsort(min_distances)[-initial_batch_size // 2:]\n",
        "        active_indices = np.concatenate([closest_points, farthest_points])\n",
        "        active_indices = np.unique(active_indices)  # Remove any duplicates\n",
        "\n",
        "        # For tracking stability across iterations\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            if len(active_indices) > 0:\n",
        "                active_X = X[active_indices]\n",
        "                active_labels = labels[active_indices]\n",
        "\n",
        "                # Update centroids based on active points\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    new_centroids, counts = _update_centroids_numba(active_X, active_labels, self.n_clusters)\n",
        "                    # Handle empty clusters\n",
        "                    for k in range(self.n_clusters):\n",
        "                        if counts[k] == 0:\n",
        "                            # Find the point furthest from its centroid\n",
        "                            active_distances = distances[active_indices]\n",
        "                            furthest_point_idx = np.argmax(np.min(active_distances, axis=1))\n",
        "                            new_centroids[k] = active_X[furthest_point_idx]\n",
        "                else:\n",
        "                    new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                    for k in range(self.n_clusters):\n",
        "                        cluster_mask = (active_labels == k)\n",
        "                        if np.any(cluster_mask):\n",
        "                            new_centroids[k] = np.mean(active_X[cluster_mask], axis=0)\n",
        "                        else:\n",
        "                            # For empty clusters, use old centroid or find a new representative\n",
        "                            if iteration > 0:\n",
        "                                new_centroids[k] = old_centroids[k]\n",
        "                            else:\n",
        "                                # First iteration, just pick a random point\n",
        "                                random_idx = random_state.randint(len(active_X))\n",
        "                                new_centroids[k] = active_X[random_idx]\n",
        "\n",
        "                centroids = new_centroids\n",
        "\n",
        "            # Compute distances and reassign labels\n",
        "            distances = self._compute_distances(X, centroids)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "            # Calculate change in active points\n",
        "            if len(active_indices) > 0:\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "            else:\n",
        "                active_changed = 0\n",
        "                active_changed_pct = 0\n",
        "\n",
        "            labels = new_labels\n",
        "\n",
        "            # Calculate inertia (sum of squared distances to closest centroid)\n",
        "            min_distances = np.min(distances, axis=1)\n",
        "\n",
        "            # Calculate inertia for active points only to monitor convergence\n",
        "            active_inertia = np.sum(min_distances[active_indices]) if len(active_indices) > 0 else 0\n",
        "\n",
        "            # Adaptive batch size growth based on convergence behavior\n",
        "            convergence_factor = 1.0\n",
        "            if iteration > 0 and prev_inertia > 0:\n",
        "                inertia_change = abs(active_inertia - prev_inertia) / prev_inertia\n",
        "                # If inertia change is small, we're converging, so grow batch size faster\n",
        "                if inertia_change < 0.01:\n",
        "                    convergence_factor = 1.5  # Accelerate batch growth\n",
        "                elif inertia_change > 0.1:\n",
        "                    convergence_factor = 0.8  # Slow down batch growth\n",
        "\n",
        "            # Update batch size for next iteration\n",
        "            next_batch_base_size = int(initial_batch_size * (self.batch_growth_factor ** iteration) * convergence_factor)\n",
        "            next_batch_size = min(\n",
        "                next_batch_base_size,\n",
        "                n_samples - len(active_indices)\n",
        "            )\n",
        "\n",
        "            # Select next batch of points to add\n",
        "            new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, iteration)\n",
        "\n",
        "            # Record iteration information\n",
        "            iteration_info = {\n",
        "                'iteration': n_iter,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'active_points_changed': active_changed,\n",
        "                'active_points_changed_pct': active_changed_pct * 100,\n",
        "                'new_points_added': len(new_batch),\n",
        "                'active_inertia': active_inertia,\n",
        "                'total_coverage': len(active_indices) / n_samples * 100 if n_samples > 0 else 0\n",
        "            }\n",
        "            iteration_table.append(iteration_info)\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 5 == 0:\n",
        "                logger.info(f\"Iteration {n_iter}: \"\n",
        "                           f\"{active_changed} active points changed ({active_changed_pct:.2%}), \"\n",
        "                           f\"{len(new_batch)} new points added, \"\n",
        "                           f\"{len(active_indices) / n_samples * 100:.1f}% coverage\")\n",
        "\n",
        "            # Add new batch to active set\n",
        "            if len(new_batch) > 0:\n",
        "                active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "            # Calculate centroid shift for convergence check\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - centroids)**2, axis=1)))\n",
        "\n",
        "            # Early stopping conditions\n",
        "\n",
        "            # 1. Centroid stability\n",
        "            if centroid_shift < self.tolerance:\n",
        "                if len(active_indices) == n_samples:\n",
        "                    # Full dataset and centroids stable = converged\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Converged at iteration {n_iter}: centroids stable\")\n",
        "                    break\n",
        "                # If not full dataset but centroids stable, add more points faster\n",
        "                next_batch_size = min(next_batch_size * 2, n_samples - len(active_indices))\n",
        "\n",
        "            # 2. Inertia stability (track consecutive stable iterations)\n",
        "            if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                stability_counter += 1\n",
        "            else:\n",
        "                stability_counter = 0\n",
        "\n",
        "            # If inertia stable for multiple iterations and we have enough points\n",
        "            if stability_counter >= 3 and len(active_indices) / n_samples > 0.5:\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"Early stopping at iteration {n_iter}: inertia stable\")\n",
        "                break\n",
        "\n",
        "            # 3. All points active and no label changes\n",
        "            if len(active_indices) == n_samples and active_changed == 0:\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"Converged at iteration {n_iter}: all points stable\")\n",
        "                break\n",
        "\n",
        "            # 4. No new points to add and centroids stable\n",
        "            if len(new_batch) == 0 and centroid_shift < self.tolerance:\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"Converged at iteration {n_iter}: no new points and centroids stable\")\n",
        "                break\n",
        "\n",
        "            prev_inertia = active_inertia\n",
        "\n",
        "        # Final update with all points\n",
        "        if NUMBA_AVAILABLE:\n",
        "            centroids, _ = _update_centroids_numba(X, labels, self.n_clusters)\n",
        "        else:\n",
        "            for k in range(self.n_clusters):\n",
        "                cluster_mask = (labels == k)\n",
        "                if np.any(cluster_mask):\n",
        "                    centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "\n",
        "        # Final assignment\n",
        "        distances = self._compute_distances(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the model to data.\n",
        "        Run multiple initializations and select the best result.\n",
        "        \"\"\"\n",
        "        X = check_array(X)\n",
        "\n",
        "        best_inertia = np.inf\n",
        "        best_centroids = None\n",
        "        best_labels = None\n",
        "        best_n_iter = 0\n",
        "\n",
        "        seeds = self._get_seeds()\n",
        "\n",
        "        for seed_idx, seed in enumerate(seeds):\n",
        "            if self.verbose and len(seeds) > 1:\n",
        "                logger.info(f\"K-means initialization {seed_idx + 1}/{len(seeds)}\")\n",
        "\n",
        "            centroids, labels, inertia, n_iter, iter_table = self._run_single_kmeans(X, seed)\n",
        "\n",
        "            if inertia < best_inertia:\n",
        "                best_centroids = centroids.copy()\n",
        "                best_labels = labels.copy()\n",
        "                best_inertia = inertia\n",
        "                best_n_iter = n_iter\n",
        "                self.iteration_table_ = iter_table\n",
        "\n",
        "        self.cluster_centers_ = best_centroids\n",
        "        self.labels_ = best_labels\n",
        "        self.inertia_ = best_inertia\n",
        "        self.n_iter_ = best_n_iter\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"BottomUpKMeans converged after {self.n_iter_} iterations. \"\n",
        "                        f\"Inertia: {self.inertia_:.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _get_seeds(self):\n",
        "        \"\"\"Generate random seeds for multiple initializations.\"\"\"\n",
        "        random_state = check_random_state(self.random_state)\n",
        "        seeds = []\n",
        "        for i in range(self.n_init):\n",
        "            seed = random_state.randint(0, 2**31 - 1)\n",
        "            seeds.append(seed)\n",
        "        return seeds\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster for each sample in X.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X)\n",
        "\n",
        "        if NUMBA_AVAILABLE:\n",
        "            labels, _ = _assign_labels_numba(X, self.cluster_centers_)\n",
        "            return labels\n",
        "        else:\n",
        "            distances = self._compute_distances(X, self.cluster_centers_)\n",
        "            return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Prints a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            return df\n",
        "        except ImportError:\n",
        "            for info in self.iteration_table_:\n",
        "                print(\", \".join([f\"{k}: {v}\" for k, v in info.items()]))\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = BottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OrACuhtxKgsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iKxec4LR7sbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _fast_distances(X, centroids):\n",
        "    \"\"\"Ultra-optimized squared Euclidean distances with Numba.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Pre-compute squared norms of X\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float64)\n",
        "    for i in prange(n_samples):\n",
        "        for k in range(n_features):\n",
        "            x_norms[i] += X[i, k] * X[i, k]\n",
        "\n",
        "    # Use ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩ identity for faster computation\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(fastmath=True)\n",
        "def _fast_distances_block(X, centroids, start_idx, end_idx):\n",
        "    \"\"\"Compute distances for a block of samples.\"\"\"\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    block_size = end_idx - start_idx\n",
        "    distances = np.empty((block_size, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Process block\n",
        "    for i in range(block_size):\n",
        "        x_idx = start_idx + i\n",
        "        # Compute x_norm for this sample\n",
        "        x_norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            x_norm += X[x_idx, k] * X[x_idx, k]\n",
        "\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norm + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[x_idx, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels - fully optimized.\"\"\"\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count points in clusters - separate loop for better cache performance\n",
        "    for i in range(X.shape[0]):\n",
        "        counts[labels[i]] += 1\n",
        "\n",
        "    # Sum points in each cluster - vectorized across features for each sample\n",
        "    for i in range(X.shape[0]):\n",
        "        cluster_id = labels[i]\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Divide by counts to get means\n",
        "    for i in range(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXoqau7lFV6K",
        "outputId": "85f0abc8-72c3-48e3-ac46-6ec27ce7337e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 1.224 seconds\n",
            "Average execution time (OptimizedKMeans): 0.022 seconds\n",
            "Average execution time (SklearnKMeans): 0.020 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install line_profiler\n",
        "%load_ext line_profiler\n",
        "%lprun -f run_bench_evaluation run_bench_evaluation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6T8Xm8kz6Od",
        "outputId": "ec348ac2-6938-4a2b-cbc6-58657a1b96c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: line_profiler in /usr/local/lib/python3.11/dist-packages (4.2.0)\n",
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 0.462 seconds\n",
            "Average execution time (OptimizedKMeans): 0.027 seconds\n",
            "Average execution time (SklearnKMeans): 0.022 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "n_clusters = 3\n",
        "n_runs = 3\n",
        "bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42, verbose=True)\n",
        "#bu_kmeans.fit(X)\n",
        "%lprun -f HybridBottomUpKMeans.fit bu_kmeans.fit(X)\n"
      ],
      "metadata": {
        "id": "3T1TG2qN4cBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cProfile, pstats\n",
        "\n",
        "# Profile your function\n",
        "cProfile.run('run_bench_evaluation()', 'profile_output')\n",
        "\n",
        "# Load the profiling data\n",
        "p = pstats.Stats('profile_output')\n",
        "p.strip_dirs().sort_stats('cumulative').print_stats(15)  # Top 10 functions by cumulative time\n",
        "\n",
        "# For a recursive breakdown, check the callers and callees of a specific function:\n",
        "p.print_callers('run_bench_evaluation')\n",
        "p.print_callees('run_bench_evaluation')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ84GT3lL-K2",
        "outputId": "866d8916-08ec-4bf7-cfbe-da03ace4bf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 0.463 seconds\n",
            "Average execution time (OptimizedKMeans): 0.028 seconds\n",
            "Average execution time (SklearnKMeans): 0.021 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n",
            "Fri Mar 14 10:27:37 2025    profile_output\n",
            "\n",
            "         678370 function calls (677501 primitive calls) in 118.214 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 650 to 10 due to restriction <10>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000  118.215  118.215 {built-in method builtins.exec}\n",
            "        1    0.000    0.000  118.214  118.214 <string>:1(<module>)\n",
            "        1    0.001    0.001  118.214  118.214 <ipython-input-3-d31acea3e1da>:1170(run_bench_evaluation)\n",
            "   360/63    0.004    0.000  116.702    1.852 _param_validation.py:185(wrapper)\n",
            "        9    0.000    0.000  116.142   12.905 _unsupervised.py:42(silhouette_score)\n",
            "        9    0.005    0.001  116.141   12.905 _unsupervised.py:196(silhouette_samples)\n",
            "       72    0.196    0.003  116.114    1.613 pairwise.py:2082(pairwise_distances_chunked)\n",
            "      117    0.001    0.000   71.929    0.615 pairwise.py:2266(pairwise_distances)\n",
            "      117    0.001    0.000   71.928    0.615 pairwise.py:1964(_parallel_pairwise)\n",
            "      117    0.002    0.000   71.919    0.615 pairwise.py:273(euclidean_distances)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pstats.Stats at 0x7d3a85252690>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p.strip_dirs().sort_stats('cumulative').print_stats(100)\n",
        "p.print_callers('run_bench_evaluation')\n",
        "p.print_callees('run_bench_evaluation')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXoLJegONAPz",
        "outputId": "9f79fb68-c6d6-4b0f-87a2-39bb4461f182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 14 10:27:37 2025    profile_output\n",
            "\n",
            "         678370 function calls (677501 primitive calls) in 118.214 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 650 to 100 due to restriction <100>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000  118.215  118.215 {built-in method builtins.exec}\n",
            "        1    0.000    0.000  118.214  118.214 <string>:1(<module>)\n",
            "        1    0.001    0.001  118.214  118.214 <ipython-input-3-d31acea3e1da>:1170(run_bench_evaluation)\n",
            "   360/63    0.004    0.000  116.702    1.852 _param_validation.py:185(wrapper)\n",
            "        9    0.000    0.000  116.142   12.905 _unsupervised.py:42(silhouette_score)\n",
            "        9    0.005    0.001  116.141   12.905 _unsupervised.py:196(silhouette_samples)\n",
            "       72    0.196    0.003  116.114    1.613 pairwise.py:2082(pairwise_distances_chunked)\n",
            "      117    0.001    0.000   71.929    0.615 pairwise.py:2266(pairwise_distances)\n",
            "      117    0.001    0.000   71.928    0.615 pairwise.py:1964(_parallel_pairwise)\n",
            "      117    0.002    0.000   71.919    0.615 pairwise.py:273(euclidean_distances)\n",
            "      126   24.443    0.194   71.846    0.570 pairwise.py:391(_euclidean_distances)\n",
            "       63   43.930    0.697   44.068    0.699 _unsupervised.py:142(_silhouette_reduce)\n",
            "      126   27.135    0.215   27.137    0.215 extmath.py:153(safe_sparse_dot)\n",
            "      225   20.212    0.090   20.213    0.090 _array_api.py:1080(_modify_in_place_if_numpy)\n",
            "        3    0.003    0.001    1.389    0.463 <ipython-input-3-d31acea3e1da>:784(fit)\n",
            "        9    0.068    0.008    1.379    0.153 <ipython-input-3-d31acea3e1da>:546(_run_kmeans)\n",
            "       27    0.338    0.013    0.754    0.028 <ipython-input-3-d31acea3e1da>:455(_select_next_batch)\n",
            "      478    0.001    0.000    0.343    0.001 fromnumeric.py:53(_wrapfunc)\n",
            "      236    0.332    0.001    0.332    0.001 {method 'argsort' of 'numpy.ndarray' objects}\n",
            "      173    0.001    0.000    0.287    0.002 fromnumeric.py:1025(argsort)\n",
            "     1676    0.286    0.000    0.286    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
            "        9    0.142    0.016    0.277    0.031 _supervised.py:926(adjusted_mutual_info_score)\n",
            "      958    0.005    0.000    0.221    0.000 fromnumeric.py:71(_wrapreduction)\n",
            "      104    0.000    0.000    0.173    0.002 fromnumeric.py:2836(min)\n",
            "       30    0.040    0.001    0.164    0.005 <ipython-input-3-d31acea3e1da>:522(_standard_kmeans_iteration)\n",
            "      180    0.001    0.000    0.146    0.001 arraysetops.py:138(unique)\n",
            "      180    0.022    0.000    0.144    0.001 arraysetops.py:323(_unique1d)\n",
            "      432    0.012    0.000    0.102    0.000 validation.py:736(check_array)\n",
            "       79    0.098    0.001    0.098    0.001 <ipython-input-3-d31acea3e1da>:83(_fast_distances)\n",
            "      153    0.094    0.001    0.094    0.001 {method 'sort' of 'numpy.ndarray' objects}\n",
            "       18    0.001    0.000    0.093    0.005 _supervised.py:87(contingency_matrix)\n",
            "       69    0.086    0.001    0.089    0.001 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
            "        9    0.002    0.000    0.087    0.010 _unsupervised.py:390(davies_bouldin_score)\n",
            "        9    0.008    0.001    0.086    0.010 <ipython-input-3-d31acea3e1da>:346(_initialize_centroids)\n",
            "        3    0.022    0.007    0.084    0.028 <ipython-input-3-d31acea3e1da>:934(fit)\n",
            "        9    0.000    0.000    0.082    0.009 _supervised.py:341(adjusted_rand_score)\n",
            "        9    0.000    0.000    0.081    0.009 _supervised.py:176(pair_confusion_matrix)\n",
            "       66    0.001    0.000    0.079    0.001 <ipython-input-3-d31acea3e1da>:412(_compute_distances_parallel)\n",
            "      117    0.002    0.000    0.077    0.001 pairwise.py:75(check_pairwise_arrays)\n",
            "        3    0.000    0.000    0.063    0.021 base.py:1372(wrapper)\n",
            "        3    0.015    0.005    0.061    0.020 _kmeans.py:1426(fit)\n",
            "   270019    0.061    0.000    0.061    0.000 multiarray.py:892(bincount)\n",
            "273879/273865    0.060    0.000    0.060    0.000 {built-in method builtins.len}\n",
            "      237    0.005    0.000    0.055    0.000 _methods.py:101(_mean)\n",
            "      333    0.008    0.000    0.054    0.000 validation.py:90(_assert_all_finite)\n",
            "       82    0.001    0.000    0.048    0.001 fromnumeric.py:1236(argmin)\n",
            "       82    0.047    0.001    0.047    0.001 {method 'argmin' of 'numpy.ndarray' objects}\n",
            "      593    0.003    0.000    0.046    0.000 fromnumeric.py:2177(sum)\n",
            "      246    0.003    0.000    0.045    0.000 extmath.py:47(row_norms)\n",
            "        9    0.014    0.002    0.044    0.005 _unsupervised.py:319(calinski_harabasz_score)\n",
            "      180    0.001    0.000    0.043    0.000 fromnumeric.py:3385(mean)\n",
            "       36    0.000    0.000    0.043    0.001 fromnumeric.py:865(sort)\n",
            "       18    0.000    0.000    0.042    0.002 _supervised.py:25(check_clusterings)\n",
            "      246    0.000    0.000    0.039    0.000 einsumfunc.py:1009(einsum)\n",
            "      246    0.038    0.000    0.038    0.000 {built-in method numpy.core._multiarray_umath.c_einsum}\n",
            "       36    0.001    0.000    0.035    0.001 multiclass.py:229(type_of_target)\n",
            "      297    0.034    0.000    0.034    0.000 {built-in method numpy.array}\n",
            "       27    0.033    0.001    0.033    0.001 <ipython-input-3-d31acea3e1da>:149(_update_centroids_numba)\n",
            "       18    0.000    0.000    0.033    0.002 _coo.py:318(tocsr)\n",
            "      198    0.029    0.000    0.029    0.000 {method 'extend' of 'list' objects}\n",
            "       27    0.000    0.000    0.028    0.001 _label.py:97(fit_transform)\n",
            "       36    0.000    0.000    0.027    0.001 _unique.py:81(cached_unique)\n",
            "       72    0.000    0.000    0.027    0.000 _unique.py:105(<genexpr>)\n",
            "       36    0.000    0.000    0.027    0.001 _unique.py:62(_cached_unique)\n",
            "       36    0.000    0.000    0.026    0.001 _array_api.py:415(unique_values)\n",
            "       10    0.017    0.002    0.026    0.003 <ipython-input-3-d31acea3e1da>:923(_update_centroids)\n",
            "       27    0.000    0.000    0.025    0.001 _encode.py:20(_unique)\n",
            "       27    0.000    0.000    0.025    0.001 _encode.py:61(_unique_np)\n",
            "       36    0.000    0.000    0.025    0.001 _compressed.py:1239(sum_duplicates)\n",
            "       27    0.000    0.000    0.024    0.001 _array_api.py:409(unique_inverse)\n",
            "      189    0.001    0.000    0.024    0.000 {method 'min' of 'numpy.ndarray' objects}\n",
            "      189    0.000    0.000    0.023    0.000 _methods.py:43(_amin)\n",
            "     1129    0.002    0.000    0.023    0.000 {built-in method builtins.any}\n",
            "       18    0.000    0.000    0.023    0.001 _compressed.py:1286(sort_indices)\n",
            "       18    0.022    0.001    0.022    0.001 {built-in method scipy.sparse._sparsetools.csr_sort_indices}\n",
            "       13    0.000    0.000    0.021    0.002 <ipython-input-3-d31acea3e1da>:914(_compute_distances)\n",
            "      594    0.001    0.000    0.021    0.000 _array_api.py:441(isdtype)\n",
            "      594    0.002    0.000    0.020    0.000 _array_api.py:225(isdtype)\n",
            "       63    0.001    0.000    0.018    0.000 _compressed.py:622(sum)\n",
            "        9    0.001    0.000    0.018    0.002 _supervised.py:810(mutual_info_score)\n",
            "       27    0.000    0.000    0.018    0.001 validation.py:1203(check_X_y)\n",
            "       18    0.001    0.000    0.017    0.001 _supervised.py:1260(entropy)\n",
            " 1116/666    0.007    0.000    0.017    0.000 _array_api.py:237(_isdtype_single)\n",
            "      570    0.007    0.000    0.017    0.000 validation.py:381(_num_samples)\n",
            "        1    0.001    0.001    0.016    0.016 <ipython-input-3-d31acea3e1da>:1033(load_synthetic_data)\n",
            "        3    0.000    0.000    0.016    0.005 parallel.py:161(wrapper)\n",
            "       33    0.007    0.000    0.015    0.000 validation.py:1490(check_random_state)\n",
            "        3    0.014    0.005    0.015    0.005 _kmeans.py:623(_kmeans_single_lloyd)\n",
            "        3    0.015    0.005    0.015    0.005 {method 'normal' of 'numpy.random.mtrand.RandomState' objects}\n",
            "       18    0.000    0.000    0.015    0.001 _array_api.py:412(unique_counts)\n",
            "       36    0.001    0.000    0.014    0.000 _base.py:1037(sum)\n",
            "    36/27    0.001    0.000    0.014    0.001 _coo.py:27(__init__)\n",
            "        9    0.000    0.000    0.014    0.002 <ipython-input-3-d31acea3e1da>:290(_set_dynamic_parameters)\n",
            "        3    0.000    0.000    0.013    0.004 _kmeans.py:955(_init_centroids)\n",
            "        3    0.002    0.001    0.013    0.004 _kmeans.py:174(_kmeans_plusplus)\n",
            "      135    0.001    0.000    0.013    0.000 _base.py:1303(_get_index_dtype)\n",
            "       57    0.000    0.000    0.012    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
            "       27    0.000    0.000    0.012    0.000 _indexing.py:179(_safe_indexing)\n",
            "    15562    0.007    0.000    0.012    0.000 {built-in method builtins.isinstance}\n",
            "     6192    0.009    0.000    0.012    0.000 _array_api.py:381(__getattr__)\n",
            "\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 650 to 1 due to restriction <'run_bench_evaluation'>\n",
            "\n",
            "Function                                                   was called by...\n",
            "                                                               ncalls  tottime  cumtime\n",
            "<ipython-input-3-d31acea3e1da>:1170(run_bench_evaluation)  <-       1    0.001  118.214  <string>:1(<module>)\n",
            "\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 650 to 1 due to restriction <'run_bench_evaluation'>\n",
            "\n",
            "Function                                                   called...\n",
            "                                                               ncalls  tottime  cumtime\n",
            "<ipython-input-3-d31acea3e1da>:1170(run_bench_evaluation)  ->       3    0.000    0.000  <ipython-input-3-d31acea3e1da>:216(__init__)\n",
            "                                                                    3    0.003    1.389  <ipython-input-3-d31acea3e1da>:784(fit)\n",
            "                                                                    1    0.000    0.008  <ipython-input-3-d31acea3e1da>:846(print_iteration_table)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:864(__init__)\n",
            "                                                                    3    0.022    0.084  <ipython-input-3-d31acea3e1da>:934(fit)\n",
            "                                                                    1    0.001    0.016  <ipython-input-3-d31acea3e1da>:1033(load_synthetic_data)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1261(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1262(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1263(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1264(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1271(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1272(<listcomp>)\n",
            "                                                                    3    0.000    0.000  _kmeans.py:1377(__init__)\n",
            "                                                                   45    0.001  116.647  _param_validation.py:185(wrapper)\n",
            "                                                                    3    0.000    0.063  base.py:1372(wrapper)\n",
            "                                                                   24    0.000    0.000  fromnumeric.py:3380(_mean_dispatcher)\n",
            "                                                                   24    0.000    0.001  fromnumeric.py:3385(mean)\n",
            "                                                                   37    0.000    0.006  {built-in method builtins.print}\n",
            "                                                                   18    0.000    0.000  {built-in method time.time}\n",
            "                                                                   27    0.000    0.000  {method 'append' of 'list' objects}\n",
            "                                                                    6    0.000    0.000  {method 'upper' of 'str' objects}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pstats.Stats at 0x7d3a85252690>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scalene"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwCOZ3BqO2t8",
        "outputId": "726595c0-13cd-47ba-da5a-af84a95e69cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scalene\n",
            "  Downloading scalene-1.5.51-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: wheel>=0.36.1 in /usr/local/lib/python3.11/dist-packages (from scalene) (0.45.1)\n",
            "Requirement already satisfied: rich>=10.7.0 in /usr/local/lib/python3.11/dist-packages (from scalene) (13.9.4)\n",
            "Requirement already satisfied: cloudpickle>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from scalene) (3.1.1)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.555.43 in /usr/local/lib/python3.11/dist-packages (from scalene) (12.570.86)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from scalene) (3.1.6)\n",
            "Requirement already satisfied: psutil>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from scalene) (5.9.5)\n",
            "Requirement already satisfied: numpy!=1.27,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from scalene) (1.26.4)\n",
            "Requirement already satisfied: pydantic>=2.6 in /usr/local/lib/python3.11/dist-packages (from scalene) (2.10.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.0.3->scalene) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.6->scalene) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.6->scalene) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.6->scalene) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.7.0->scalene) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.7.0->scalene) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.7.0->scalene) (0.1.2)\n",
            "Downloading scalene-1.5.51-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scalene\n",
            "Successfully installed scalene-1.5.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "from scalene import scalene_profiler\n",
        "\n",
        "# Turn profiling on\n",
        "scalene_profiler.start()\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _fast_distances(X, centroids):\n",
        "    \"\"\"Ultra-optimized squared Euclidean distances with Numba.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Pre-compute squared norms of X\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float64)\n",
        "    for i in prange(n_samples):\n",
        "        for k in range(n_features):\n",
        "            x_norms[i] += X[i, k] * X[i, k]\n",
        "\n",
        "    # Use ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩ identity for faster computation\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(fastmath=True)\n",
        "def _fast_distances_block(X, centroids, start_idx, end_idx):\n",
        "    \"\"\"Compute distances for a block of samples.\"\"\"\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    block_size = end_idx - start_idx\n",
        "    distances = np.empty((block_size, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Process block\n",
        "    for i in range(block_size):\n",
        "        x_idx = start_idx + i\n",
        "        # Compute x_norm for this sample\n",
        "        x_norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            x_norm += X[x_idx, k] * X[x_idx, k]\n",
        "\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norm + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[x_idx, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels - fully optimized.\"\"\"\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count points in clusters - separate loop for better cache performance\n",
        "    for i in range(X.shape[0]):\n",
        "        counts[labels[i]] += 1\n",
        "\n",
        "    # Sum points in each cluster - vectorized across features for each sample\n",
        "    for i in range(X.shape[0]):\n",
        "        cluster_id = labels[i]\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Divide by counts to get means\n",
        "    for i in range(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "y-y9u3SwOsHG",
        "outputId": "34fca688-b4cb-4ed3-dfb7-a48f8d3ba3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: Do not try to invoke `start` if you have not called Scalene using one of the methods\n",
            "in https://github.com/plasma-umass/scalene#using-scalene\n",
            "(The most likely issue is that you need to run your code with `scalene`, not `python`).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "1",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "import numpy as np\n",
        "from numba import njit, prange, float64, int32\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _fast_distances(X, centroids):\n",
        "    \"\"\"Ultra-optimized squared Euclidean distances with Numba.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.empty(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            norm += centroids[j, k] * centroids[j, k]\n",
        "        centroid_norms[j] = norm\n",
        "\n",
        "    # Pre-compute squared norms of X\n",
        "    x_norms = np.empty(n_samples, dtype=np.float64)\n",
        "    for i in prange(n_samples):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            norm += X[i, k] * X[i, k]\n",
        "        x_norms[i] = norm\n",
        "\n",
        "    # Compute distances using ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
        "    for i in prange(n_samples):\n",
        "        x_norm = x_norms[i]\n",
        "        for j in range(n_clusters):\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] = x_norm + centroid_norms[j] - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(fastmath=True, cache=True)\n",
        "def _fast_distances_block(X, centroids, start_idx, end_idx):\n",
        "    \"\"\"Compute distances for a block of samples.\"\"\"\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    block_size = end_idx - start_idx\n",
        "    distances = np.empty((block_size, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.empty(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            norm += centroids[j, k] * centroids[j, k]\n",
        "        centroid_norms[j] = norm\n",
        "\n",
        "    # Process block with improved locality\n",
        "    for i in range(block_size):\n",
        "        x_idx = start_idx + i\n",
        "\n",
        "        # Compute x_norm for this sample\n",
        "        x_norm = 0.0\n",
        "        x_values = np.empty(n_features, dtype=np.float64)\n",
        "\n",
        "        # Cache the values in x_values for better memory access\n",
        "        for k in range(n_features):\n",
        "            val = X[x_idx, k]\n",
        "            x_values[k] = val\n",
        "            x_norm += val * val\n",
        "\n",
        "        for j in range(n_clusters):\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += x_values[k] * centroids[j, k]\n",
        "            distances[i, j] = x_norm + centroid_norms[j] - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels - fully optimized.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count points in clusters - separate loop for better cache performance\n",
        "    for i in range(n_samples):\n",
        "        counts[labels[i]] += 1\n",
        "\n",
        "    # Sum points in each cluster - use prange for outer loop\n",
        "    # but handle accumulation carefully to avoid race conditions\n",
        "    for i in prange(n_samples):\n",
        "        cluster_id = labels[i]\n",
        "        for j in range(n_features):\n",
        "            # Use atomic add to avoid race conditions when updating centroids\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Divide by counts to get means\n",
        "    for i in range(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "        else:\n",
        "            # Handle empty clusters - set to a random point\n",
        "            idx = np.random.randint(0, n_samples)\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] = X[idx, j]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    #scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-aHuTz3KjmI",
        "outputId": "803a1b0c-7a85-4434-e009-136579e029e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "import numpy as np\n",
        "from numba import njit, prange, float64, int32\n",
        "from numba import njit, prange, cuda\n",
        "import numpy as np\n",
        "from numba import njit, prange\n",
        "import numpy as np\n",
        "\n",
        "# Check if CuPy is available\n",
        "try:\n",
        "    import cupy as cp\n",
        "    HAS_GPU = True\n",
        "except ImportError:\n",
        "    HAS_GPU = False\n",
        "\n",
        "# GPU version using CuPy\n",
        "if HAS_GPU:\n",
        "    def _fast_distances_gpu(X, centroids):\n",
        "        \"\"\"Compute squared Euclidean distances with CuPy GPU acceleration.\"\"\"\n",
        "        # Transfer data to GPU if needed\n",
        "        X_gpu = cp.asarray(X)\n",
        "        centroids_gpu = cp.asarray(centroids)\n",
        "\n",
        "        # Compute squared norms\n",
        "        X_norm = cp.sum(X_gpu**2, axis=1, keepdims=True)\n",
        "        centroids_norm = cp.sum(centroids_gpu**2, axis=1, keepdims=True).T\n",
        "\n",
        "        # Use matrix multiplication for dot product\n",
        "        dot_product = cp.dot(X_gpu, centroids_gpu.T)\n",
        "\n",
        "        # Compute distances using ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
        "        distances = X_norm + centroids_norm - 2.0 * dot_product\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def compute_distances(X, centroids):\n",
        "        \"\"\"Wrapper to handle GPU distance computation.\"\"\"\n",
        "        distances_gpu = _fast_distances_gpu(X, centroids)\n",
        "        return cp.asnumpy(distances_gpu)\n",
        "\n",
        "    def assign_labels(distances):\n",
        "        \"\"\"Assign labels based on distances using GPU.\"\"\"\n",
        "        distances_gpu = cp.asarray(distances)\n",
        "        min_distances = cp.min(distances_gpu, axis=1)\n",
        "        labels = cp.argmin(distances_gpu, axis=1)\n",
        "\n",
        "        return cp.asnumpy(labels).astype(np.int32), cp.asnumpy(min_distances)\n",
        "\n",
        "    def _update_centroids_gpu(X, labels, n_clusters):\n",
        "        \"\"\"Update centroids using GPU acceleration.\"\"\"\n",
        "        X_gpu = cp.asarray(X)\n",
        "        labels_gpu = cp.asarray(labels)\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        centroids = cp.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "        counts = cp.zeros(n_clusters, dtype=cp.int32)\n",
        "\n",
        "        # Process each cluster\n",
        "        for k in range(n_clusters):\n",
        "            mask = (labels_gpu == k)\n",
        "            cluster_points = X_gpu[mask]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[k] = cp.mean(cluster_points, axis=0)\n",
        "                counts[k] = len(cluster_points)\n",
        "\n",
        "        return cp.asnumpy(centroids), cp.asnumpy(counts)\n",
        "\n",
        "# CPU optimized version - unchanged from original\n",
        "@njit(parallel=True)\n",
        "def _fast_distances_cpu(X, centroids):\n",
        "    \"\"\"Compute squared Euclidean distances with Numba - vectorized version.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms - parallelized\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float64)\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "\n",
        "    for i in prange(n_samples):\n",
        "        for k in range(n_features):\n",
        "            x_norms[i] += X[i, k] * X[i, k]\n",
        "\n",
        "    for j in prange(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Compute distances in parallel\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j] - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True)\n",
        "def _assign_labels_cpu(distances):\n",
        "    \"\"\"Assign labels to data points based on nearest centroid.\"\"\"\n",
        "    n_samples = distances.shape[0]\n",
        "    labels = np.zeros(n_samples, dtype=np.int32)\n",
        "    min_distances = np.full(n_samples, np.inf)\n",
        "\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(distances.shape[1]):\n",
        "            if distances[i, j] < min_distances[i]:\n",
        "                min_distances[i] = distances[i, j]\n",
        "                labels[i] = j\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "@njit(parallel=True)\n",
        "def _update_centroids_cpu(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count cluster members and sum values\n",
        "    for i in range(n_samples):\n",
        "        cluster_id = labels[i]\n",
        "        counts[cluster_id] += 1\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Calculate means\n",
        "    for i in prange(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "# Unified interface\n",
        "def _fast_distances(X, centroids):\n",
        "    if HAS_GPU:\n",
        "        return compute_distances(X, centroids)\n",
        "    else:\n",
        "        return _fast_distances_cpu(X, centroids)\n",
        "\n",
        "def _assign_labels_numba(distances):\n",
        "    if HAS_GPU:\n",
        "        return assign_labels(distances)\n",
        "    else:\n",
        "        return _assign_labels_cpu(distances)\n",
        "\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    if HAS_GPU:\n",
        "        return _update_centroids_gpu(X, labels, n_clusters)\n",
        "    else:\n",
        "        return _update_centroids_cpu(X, labels, n_clusters)\n",
        "\n",
        "# Main kmeans function\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    #scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS9Uu76ZUbY5",
        "outputId": "f00ce208-65a2-4e6f-9517-a7990a08a503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n"
          ]
        }
      ]
    }
  ]
}