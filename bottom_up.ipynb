{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHiqh+lwhIVc40wilqE7WF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejash09/K-means/blob/main/bottom_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 1st try\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "y2PhSg_57tw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTJiPzx77qYK",
        "outputId": "3ad8a3e3-b57c-4628-c8bc-8ca4a977dc6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 6.824 seconds\n",
            "Average execution time (OptimizedKMeans): 0.027 seconds\n",
            "Average execution time (SklearnKMeans): 0.037 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 4.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "@njit(parallel=True)\n",
        "def _fast_distances(X, centroids):\n",
        "    \"\"\"Compute squared Euclidean distances with Numba - highly optimized version.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids (constant for all data points)\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Pre-compute squared norms of X\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float64)\n",
        "    for i in range(n_samples):\n",
        "        for k in range(n_features):\n",
        "            x_norms[i] += X[i, k] * X[i, k]\n",
        "\n",
        "    # Use ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩ identity for faster computation\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit\n",
        "def _assign_labels_numba(X, centroids):\n",
        "    \"\"\"Assign labels to data points based on nearest centroid.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    labels = np.zeros(n_samples, dtype=np.int32)\n",
        "    min_distances = np.full(n_samples, np.inf)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            dist = 0.0\n",
        "            for k in range(X.shape[1]):\n",
        "                diff = X[i, k] - centroids[j, k]\n",
        "                dist += diff * diff\n",
        "            if dist < min_distances[i]:\n",
        "                min_distances[i] = dist\n",
        "                labels[i] = j\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "@njit\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels.\"\"\"\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Sum points in each cluster\n",
        "    for i in range(X.shape[0]):\n",
        "        cluster_id = labels[i]\n",
        "        counts[cluster_id] += 1\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Divide by counts to get means\n",
        "    for i in range(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "# =============================================================================\n",
        "# Optimized Bottom-Up KMeans Implementation\n",
        "# =============================================================================\n",
        "class BottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation with a bottom-up approach, gradually adding points\n",
        "    to the calculation until convergence.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor=0.1,\n",
        "                 batch_growth_factor=15, verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor  # For early convergence detection\n",
        "        self.n_init = n_init  # Run algorithm multiple times and select best\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Enhanced centroid initialization with multiple methods.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Stratified random selection for better coverage\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Divide the data range into strata for more uniform coverage\n",
        "            sample_indices = []\n",
        "            if n_samples >= self.n_clusters * 10:  # If we have enough samples\n",
        "                # Use approximate quantiles to divide data\n",
        "                from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "                # Use feature with highest variance for stratification\n",
        "                feature_var = np.var(X, axis=0)\n",
        "                stratify_feature = np.argmax(feature_var)\n",
        "\n",
        "                discretizer = KBinsDiscretizer(n_bins=min(self.n_clusters, 20),\n",
        "                                              encode='ordinal', strategy='quantile')\n",
        "                strata = discretizer.fit_transform(X[:, stratify_feature].reshape(-1, 1)).astype(int).flatten()\n",
        "                unique_strata = np.unique(strata)\n",
        "\n",
        "                # Select points from different strata\n",
        "                for stratum in unique_strata:\n",
        "                    stratum_indices = np.where(strata == stratum)[0]\n",
        "                    if len(stratum_indices) > 0:\n",
        "                        idx = random_state.choice(stratum_indices,\n",
        "                                                 size=min(max(1, self.n_clusters // len(unique_strata)),\n",
        "                                                         len(stratum_indices)),\n",
        "                                                 replace=False)\n",
        "                        sample_indices.extend(idx)\n",
        "\n",
        "            # If we couldn't get enough from stratification, add random ones\n",
        "            if len(sample_indices) < self.n_clusters:\n",
        "                remaining = self.n_clusters - len(sample_indices)\n",
        "                avail_indices = list(set(range(n_samples)) - set(sample_indices))\n",
        "                if avail_indices:\n",
        "                    additional = random_state.choice(avail_indices,\n",
        "                                                   size=min(remaining, len(avail_indices)),\n",
        "                                                   replace=False)\n",
        "                    sample_indices.extend(additional)\n",
        "\n",
        "            # If we still don't have enough, allow repeats\n",
        "            if len(sample_indices) < self.n_clusters:\n",
        "                remaining = self.n_clusters - len(sample_indices)\n",
        "                additional = random_state.choice(n_samples, size=remaining, replace=True)\n",
        "                sample_indices.extend(additional)\n",
        "\n",
        "            # Trim to exactly k clusters\n",
        "            sample_indices = sample_indices[:self.n_clusters]\n",
        "            return X[sample_indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ with vectorization\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # Use vectorized operations for faster distance calculations\n",
        "            for c in range(1, self.n_clusters):\n",
        "                # Calculate squared distances to closest centroid\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    closest_dist_sq = np.min(np.sum((X[:, np.newaxis, :] -\n",
        "                                              centroids[np.newaxis, :c, :])**2, axis=2), axis=1)\n",
        "\n",
        "                # Select next centroid with probability proportional to square distance\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    next_centroid_idx = random_state.choice(n_samples, p=probs)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "\n",
        "        elif self.init == 'k-means++-fast':\n",
        "            # Faster approximate k-means++ using subsampling for large datasets\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Subsample for very large datasets\n",
        "            subsample_limit = 10000\n",
        "            if n_samples > subsample_limit:\n",
        "                subsample_indices = random_state.choice(n_samples,\n",
        "                                                     size=subsample_limit,\n",
        "                                                     replace=False)\n",
        "                X_subset = X[subsample_indices]\n",
        "            else:\n",
        "                X_subset = X\n",
        "                subsample_indices = np.arange(n_samples)\n",
        "\n",
        "            # Choose first centroid randomly from subset\n",
        "            first_idx = random_state.randint(len(X_subset))\n",
        "            centroids[0] = X_subset[first_idx].copy()\n",
        "\n",
        "            # Initialize distances array once\n",
        "            subset_size = len(X_subset)\n",
        "            closest_dist_sq = np.zeros(subset_size)\n",
        "\n",
        "            for c in range(1, self.n_clusters):\n",
        "                # Update distances for new centroid\n",
        "                for i in range(subset_size):\n",
        "                    dist = np.sum((X_subset[i] - centroids[c-1])**2)\n",
        "                    if c == 1 or dist < closest_dist_sq[i]:\n",
        "                        closest_dist_sq[i] = dist\n",
        "\n",
        "                # Select next centroid\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    subset_idx = random_state.choice(subset_size, p=probs)\n",
        "                    centroids[c] = X_subset[subset_idx].copy()\n",
        "                else:\n",
        "                    # If all points are identical to centroids, pick randomly\n",
        "                    subset_idx = random_state.randint(subset_size)\n",
        "                    centroids[c] = X_subset[subset_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        \"\"\"Compute distances with vectorization and Numba.\"\"\"\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            # Vectorized implementation for non-Numba case\n",
        "            # Reshape to allow broadcasting\n",
        "            expanded_X = X[:, np.newaxis, :]\n",
        "            expanded_centroids = centroids[np.newaxis, :, :]\n",
        "\n",
        "            # Calculate squared distances\n",
        "            squared_diff = (expanded_X - expanded_centroids) ** 2\n",
        "            squared_distances = np.sum(squared_diff, axis=2)\n",
        "\n",
        "            return squared_distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, iteration):\n",
        "        \"\"\"\n",
        "        Enhanced batch selection strategy with adaptive sampling.\n",
        "        Selects points that are likely to improve centroid positions the most.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        inactive_mask = np.ones(n_samples, dtype=bool)\n",
        "        inactive_mask[current_active] = False\n",
        "        inactive_indices = np.where(inactive_mask)[0]\n",
        "\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([])\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        labels = np.argmin(inactive_distances, axis=1)\n",
        "\n",
        "        # Calculate multiple selection criteria\n",
        "        # 1. Uncertainty: difference between closest and second closest centroid\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            uncertainty = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            uncertainty = uncertainty / (np.max(uncertainty) + 1e-10)\n",
        "        else:\n",
        "            uncertainty = np.zeros(len(inactive_indices))\n",
        "\n",
        "        # 2. Representativeness: distance to closest centroid\n",
        "        closest_distances = np.min(inactive_distances, axis=1)\n",
        "        if np.max(closest_distances) > np.min(closest_distances):\n",
        "            representativeness = (closest_distances - np.min(closest_distances)) / (np.max(closest_distances) - np.min(closest_distances) + 1e-10)\n",
        "        else:\n",
        "            representativeness = np.zeros_like(closest_distances)\n",
        "\n",
        "        # 3. Cluster balance: prioritize points from underrepresented clusters\n",
        "        cluster_counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        balance_score = np.zeros(len(inactive_indices))\n",
        "        for i, label in enumerate(labels):\n",
        "            if cluster_counts[label] > 0:\n",
        "                balance_score[i] = 1.0 / cluster_counts[label]\n",
        "        balance_score = balance_score / (np.max(balance_score) + 1e-10)\n",
        "\n",
        "        # Adaptive weighting of criteria based on iteration\n",
        "        # Early: focus on representativeness (exploration)\n",
        "        # Late: focus on uncertainty (refinement)\n",
        "        exploration_weight = max(0, 1 - iteration / self.max_iterations)\n",
        "        refinement_weight = 1 - exploration_weight\n",
        "\n",
        "        # Combine criteria with adaptive weights\n",
        "        combined_score = (\n",
        "            exploration_weight * representativeness +\n",
        "            refinement_weight * uncertainty +\n",
        "            0.2 * balance_score  # Fixed weight for balance\n",
        "        )\n",
        "\n",
        "        # Select best points according to combined score\n",
        "        n_to_select = min(batch_size, len(inactive_indices))\n",
        "        selected_idx = np.argsort(-combined_score)[:n_to_select]  # Descending order\n",
        "\n",
        "        return inactive_indices[selected_idx]\n",
        "\n",
        "    def _run_single_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run a single instance of the bottom-up K-means algorithm.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        inertia = 0.0\n",
        "        n_iter = 0\n",
        "        iteration_table = []\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize batch size and active set\n",
        "        initial_batch_size = max(int(n_samples * self.batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        # Initial active set with points closest to centroids and some distant points\n",
        "        min_distances = np.min(distances, axis=1)\n",
        "        closest_points = np.argsort(min_distances)[:initial_batch_size // 2]\n",
        "        farthest_points = np.argsort(min_distances)[-initial_batch_size // 2:]\n",
        "        active_indices = np.concatenate([closest_points, farthest_points])\n",
        "        active_indices = np.unique(active_indices)  # Remove any duplicates\n",
        "\n",
        "        # For tracking stability across iterations\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            if len(active_indices) > 0:\n",
        "                active_X = X[active_indices]\n",
        "                active_labels = labels[active_indices]\n",
        "\n",
        "                # Update centroids based on active points\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    new_centroids, counts = _update_centroids_numba(active_X, active_labels, self.n_clusters)\n",
        "                    # Handle empty clusters\n",
        "                    for k in range(self.n_clusters):\n",
        "                        if counts[k] == 0:\n",
        "                            # Find the point furthest from its centroid\n",
        "                            active_distances = distances[active_indices]\n",
        "                            furthest_point_idx = np.argmax(np.min(active_distances, axis=1))\n",
        "                            new_centroids[k] = active_X[furthest_point_idx]\n",
        "                else:\n",
        "                    new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                    for k in range(self.n_clusters):\n",
        "                        cluster_mask = (active_labels == k)\n",
        "                        if np.any(cluster_mask):\n",
        "                            new_centroids[k] = np.mean(active_X[cluster_mask], axis=0)\n",
        "                        else:\n",
        "                            # For empty clusters, use old centroid or find a new representative\n",
        "                            if iteration > 0:\n",
        "                                new_centroids[k] = old_centroids[k]\n",
        "                            else:\n",
        "                                # First iteration, just pick a random point\n",
        "                                random_idx = random_state.randint(len(active_X))\n",
        "                                new_centroids[k] = active_X[random_idx]\n",
        "\n",
        "                centroids = new_centroids\n",
        "\n",
        "            # Compute distances and reassign labels\n",
        "            distances = self._compute_distances(X, centroids)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "            # Calculate change in active points\n",
        "            if len(active_indices) > 0:\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "            else:\n",
        "                active_changed = 0\n",
        "                active_changed_pct = 0\n",
        "\n",
        "            labels = new_labels\n",
        "\n",
        "            # Calculate inertia (sum of squared distances to closest centroid)\n",
        "            min_distances = np.min(distances, axis=1)\n",
        "\n",
        "            # Calculate inertia for active points only to monitor convergence\n",
        "            active_inertia = np.sum(min_distances[active_indices]) if len(active_indices) > 0 else 0\n",
        "\n",
        "            # Adaptive batch size growth based on convergence behavior\n",
        "            convergence_factor = 1.0\n",
        "            if iteration > 0 and prev_inertia > 0:\n",
        "                inertia_change = abs(active_inertia - prev_inertia) / prev_inertia\n",
        "                # If inertia change is small, we're converging, so grow batch size faster\n",
        "                if inertia_change < 0.01:\n",
        "                    convergence_factor = 1.5  # Accelerate batch growth\n",
        "                elif inertia_change > 0.1:\n",
        "                    convergence_factor = 0.8  # Slow down batch growth\n",
        "\n",
        "            # Update batch size for next iteration\n",
        "            next_batch_base_size = int(initial_batch_size * (self.batch_growth_factor ** iteration) * convergence_factor)\n",
        "            next_batch_size = min(\n",
        "                next_batch_base_size,\n",
        "                n_samples - len(active_indices)\n",
        "            )\n",
        "\n",
        "            # Select next batch of points to add\n",
        "            new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, iteration)\n",
        "\n",
        "            # Record iteration information\n",
        "            iteration_info = {\n",
        "                'iteration': n_iter,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'active_points_changed': active_changed,\n",
        "                'active_points_changed_pct': active_changed_pct * 100,\n",
        "                'new_points_added': len(new_batch),\n",
        "                'active_inertia': active_inertia,\n",
        "                'total_coverage': len(active_indices) / n_samples * 100 if n_samples > 0 else 0\n",
        "            }\n",
        "            iteration_table.append(iteration_info)\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 5 == 0:\n",
        "                logger.info(f\"Iteration {n_iter}: \"\n",
        "                           f\"{active_changed} active points changed ({active_changed_pct:.2%}), \"\n",
        "                           f\"{len(new_batch)} new points added, \"\n",
        "                           f\"{len(active_indices) / n_samples * 100:.1f}% coverage\")\n",
        "\n",
        "            # Add new batch to active set\n",
        "            if len(new_batch) > 0:\n",
        "                active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "            # Calculate centroid shift for convergence check\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - centroids)**2, axis=1)))\n",
        "\n",
        "            # Early stopping conditions\n",
        "\n",
        "            # 1. Centroid stability\n",
        "            if centroid_shift < self.tolerance:\n",
        "                if len(active_indices) == n_samples:\n",
        "                    # Full dataset and centroids stable = converged\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Converged at iteration {n_iter}: centroids stable\")\n",
        "                    break\n",
        "                # If not full dataset but centroids stable, add more points faster\n",
        "                next_batch_size = min(next_batch_size * 2, n_samples - len(active_indices))\n",
        "\n",
        "            # 2. Inertia stability (track consecutive stable iterations)\n",
        "            if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                stability_counter += 1\n",
        "            else:\n",
        "                stability_counter = 0\n",
        "\n",
        "            # If inertia stable for multiple iterations and we have enough points\n",
        "            if stability_counter >= 3 and len(active_indices) / n_samples > 0.5:\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"Early stopping at iteration {n_iter}: inertia stable\")\n",
        "                break\n",
        "\n",
        "            # 3. All points active and no label changes\n",
        "            if len(active_indices) == n_samples and active_changed == 0:\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"Converged at iteration {n_iter}: all points stable\")\n",
        "                break\n",
        "\n",
        "            # 4. No new points to add and centroids stable\n",
        "            if len(new_batch) == 0 and centroid_shift < self.tolerance:\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"Converged at iteration {n_iter}: no new points and centroids stable\")\n",
        "                break\n",
        "\n",
        "            prev_inertia = active_inertia\n",
        "\n",
        "        # Final update with all points\n",
        "        if NUMBA_AVAILABLE:\n",
        "            centroids, _ = _update_centroids_numba(X, labels, self.n_clusters)\n",
        "        else:\n",
        "            for k in range(self.n_clusters):\n",
        "                cluster_mask = (labels == k)\n",
        "                if np.any(cluster_mask):\n",
        "                    centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "\n",
        "        # Final assignment\n",
        "        distances = self._compute_distances(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the model to data.\n",
        "        Run multiple initializations and select the best result.\n",
        "        \"\"\"\n",
        "        X = check_array(X)\n",
        "\n",
        "        best_inertia = np.inf\n",
        "        best_centroids = None\n",
        "        best_labels = None\n",
        "        best_n_iter = 0\n",
        "\n",
        "        seeds = self._get_seeds()\n",
        "\n",
        "        for seed_idx, seed in enumerate(seeds):\n",
        "            if self.verbose and len(seeds) > 1:\n",
        "                logger.info(f\"K-means initialization {seed_idx + 1}/{len(seeds)}\")\n",
        "\n",
        "            centroids, labels, inertia, n_iter, iter_table = self._run_single_kmeans(X, seed)\n",
        "\n",
        "            if inertia < best_inertia:\n",
        "                best_centroids = centroids.copy()\n",
        "                best_labels = labels.copy()\n",
        "                best_inertia = inertia\n",
        "                best_n_iter = n_iter\n",
        "                self.iteration_table_ = iter_table\n",
        "\n",
        "        self.cluster_centers_ = best_centroids\n",
        "        self.labels_ = best_labels\n",
        "        self.inertia_ = best_inertia\n",
        "        self.n_iter_ = best_n_iter\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"BottomUpKMeans converged after {self.n_iter_} iterations. \"\n",
        "                        f\"Inertia: {self.inertia_:.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _get_seeds(self):\n",
        "        \"\"\"Generate random seeds for multiple initializations.\"\"\"\n",
        "        random_state = check_random_state(self.random_state)\n",
        "        seeds = []\n",
        "        for i in range(self.n_init):\n",
        "            seed = random_state.randint(0, 2**31 - 1)\n",
        "            seeds.append(seed)\n",
        "        return seeds\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster for each sample in X.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X)\n",
        "\n",
        "        if NUMBA_AVAILABLE:\n",
        "            labels, _ = _assign_labels_numba(X, self.cluster_centers_)\n",
        "            return labels\n",
        "        else:\n",
        "            distances = self._compute_distances(X, self.cluster_centers_)\n",
        "            return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Prints a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            return df\n",
        "        except ImportError:\n",
        "            for info in self.iteration_table_:\n",
        "                print(\", \".join([f\"{k}: {v}\" for k, v in info.items()]))\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = BottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OrACuhtxKgsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iKxec4LR7sbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _fast_distances(X, centroids):\n",
        "    \"\"\"Ultra-optimized squared Euclidean distances with Numba.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Pre-compute squared norms of X\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float64)\n",
        "    for i in prange(n_samples):\n",
        "        for k in range(n_features):\n",
        "            x_norms[i] += X[i, k] * X[i, k]\n",
        "\n",
        "    # Use ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩ identity for faster computation\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(fastmath=True)\n",
        "def _fast_distances_block(X, centroids, start_idx, end_idx):\n",
        "    \"\"\"Compute distances for a block of samples.\"\"\"\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    block_size = end_idx - start_idx\n",
        "    distances = np.empty((block_size, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Process block\n",
        "    for i in range(block_size):\n",
        "        x_idx = start_idx + i\n",
        "        # Compute x_norm for this sample\n",
        "        x_norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            x_norm += X[x_idx, k] * X[x_idx, k]\n",
        "\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norm + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[x_idx, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels - fully optimized.\"\"\"\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count points in clusters - separate loop for better cache performance\n",
        "    for i in range(X.shape[0]):\n",
        "        counts[labels[i]] += 1\n",
        "\n",
        "    # Sum points in each cluster - vectorized across features for each sample\n",
        "    for i in range(X.shape[0]):\n",
        "        cluster_id = labels[i]\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Divide by counts to get means\n",
        "    for i in range(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXoqau7lFV6K",
        "outputId": "85f0abc8-72c3-48e3-ac46-6ec27ce7337e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 1.224 seconds\n",
            "Average execution time (OptimizedKMeans): 0.022 seconds\n",
            "Average execution time (SklearnKMeans): 0.020 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install line_profiler\n",
        "%load_ext line_profiler\n",
        "%lprun -f run_bench_evaluation run_bench_evaluation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6T8Xm8kz6Od",
        "outputId": "ec348ac2-6938-4a2b-cbc6-58657a1b96c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: line_profiler in /usr/local/lib/python3.11/dist-packages (4.2.0)\n",
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 0.462 seconds\n",
            "Average execution time (OptimizedKMeans): 0.027 seconds\n",
            "Average execution time (SklearnKMeans): 0.022 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "n_clusters = 3\n",
        "n_runs = 3\n",
        "bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42, verbose=True)\n",
        "#bu_kmeans.fit(X)\n",
        "%lprun -f HybridBottomUpKMeans.fit bu_kmeans.fit(X)\n"
      ],
      "metadata": {
        "id": "3T1TG2qN4cBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cProfile, pstats\n",
        "\n",
        "# Profile your function\n",
        "cProfile.run('run_bench_evaluation()', 'profile_output')\n",
        "\n",
        "# Load the profiling data\n",
        "p = pstats.Stats('profile_output')\n",
        "p.strip_dirs().sort_stats('cumulative').print_stats(15)  # Top 10 functions by cumulative time\n",
        "\n",
        "# For a recursive breakdown, check the callers and callees of a specific function:\n",
        "p.print_callers('run_bench_evaluation')\n",
        "p.print_callees('run_bench_evaluation')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ84GT3lL-K2",
        "outputId": "866d8916-08ec-4bf7-cfbe-da03ace4bf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 0.463 seconds\n",
            "Average execution time (OptimizedKMeans): 0.028 seconds\n",
            "Average execution time (SklearnKMeans): 0.021 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n",
            "Fri Mar 14 10:27:37 2025    profile_output\n",
            "\n",
            "         678370 function calls (677501 primitive calls) in 118.214 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 650 to 10 due to restriction <10>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000  118.215  118.215 {built-in method builtins.exec}\n",
            "        1    0.000    0.000  118.214  118.214 <string>:1(<module>)\n",
            "        1    0.001    0.001  118.214  118.214 <ipython-input-3-d31acea3e1da>:1170(run_bench_evaluation)\n",
            "   360/63    0.004    0.000  116.702    1.852 _param_validation.py:185(wrapper)\n",
            "        9    0.000    0.000  116.142   12.905 _unsupervised.py:42(silhouette_score)\n",
            "        9    0.005    0.001  116.141   12.905 _unsupervised.py:196(silhouette_samples)\n",
            "       72    0.196    0.003  116.114    1.613 pairwise.py:2082(pairwise_distances_chunked)\n",
            "      117    0.001    0.000   71.929    0.615 pairwise.py:2266(pairwise_distances)\n",
            "      117    0.001    0.000   71.928    0.615 pairwise.py:1964(_parallel_pairwise)\n",
            "      117    0.002    0.000   71.919    0.615 pairwise.py:273(euclidean_distances)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pstats.Stats at 0x7d3a85252690>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p.strip_dirs().sort_stats('cumulative').print_stats(100)\n",
        "p.print_callers('run_bench_evaluation')\n",
        "p.print_callees('run_bench_evaluation')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXoLJegONAPz",
        "outputId": "9f79fb68-c6d6-4b0f-87a2-39bb4461f182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 14 10:27:37 2025    profile_output\n",
            "\n",
            "         678370 function calls (677501 primitive calls) in 118.214 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 650 to 100 due to restriction <100>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000  118.215  118.215 {built-in method builtins.exec}\n",
            "        1    0.000    0.000  118.214  118.214 <string>:1(<module>)\n",
            "        1    0.001    0.001  118.214  118.214 <ipython-input-3-d31acea3e1da>:1170(run_bench_evaluation)\n",
            "   360/63    0.004    0.000  116.702    1.852 _param_validation.py:185(wrapper)\n",
            "        9    0.000    0.000  116.142   12.905 _unsupervised.py:42(silhouette_score)\n",
            "        9    0.005    0.001  116.141   12.905 _unsupervised.py:196(silhouette_samples)\n",
            "       72    0.196    0.003  116.114    1.613 pairwise.py:2082(pairwise_distances_chunked)\n",
            "      117    0.001    0.000   71.929    0.615 pairwise.py:2266(pairwise_distances)\n",
            "      117    0.001    0.000   71.928    0.615 pairwise.py:1964(_parallel_pairwise)\n",
            "      117    0.002    0.000   71.919    0.615 pairwise.py:273(euclidean_distances)\n",
            "      126   24.443    0.194   71.846    0.570 pairwise.py:391(_euclidean_distances)\n",
            "       63   43.930    0.697   44.068    0.699 _unsupervised.py:142(_silhouette_reduce)\n",
            "      126   27.135    0.215   27.137    0.215 extmath.py:153(safe_sparse_dot)\n",
            "      225   20.212    0.090   20.213    0.090 _array_api.py:1080(_modify_in_place_if_numpy)\n",
            "        3    0.003    0.001    1.389    0.463 <ipython-input-3-d31acea3e1da>:784(fit)\n",
            "        9    0.068    0.008    1.379    0.153 <ipython-input-3-d31acea3e1da>:546(_run_kmeans)\n",
            "       27    0.338    0.013    0.754    0.028 <ipython-input-3-d31acea3e1da>:455(_select_next_batch)\n",
            "      478    0.001    0.000    0.343    0.001 fromnumeric.py:53(_wrapfunc)\n",
            "      236    0.332    0.001    0.332    0.001 {method 'argsort' of 'numpy.ndarray' objects}\n",
            "      173    0.001    0.000    0.287    0.002 fromnumeric.py:1025(argsort)\n",
            "     1676    0.286    0.000    0.286    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
            "        9    0.142    0.016    0.277    0.031 _supervised.py:926(adjusted_mutual_info_score)\n",
            "      958    0.005    0.000    0.221    0.000 fromnumeric.py:71(_wrapreduction)\n",
            "      104    0.000    0.000    0.173    0.002 fromnumeric.py:2836(min)\n",
            "       30    0.040    0.001    0.164    0.005 <ipython-input-3-d31acea3e1da>:522(_standard_kmeans_iteration)\n",
            "      180    0.001    0.000    0.146    0.001 arraysetops.py:138(unique)\n",
            "      180    0.022    0.000    0.144    0.001 arraysetops.py:323(_unique1d)\n",
            "      432    0.012    0.000    0.102    0.000 validation.py:736(check_array)\n",
            "       79    0.098    0.001    0.098    0.001 <ipython-input-3-d31acea3e1da>:83(_fast_distances)\n",
            "      153    0.094    0.001    0.094    0.001 {method 'sort' of 'numpy.ndarray' objects}\n",
            "       18    0.001    0.000    0.093    0.005 _supervised.py:87(contingency_matrix)\n",
            "       69    0.086    0.001    0.089    0.001 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
            "        9    0.002    0.000    0.087    0.010 _unsupervised.py:390(davies_bouldin_score)\n",
            "        9    0.008    0.001    0.086    0.010 <ipython-input-3-d31acea3e1da>:346(_initialize_centroids)\n",
            "        3    0.022    0.007    0.084    0.028 <ipython-input-3-d31acea3e1da>:934(fit)\n",
            "        9    0.000    0.000    0.082    0.009 _supervised.py:341(adjusted_rand_score)\n",
            "        9    0.000    0.000    0.081    0.009 _supervised.py:176(pair_confusion_matrix)\n",
            "       66    0.001    0.000    0.079    0.001 <ipython-input-3-d31acea3e1da>:412(_compute_distances_parallel)\n",
            "      117    0.002    0.000    0.077    0.001 pairwise.py:75(check_pairwise_arrays)\n",
            "        3    0.000    0.000    0.063    0.021 base.py:1372(wrapper)\n",
            "        3    0.015    0.005    0.061    0.020 _kmeans.py:1426(fit)\n",
            "   270019    0.061    0.000    0.061    0.000 multiarray.py:892(bincount)\n",
            "273879/273865    0.060    0.000    0.060    0.000 {built-in method builtins.len}\n",
            "      237    0.005    0.000    0.055    0.000 _methods.py:101(_mean)\n",
            "      333    0.008    0.000    0.054    0.000 validation.py:90(_assert_all_finite)\n",
            "       82    0.001    0.000    0.048    0.001 fromnumeric.py:1236(argmin)\n",
            "       82    0.047    0.001    0.047    0.001 {method 'argmin' of 'numpy.ndarray' objects}\n",
            "      593    0.003    0.000    0.046    0.000 fromnumeric.py:2177(sum)\n",
            "      246    0.003    0.000    0.045    0.000 extmath.py:47(row_norms)\n",
            "        9    0.014    0.002    0.044    0.005 _unsupervised.py:319(calinski_harabasz_score)\n",
            "      180    0.001    0.000    0.043    0.000 fromnumeric.py:3385(mean)\n",
            "       36    0.000    0.000    0.043    0.001 fromnumeric.py:865(sort)\n",
            "       18    0.000    0.000    0.042    0.002 _supervised.py:25(check_clusterings)\n",
            "      246    0.000    0.000    0.039    0.000 einsumfunc.py:1009(einsum)\n",
            "      246    0.038    0.000    0.038    0.000 {built-in method numpy.core._multiarray_umath.c_einsum}\n",
            "       36    0.001    0.000    0.035    0.001 multiclass.py:229(type_of_target)\n",
            "      297    0.034    0.000    0.034    0.000 {built-in method numpy.array}\n",
            "       27    0.033    0.001    0.033    0.001 <ipython-input-3-d31acea3e1da>:149(_update_centroids_numba)\n",
            "       18    0.000    0.000    0.033    0.002 _coo.py:318(tocsr)\n",
            "      198    0.029    0.000    0.029    0.000 {method 'extend' of 'list' objects}\n",
            "       27    0.000    0.000    0.028    0.001 _label.py:97(fit_transform)\n",
            "       36    0.000    0.000    0.027    0.001 _unique.py:81(cached_unique)\n",
            "       72    0.000    0.000    0.027    0.000 _unique.py:105(<genexpr>)\n",
            "       36    0.000    0.000    0.027    0.001 _unique.py:62(_cached_unique)\n",
            "       36    0.000    0.000    0.026    0.001 _array_api.py:415(unique_values)\n",
            "       10    0.017    0.002    0.026    0.003 <ipython-input-3-d31acea3e1da>:923(_update_centroids)\n",
            "       27    0.000    0.000    0.025    0.001 _encode.py:20(_unique)\n",
            "       27    0.000    0.000    0.025    0.001 _encode.py:61(_unique_np)\n",
            "       36    0.000    0.000    0.025    0.001 _compressed.py:1239(sum_duplicates)\n",
            "       27    0.000    0.000    0.024    0.001 _array_api.py:409(unique_inverse)\n",
            "      189    0.001    0.000    0.024    0.000 {method 'min' of 'numpy.ndarray' objects}\n",
            "      189    0.000    0.000    0.023    0.000 _methods.py:43(_amin)\n",
            "     1129    0.002    0.000    0.023    0.000 {built-in method builtins.any}\n",
            "       18    0.000    0.000    0.023    0.001 _compressed.py:1286(sort_indices)\n",
            "       18    0.022    0.001    0.022    0.001 {built-in method scipy.sparse._sparsetools.csr_sort_indices}\n",
            "       13    0.000    0.000    0.021    0.002 <ipython-input-3-d31acea3e1da>:914(_compute_distances)\n",
            "      594    0.001    0.000    0.021    0.000 _array_api.py:441(isdtype)\n",
            "      594    0.002    0.000    0.020    0.000 _array_api.py:225(isdtype)\n",
            "       63    0.001    0.000    0.018    0.000 _compressed.py:622(sum)\n",
            "        9    0.001    0.000    0.018    0.002 _supervised.py:810(mutual_info_score)\n",
            "       27    0.000    0.000    0.018    0.001 validation.py:1203(check_X_y)\n",
            "       18    0.001    0.000    0.017    0.001 _supervised.py:1260(entropy)\n",
            " 1116/666    0.007    0.000    0.017    0.000 _array_api.py:237(_isdtype_single)\n",
            "      570    0.007    0.000    0.017    0.000 validation.py:381(_num_samples)\n",
            "        1    0.001    0.001    0.016    0.016 <ipython-input-3-d31acea3e1da>:1033(load_synthetic_data)\n",
            "        3    0.000    0.000    0.016    0.005 parallel.py:161(wrapper)\n",
            "       33    0.007    0.000    0.015    0.000 validation.py:1490(check_random_state)\n",
            "        3    0.014    0.005    0.015    0.005 _kmeans.py:623(_kmeans_single_lloyd)\n",
            "        3    0.015    0.005    0.015    0.005 {method 'normal' of 'numpy.random.mtrand.RandomState' objects}\n",
            "       18    0.000    0.000    0.015    0.001 _array_api.py:412(unique_counts)\n",
            "       36    0.001    0.000    0.014    0.000 _base.py:1037(sum)\n",
            "    36/27    0.001    0.000    0.014    0.001 _coo.py:27(__init__)\n",
            "        9    0.000    0.000    0.014    0.002 <ipython-input-3-d31acea3e1da>:290(_set_dynamic_parameters)\n",
            "        3    0.000    0.000    0.013    0.004 _kmeans.py:955(_init_centroids)\n",
            "        3    0.002    0.001    0.013    0.004 _kmeans.py:174(_kmeans_plusplus)\n",
            "      135    0.001    0.000    0.013    0.000 _base.py:1303(_get_index_dtype)\n",
            "       57    0.000    0.000    0.012    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
            "       27    0.000    0.000    0.012    0.000 _indexing.py:179(_safe_indexing)\n",
            "    15562    0.007    0.000    0.012    0.000 {built-in method builtins.isinstance}\n",
            "     6192    0.009    0.000    0.012    0.000 _array_api.py:381(__getattr__)\n",
            "\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 650 to 1 due to restriction <'run_bench_evaluation'>\n",
            "\n",
            "Function                                                   was called by...\n",
            "                                                               ncalls  tottime  cumtime\n",
            "<ipython-input-3-d31acea3e1da>:1170(run_bench_evaluation)  <-       1    0.001  118.214  <string>:1(<module>)\n",
            "\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 650 to 1 due to restriction <'run_bench_evaluation'>\n",
            "\n",
            "Function                                                   called...\n",
            "                                                               ncalls  tottime  cumtime\n",
            "<ipython-input-3-d31acea3e1da>:1170(run_bench_evaluation)  ->       3    0.000    0.000  <ipython-input-3-d31acea3e1da>:216(__init__)\n",
            "                                                                    3    0.003    1.389  <ipython-input-3-d31acea3e1da>:784(fit)\n",
            "                                                                    1    0.000    0.008  <ipython-input-3-d31acea3e1da>:846(print_iteration_table)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:864(__init__)\n",
            "                                                                    3    0.022    0.084  <ipython-input-3-d31acea3e1da>:934(fit)\n",
            "                                                                    1    0.001    0.016  <ipython-input-3-d31acea3e1da>:1033(load_synthetic_data)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1261(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1262(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1263(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1264(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1271(<listcomp>)\n",
            "                                                                    3    0.000    0.000  <ipython-input-3-d31acea3e1da>:1272(<listcomp>)\n",
            "                                                                    3    0.000    0.000  _kmeans.py:1377(__init__)\n",
            "                                                                   45    0.001  116.647  _param_validation.py:185(wrapper)\n",
            "                                                                    3    0.000    0.063  base.py:1372(wrapper)\n",
            "                                                                   24    0.000    0.000  fromnumeric.py:3380(_mean_dispatcher)\n",
            "                                                                   24    0.000    0.001  fromnumeric.py:3385(mean)\n",
            "                                                                   37    0.000    0.006  {built-in method builtins.print}\n",
            "                                                                   18    0.000    0.000  {built-in method time.time}\n",
            "                                                                   27    0.000    0.000  {method 'append' of 'list' objects}\n",
            "                                                                    6    0.000    0.000  {method 'upper' of 'str' objects}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pstats.Stats at 0x7d3a85252690>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scalene"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwCOZ3BqO2t8",
        "outputId": "726595c0-13cd-47ba-da5a-af84a95e69cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scalene\n",
            "  Downloading scalene-1.5.51-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: wheel>=0.36.1 in /usr/local/lib/python3.11/dist-packages (from scalene) (0.45.1)\n",
            "Requirement already satisfied: rich>=10.7.0 in /usr/local/lib/python3.11/dist-packages (from scalene) (13.9.4)\n",
            "Requirement already satisfied: cloudpickle>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from scalene) (3.1.1)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.555.43 in /usr/local/lib/python3.11/dist-packages (from scalene) (12.570.86)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from scalene) (3.1.6)\n",
            "Requirement already satisfied: psutil>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from scalene) (5.9.5)\n",
            "Requirement already satisfied: numpy!=1.27,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from scalene) (1.26.4)\n",
            "Requirement already satisfied: pydantic>=2.6 in /usr/local/lib/python3.11/dist-packages (from scalene) (2.10.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.0.3->scalene) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.6->scalene) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.6->scalene) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.6->scalene) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.7.0->scalene) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.7.0->scalene) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.7.0->scalene) (0.1.2)\n",
            "Downloading scalene-1.5.51-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scalene\n",
            "Successfully installed scalene-1.5.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "from scalene import scalene_profiler\n",
        "\n",
        "# Turn profiling on\n",
        "scalene_profiler.start()\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _fast_distances(X, centroids):\n",
        "    \"\"\"Ultra-optimized squared Euclidean distances with Numba.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Pre-compute squared norms of X\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float64)\n",
        "    for i in prange(n_samples):\n",
        "        for k in range(n_features):\n",
        "            x_norms[i] += X[i, k] * X[i, k]\n",
        "\n",
        "    # Use ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩ identity for faster computation\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(fastmath=True)\n",
        "def _fast_distances_block(X, centroids, start_idx, end_idx):\n",
        "    \"\"\"Compute distances for a block of samples.\"\"\"\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    block_size = end_idx - start_idx\n",
        "    distances = np.empty((block_size, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Process block\n",
        "    for i in range(block_size):\n",
        "        x_idx = start_idx + i\n",
        "        # Compute x_norm for this sample\n",
        "        x_norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            x_norm += X[x_idx, k] * X[x_idx, k]\n",
        "\n",
        "        for j in range(n_clusters):\n",
        "            # Start with ||x||² + ||y||²\n",
        "            distances[i, j] = x_norm + centroid_norms[j]\n",
        "            # Subtract 2⟨x,y⟩\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[x_idx, k] * centroids[j, k]\n",
        "            distances[i, j] -= 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels - fully optimized.\"\"\"\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count points in clusters - separate loop for better cache performance\n",
        "    for i in range(X.shape[0]):\n",
        "        counts[labels[i]] += 1\n",
        "\n",
        "    # Sum points in each cluster - vectorized across features for each sample\n",
        "    for i in range(X.shape[0]):\n",
        "        cluster_id = labels[i]\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Divide by counts to get means\n",
        "    for i in range(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "y-y9u3SwOsHG",
        "outputId": "34fca688-b4cb-4ed3-dfb7-a48f8d3ba3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: Do not try to invoke `start` if you have not called Scalene using one of the methods\n",
            "in https://github.com/plasma-umass/scalene#using-scalene\n",
            "(The most likely issue is that you need to run your code with `scalene`, not `python`).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "1",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "import numpy as np\n",
        "from numba import njit, prange, float64, int32\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _fast_distances(X, centroids):\n",
        "    \"\"\"Ultra-optimized squared Euclidean distances with Numba.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.empty(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            norm += centroids[j, k] * centroids[j, k]\n",
        "        centroid_norms[j] = norm\n",
        "\n",
        "    # Pre-compute squared norms of X\n",
        "    x_norms = np.empty(n_samples, dtype=np.float64)\n",
        "    for i in prange(n_samples):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            norm += X[i, k] * X[i, k]\n",
        "        x_norms[i] = norm\n",
        "\n",
        "    # Compute distances using ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
        "    for i in prange(n_samples):\n",
        "        x_norm = x_norms[i]\n",
        "        for j in range(n_clusters):\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] = x_norm + centroid_norms[j] - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(fastmath=True, cache=True)\n",
        "def _fast_distances_block(X, centroids, start_idx, end_idx):\n",
        "    \"\"\"Compute distances for a block of samples.\"\"\"\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    block_size = end_idx - start_idx\n",
        "    distances = np.empty((block_size, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms of centroids\n",
        "    centroid_norms = np.empty(n_clusters, dtype=np.float64)\n",
        "    for j in range(n_clusters):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            norm += centroids[j, k] * centroids[j, k]\n",
        "        centroid_norms[j] = norm\n",
        "\n",
        "    # Process block with improved locality\n",
        "    for i in range(block_size):\n",
        "        x_idx = start_idx + i\n",
        "\n",
        "        # Compute x_norm for this sample\n",
        "        x_norm = 0.0\n",
        "        x_values = np.empty(n_features, dtype=np.float64)\n",
        "\n",
        "        # Cache the values in x_values for better memory access\n",
        "        for k in range(n_features):\n",
        "            val = X[x_idx, k]\n",
        "            x_values[k] = val\n",
        "            x_norm += val * val\n",
        "\n",
        "        for j in range(n_clusters):\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += x_values[k] * centroids[j, k]\n",
        "            distances[i, j] = x_norm + centroid_norms[j] - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels - fully optimized.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count points in clusters - separate loop for better cache performance\n",
        "    for i in range(n_samples):\n",
        "        counts[labels[i]] += 1\n",
        "\n",
        "    # Sum points in each cluster - use prange for outer loop\n",
        "    # but handle accumulation carefully to avoid race conditions\n",
        "    for i in prange(n_samples):\n",
        "        cluster_id = labels[i]\n",
        "        for j in range(n_features):\n",
        "            # Use atomic add to avoid race conditions when updating centroids\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Divide by counts to get means\n",
        "    for i in range(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "        else:\n",
        "            # Handle empty clusters - set to a random point\n",
        "            idx = np.random.randint(0, n_samples)\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] = X[idx, j]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    #scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-aHuTz3KjmI",
        "outputId": "803a1b0c-7a85-4434-e009-136579e029e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "import numpy as np\n",
        "from numba import njit, prange, float64, int32\n",
        "from numba import njit, prange, cuda\n",
        "import numpy as np\n",
        "from numba import njit, prange\n",
        "import numpy as np\n",
        "\n",
        "# Check if CuPy is available\n",
        "try:\n",
        "    import cupy as cp\n",
        "    HAS_GPU = True\n",
        "except ImportError:\n",
        "    HAS_GPU = False\n",
        "\n",
        "# GPU version using CuPy\n",
        "if HAS_GPU:\n",
        "    def _fast_distances_gpu(X, centroids):\n",
        "        \"\"\"Compute squared Euclidean distances with CuPy GPU acceleration.\"\"\"\n",
        "        # Transfer data to GPU if needed\n",
        "        X_gpu = cp.asarray(X)\n",
        "        centroids_gpu = cp.asarray(centroids)\n",
        "\n",
        "        # Compute squared norms\n",
        "        X_norm = cp.sum(X_gpu**2, axis=1, keepdims=True)\n",
        "        centroids_norm = cp.sum(centroids_gpu**2, axis=1, keepdims=True).T\n",
        "\n",
        "        # Use matrix multiplication for dot product\n",
        "        dot_product = cp.dot(X_gpu, centroids_gpu.T)\n",
        "\n",
        "        # Compute distances using ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
        "        distances = X_norm + centroids_norm - 2.0 * dot_product\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def compute_distances(X, centroids):\n",
        "        \"\"\"Wrapper to handle GPU distance computation.\"\"\"\n",
        "        distances_gpu = _fast_distances_gpu(X, centroids)\n",
        "        return cp.asnumpy(distances_gpu)\n",
        "\n",
        "    def assign_labels(distances):\n",
        "        \"\"\"Assign labels based on distances using GPU.\"\"\"\n",
        "        distances_gpu = cp.asarray(distances)\n",
        "        min_distances = cp.min(distances_gpu, axis=1)\n",
        "        labels = cp.argmin(distances_gpu, axis=1)\n",
        "\n",
        "        return cp.asnumpy(labels).astype(np.int32), cp.asnumpy(min_distances)\n",
        "\n",
        "    def _update_centroids_gpu(X, labels, n_clusters):\n",
        "        \"\"\"Update centroids using GPU acceleration.\"\"\"\n",
        "        X_gpu = cp.asarray(X)\n",
        "        labels_gpu = cp.asarray(labels)\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        centroids = cp.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "        counts = cp.zeros(n_clusters, dtype=cp.int32)\n",
        "\n",
        "        # Process each cluster\n",
        "        for k in range(n_clusters):\n",
        "            mask = (labels_gpu == k)\n",
        "            cluster_points = X_gpu[mask]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[k] = cp.mean(cluster_points, axis=0)\n",
        "                counts[k] = len(cluster_points)\n",
        "\n",
        "        return cp.asnumpy(centroids), cp.asnumpy(counts)\n",
        "\n",
        "# CPU optimized version - unchanged from original\n",
        "@njit(parallel=True)\n",
        "def _fast_distances_cpu(X, centroids):\n",
        "    \"\"\"Compute squared Euclidean distances with Numba - vectorized version.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms - parallelized\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float64)\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "\n",
        "    for i in prange(n_samples):\n",
        "        for k in range(n_features):\n",
        "            x_norms[i] += X[i, k] * X[i, k]\n",
        "\n",
        "    for j in prange(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Compute distances in parallel\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j] - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True)\n",
        "def _assign_labels_cpu(distances):\n",
        "    \"\"\"Assign labels to data points based on nearest centroid.\"\"\"\n",
        "    n_samples = distances.shape[0]\n",
        "    labels = np.zeros(n_samples, dtype=np.int32)\n",
        "    min_distances = np.full(n_samples, np.inf)\n",
        "\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(distances.shape[1]):\n",
        "            if distances[i, j] < min_distances[i]:\n",
        "                min_distances[i] = distances[i, j]\n",
        "                labels[i] = j\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "@njit(parallel=True)\n",
        "def _update_centroids_cpu(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count cluster members and sum values\n",
        "    for i in range(n_samples):\n",
        "        cluster_id = labels[i]\n",
        "        counts[cluster_id] += 1\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Calculate means\n",
        "    for i in prange(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "# Unified interface\n",
        "def _fast_distances(X, centroids):\n",
        "    if HAS_GPU:\n",
        "        return compute_distances(X, centroids)\n",
        "    else:\n",
        "        return _fast_distances_cpu(X, centroids)\n",
        "\n",
        "def _assign_labels_numba(distances):\n",
        "    if HAS_GPU:\n",
        "        return assign_labels(distances)\n",
        "    else:\n",
        "        return _assign_labels_cpu(distances)\n",
        "\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    if HAS_GPU:\n",
        "        return _update_centroids_gpu(X, labels, n_clusters)\n",
        "    else:\n",
        "        return _update_centroids_cpu(X, labels, n_clusters)\n",
        "\n",
        "# Main kmeans function\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    #scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS9Uu76ZUbY5",
        "outputId": "25e4ba14-4f80-4a7d-beed-ec3ade63e08c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 0.668 seconds\n",
            "Average execution time (OptimizedKMeans): 0.022 seconds\n",
            "Average execution time (SklearnKMeans): 0.038 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kINDRJY7eTl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fastest is above this\n"
      ],
      "metadata": {
        "id": "Pkb5AXg1XS8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cProfile, pstats\n",
        "\n",
        "# Profile your function\n",
        "cProfile.run('run_bench_evaluation()', 'profile_output')\n",
        "\n",
        "# Load the profiling data\n",
        "p = pstats.Stats('profile_output')\n",
        "p.strip_dirs().sort_stats('cumulative').print_stats(15)  # Top 10 functions by cumulative time\n",
        "\n",
        "# For a recursive breakdown, check the callers and callees of a specific function:\n",
        "p.print_callers('run_bench_evaluation')\n",
        "p.print_callees('run_bench_evaluation')\n"
      ],
      "metadata": {
        "id": "KQjjcKUweUhh",
        "outputId": "44853329-810e-4028-da3f-367f6ac245c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-09d11110d699>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Profile your function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcProfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run_bench_evaluation()'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'profile_output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the profiling data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/cProfile.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(statement, filename, sort)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pyprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Utils\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrunctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/profile.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, statement, filename, sort)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mprof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/cProfile.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0m__main__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrunctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/cProfile.py\u001b[0m in \u001b[0;36mrunctx\u001b[0;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5112399303c8>\u001b[0m in \u001b[0;36mrun_bench_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1508\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bottomup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbu_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         bu_metrics = {\n\u001b[0;32m-> 1510\u001b[0;31m             \u001b[0;34m'silhouette'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbu_kmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m             \u001b[0;34m'calinski_harabasz'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcalinski_harabasz_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbu_kmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0;34m'davies_bouldin'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdavies_bouldin_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbu_kmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0m_silhouette_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_freqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_freqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     )\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpairwise_distances_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m         \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2253\u001b[0m         if (X is Y or Y is None) and PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001b[1;32m   2254\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2478\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_euclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_norm_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_norm_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     if (\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36missparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m     \"\"\"Is `x` of a sparse array or sparse matrix type?\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p.strip_dirs().sort_stats('cumulative').print_stats(100)\n",
        "p.print_callers('run_bench_evaluation')\n",
        "p.print_callees('run_bench_evaluation')\n"
      ],
      "metadata": {
        "id": "K7kk8jfdfO6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import warnings\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (silhouette_score, calinski_harabasz_score,\n",
        "                             davies_bouldin_score, adjusted_rand_score,\n",
        "                             adjusted_mutual_info_score)\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit, prange, config, set_num_threads\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "# Try to import CuPy for GPU acceleration\n",
        "try:\n",
        "    import cupy as cp\n",
        "    from cupyx.scipy import sparse as cp_sparse\n",
        "    HAS_GPU = True\n",
        "except ImportError:\n",
        "    HAS_GPU = False\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# GPU and CPU optimized functions (adapted from gpubottomup.py :contentReference[oaicite:4]{index=4})\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "if HAS_GPU:\n",
        "    def _fast_distances_gpu(X, centroids):\n",
        "        # Use float32 for better GPU performance\n",
        "        X_gpu = cp.asarray(X, dtype=np.float32)\n",
        "        centroids_gpu = cp.asarray(centroids, dtype=np.float32)\n",
        "        n_samples = X_gpu.shape[0]\n",
        "        n_clusters = centroids_gpu.shape[0]\n",
        "        distances = cp.empty((n_samples, n_clusters), dtype=np.float32)\n",
        "        batch_size = 10000  # adjust batch size as needed\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            end_idx = min(i + batch_size, n_samples)\n",
        "            batch = X_gpu[i:end_idx]\n",
        "            X_norm = cp.sum(batch * batch, axis=1, keepdims=True)\n",
        "            centroids_norm = cp.sum(centroids_gpu * centroids_gpu, axis=1, keepdims=True).T\n",
        "            dot_product = cp.dot(batch, centroids_gpu.T)\n",
        "            distances[i:end_idx] = X_norm + centroids_norm - 2.0 * dot_product\n",
        "        return distances\n",
        "\n",
        "    def assign_labels(distances):\n",
        "        distances_gpu = cp.asarray(distances, dtype=np.float32)\n",
        "        min_distances = cp.min(distances_gpu, axis=1)\n",
        "        labels = cp.argmin(distances_gpu, axis=1)\n",
        "        return cp.asnumpy(labels).astype(np.int32), cp.asnumpy(min_distances)\n",
        "\n",
        "    def _update_centroids_gpu(X, labels, n_clusters):\n",
        "        X_gpu = cp.asarray(X, dtype=np.float32)\n",
        "        labels_gpu = cp.asarray(labels, dtype=np.int32)\n",
        "        n_features = X.shape[1]\n",
        "        centroids = cp.zeros((n_clusters, n_features), dtype=cp.float32)\n",
        "        counts = cp.zeros(n_clusters, dtype=cp.int32)\n",
        "        for k in range(n_clusters):\n",
        "            mask = (labels_gpu == k)\n",
        "            cluster_points = X_gpu[mask]\n",
        "            if cluster_points.shape[0] > 0:\n",
        "                centroids[k] = cp.mean(cluster_points, axis=0)\n",
        "                counts[k] = cluster_points.shape[0]\n",
        "        return cp.asnumpy(centroids), cp.asnumpy(counts)\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _fast_distances_cpu(X, centroids):\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float32)\n",
        "    x_norms = np.empty(n_samples, dtype=np.float32)\n",
        "    centroid_norms = np.empty(n_clusters, dtype=np.float32)\n",
        "    for i in prange(n_samples):\n",
        "        s = 0.0\n",
        "        for j in range(n_features):\n",
        "            s += X[i, j] * X[i, j]\n",
        "        x_norms[i] = s\n",
        "    for k in prange(n_clusters):\n",
        "        s = 0.0\n",
        "        for j in range(n_features):\n",
        "            s += centroids[k, j] * centroids[k, j]\n",
        "        centroid_norms[k] = s\n",
        "    for i in prange(n_samples):\n",
        "        for k in range(n_clusters):\n",
        "            dot = 0.0\n",
        "            for j in range(n_features):\n",
        "                dot += X[i, j] * centroids[k, j]\n",
        "            distances[i, k] = x_norms[i] + centroid_norms[k] - 2.0 * dot\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _assign_labels_cpu(distances):\n",
        "    n_samples = distances.shape[0]\n",
        "    n_clusters = distances.shape[1]\n",
        "    labels = np.empty(n_samples, dtype=np.int32)\n",
        "    min_distances = np.empty(n_samples, dtype=np.float32)\n",
        "    for i in prange(n_samples):\n",
        "        min_val = 1e10\n",
        "        min_idx = 0\n",
        "        for k in range(n_clusters):\n",
        "            if distances[i, k] < min_val:\n",
        "                min_val = distances[i, k]\n",
        "                min_idx = k\n",
        "        labels[i] = min_idx\n",
        "        min_distances[i] = min_val\n",
        "    return labels, min_distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _update_centroids_cpu(X, labels, n_clusters):\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "    for i in range(n_samples):\n",
        "        c = labels[i]\n",
        "        counts[c] += 1\n",
        "        for j in range(n_features):\n",
        "            centroids[c, j] += X[i, j]\n",
        "    for k in prange(n_clusters):\n",
        "        if counts[k] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[k, j] /= counts[k]\n",
        "    return centroids, counts\n",
        "\n",
        "def _fast_distances(X, centroids):\n",
        "    if HAS_GPU:\n",
        "        # For very large datasets, you might add additional logic here\n",
        "        distances_gpu = _fast_distances_gpu(X, centroids)\n",
        "        return cp.asnumpy(distances_gpu)\n",
        "    else:\n",
        "        return _fast_distances_cpu(X, centroids)\n",
        "\n",
        "def _assign_labels_numba(distances):\n",
        "    if HAS_GPU and distances.shape[0] > 10000:\n",
        "        return assign_labels(distances)\n",
        "    else:\n",
        "        return _assign_labels_cpu(distances)\n",
        "\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    if HAS_GPU and X.shape[0] > 10000:\n",
        "        return _update_centroids_gpu(X, labels, n_clusters)\n",
        "    else:\n",
        "        return _update_centroids_cpu(X, labels, n_clusters)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# GPU-optimized HybridBottomUpKMeans class (integrated from gpubottomup.py :contentReference[oaicite:5]{index=5})\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "        if n_jobs is not None and n_jobs > 0:\n",
        "            set_num_threads(n_jobs)\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        if sparse.issparse(X):\n",
        "            sparsity = 1.0 - (X.count_nonzero() / (n_samples * n_features))\n",
        "        else:\n",
        "            if n_samples > 10000:\n",
        "                sample = X[np.random.choice(n_samples, 1000, replace=False)]\n",
        "                sparsity = np.sum(sample == 0) / sample.size\n",
        "            else:\n",
        "                sparsity = np.sum(X == 0) / X.size\n",
        "        is_large = n_samples > 50000\n",
        "        is_high_dim = n_features > 100\n",
        "        if is_large:\n",
        "            self._batch_size_factor = 0.02 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "        elif is_high_dim:\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "        else:\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "        self._hybrid_threshold = self.hybrid_threshold\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data: samples={n_samples}, features={n_features}, initial_batch={self._initial_batch_size}, growth_factor={self._batch_growth_factor}, hybrid_threshold={self._hybrid_threshold}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "        if self.init == 'random':\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            return X[indices].toarray() if sparse.issparse(X) else X[indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].toarray().flatten() if sparse.issparse(X) else X[first_idx].copy()\n",
        "            if n_samples > 10000:\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "                X_sample = X[sample_indices].toarray() if sparse.issparse(X) else X[sample_indices]\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    min_dists = np.zeros(X_sample.shape[0])\n",
        "                    for i in range(X_sample.shape[0]):\n",
        "                        min_val = np.inf\n",
        "                        for k in range(c):\n",
        "                            dist = np.sum((X_sample[i] - centroids[k]) ** 2)\n",
        "                            if dist < min_val:\n",
        "                                min_val = dist\n",
        "                        min_dists[i] = min_val\n",
        "                    sum_dists = min_dists.sum()\n",
        "                    if sum_dists > 0:\n",
        "                        probs = min_dists / sum_dists\n",
        "                        next_idx = random_state.choice(sample_size, p=probs)\n",
        "                        centroids[c] = X_sample[next_idx].copy()\n",
        "                    else:\n",
        "                        next_idx = random_state.randint(sample_size)\n",
        "                        centroids[c] = X_sample[next_idx].copy()\n",
        "            else:\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = np.zeros(n_samples)\n",
        "                        for i in range(n_samples):\n",
        "                            min_val = np.inf\n",
        "                            for k in range(c):\n",
        "                                dist = np.sum((X_dense[i] - centroids[k]) ** 2)\n",
        "                                if dist < min_val:\n",
        "                                    min_val = dist\n",
        "                            min_dists[i] = min_val\n",
        "                        sum_dists = min_dists.sum()\n",
        "                        if sum_dists > 0:\n",
        "                            probs = min_dists / sum_dists\n",
        "                            next_idx = random_state.choice(n_samples, p=probs)\n",
        "                            centroids[c] = X_dense[next_idx].copy()\n",
        "                        else:\n",
        "                            next_idx = random_state.randint(n_samples)\n",
        "                            centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = np.zeros(n_samples)\n",
        "                        for i in range(n_samples):\n",
        "                            min_val = np.inf\n",
        "                            for k in range(c):\n",
        "                                dist = np.sum((X[i] - centroids[k]) ** 2)\n",
        "                                if dist < min_val:\n",
        "                                    min_val = dist\n",
        "                            min_dists[i] = min_val\n",
        "                        sum_dists = min_dists.sum()\n",
        "                        if sum_dists > 0:\n",
        "                            probs = min_dists / sum_dists\n",
        "                            next_idx = random_state.choice(n_samples, p=probs)\n",
        "                            centroids[c] = X[next_idx].copy()\n",
        "                        else:\n",
        "                            next_idx = random_state.randint(n_samples)\n",
        "                            centroids[c] = X[next_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "        if sparse.issparse(X):\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "        blocks = [(i * block_size, min(i * block_size + block_size, n_samples)) for i in range(n_blocks)]\n",
        "        results = Parallel(n_jobs=n_jobs)(delayed(_fast_distances_cpu)(X[start:end], centroids) for start, end in blocks)\n",
        "        return np.vstack(results)\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        n_samples = X.shape[0]\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "        batch_indices = []\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (inactive_labels == k)\n",
        "            cluster_indices = inactive_indices[cluster_mask]\n",
        "            if len(cluster_indices) > 0:\n",
        "                cluster_dists = distances[cluster_indices][:, k]\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest_idx = np.argsort(cluster_dists)[:num_to_take]\n",
        "                batch_indices.extend(cluster_indices[closest_idx])\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_idx = np.argsort(margins)[:num_boundary]\n",
        "            batch_indices.extend(inactive_indices[boundary_idx])\n",
        "        min_dists = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_idx = np.argsort(-min_dists)[:num_outliers]\n",
        "        batch_indices.extend(inactive_indices[outlier_idx])\n",
        "        batch_indices = list(set(batch_indices))\n",
        "        if len(batch_indices) > batch_size:\n",
        "            batch_distances = np.min(distances[batch_indices], axis=1)\n",
        "            priority_idx = np.argsort(batch_distances)[:batch_size]\n",
        "            batch_indices = [batch_indices[i] for i in priority_idx]\n",
        "        if len(batch_indices) < batch_size:\n",
        "            remaining = batch_size - len(batch_indices)\n",
        "            available = list(set(inactive_indices) - set(batch_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available, size=min(remaining, len(available)), replace=False)\n",
        "                batch_indices.extend(random_indices)\n",
        "        return np.array(batch_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        if NUMBA_AVAILABLE:\n",
        "            labels, min_distances = _assign_labels_numba(distances)\n",
        "        else:\n",
        "            labels = np.argmin(distances, axis=1)\n",
        "            min_distances = np.min(distances, axis=1)\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "            new_centroids, counts = _update_centroids_numba(X, labels, self.n_clusters)\n",
        "            for k in range(self.n_clusters):\n",
        "                if counts[k] == 0:\n",
        "                    new_centroids[k] = centroids[k]\n",
        "        else:\n",
        "            for k in range(self.n_clusters):\n",
        "                cluster_mask = (labels == k)\n",
        "                if np.any(cluster_mask):\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "                else:\n",
        "                    new_centroids[k] = centroids[k]\n",
        "        inertia = np.sum(min_distances)\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids) ** 2))\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "        self._set_dynamic_parameters(X)\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        if NUMBA_AVAILABLE:\n",
        "            labels, _ = _assign_labels_numba(distances)\n",
        "        else:\n",
        "            labels = np.argmin(distances, axis=1)\n",
        "        initial_indices = []\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            cluster_points = np.where(cluster_mask)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_dists = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest_idx = np.argsort(cluster_dists)[:num_to_take]\n",
        "                initial_indices.extend(cluster_points[closest_idx])\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_idx = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_idx)\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                if len(active_indices) > 0:\n",
        "                    X_active = X[active_indices].toarray() if sparse.issparse(X) else X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                new_centroids[k] = old_centroids[k] if iteration > 0 else X_active[np.random.randint(len(X_active))]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                new_centroids[k] = old_centroids[k] if iteration > 0 else X_active[np.random.randint(len(X_active))]\n",
        "                    centroids = new_centroids\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    new_labels, min_distances = _assign_labels_numba(distances)\n",
        "                else:\n",
        "                    new_labels = np.argmin(distances, axis=1)\n",
        "                    min_distances = np.min(distances, axis=1)\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5\n",
        "                next_batch_size = min(int(self._initial_batch_size * (growth_factor ** iteration)), n_samples - len(active_indices))\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids) ** 2))\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {'iteration': n_iter, 'phase': 'bottom-up', 'active_points': len(active_indices),\n",
        "                                  'coverage': len(active_indices) / n_samples * 100, 'active_changed': active_changed,\n",
        "                                  'active_changed_pct': active_changed_pct * 100, 'centroid_shift': centroid_shift,\n",
        "                                  'new_points_added': len(new_batch), 'inertia': active_inertia, 'time': iter_time}\n",
        "                iteration_table.append(iteration_info)\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices) / n_samples * 100:.1f}% points, {active_changed_pct * 100:.1f}% changed, {iter_time:.3f}s\")\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    break\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "            else:\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {'iteration': n_iter, 'phase': 'standard', 'active_points': n_samples,\n",
        "                                  'coverage': 100.0, 'active_changed': np.nan, 'active_changed_pct': np.nan,\n",
        "                                  'centroid_shift': centroid_shift, 'new_points_added': 0, 'inertia': inertia, 'time': iter_time}\n",
        "                iteration_table.append(iteration_info)\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "                prev_inertia = inertia\n",
        "        if len(active_indices) / n_samples < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "        total_time = time.time() - start_time\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter + 1}\")\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i + 1}/{len(seeds)}\")\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, self.random_state)\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iter_table\n",
        "        return self\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        self.fit(X)\n",
        "        return self.labels_\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=True)\n",
        "        if HAS_GPU and X.shape[0] > 10000:\n",
        "            batch_size = 10000\n",
        "            n_samples = X.shape[0]\n",
        "            labels = np.empty(n_samples, dtype=np.int32)\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                end = min(i + batch_size, n_samples)\n",
        "                X_batch = X[i:end]\n",
        "                distances = _fast_distances(X_batch, self.cluster_centers_)\n",
        "                batch_labels, _ = _assign_labels_numba(distances)\n",
        "                labels[i:end] = batch_labels\n",
        "            return labels\n",
        "        else:\n",
        "            if NUMBA_AVAILABLE:\n",
        "                distances = _fast_distances(X, self.cluster_centers_)\n",
        "                labels, _ = _assign_labels_numba(distances)\n",
        "                return labels\n",
        "            else:\n",
        "                distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "                return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            if 'time' in df.columns and 'active_points' in df.columns:\n",
        "                df['throughput'] = df['active_points'] / df['time']\n",
        "                df['cumulative_time'] = df['time'].cumsum()\n",
        "            df.name = \"Iteration History (GPU optimized)\"\n",
        "            if 'phase' in df.columns:\n",
        "                df['phase'] = df['phase'].str.capitalize()\n",
        "            for col in df.select_dtypes(include=['float']).columns:\n",
        "                df[col] = df[col].round(3)\n",
        "            return df\n",
        "        except ImportError:\n",
        "            print(\"Iteration History:\")\n",
        "            for i, info in enumerate(self.iteration_table_):\n",
        "                print(f\"Iteration {i + 1}: \" + \", \".join([f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\" for k, v in info.items()]))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Benchmark and Evaluation Functions (adapted from bottomup.py :contentReference[oaicite:6]{index=6})\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3):\n",
        "    from sklearn.datasets import make_blobs\n",
        "    X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=42)\n",
        "    return X, y\n",
        "\n",
        "def load_wine_data():\n",
        "    from sklearn.datasets import load_wine\n",
        "    data = load_wine()\n",
        "    df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "    df['target'] = data.target\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df):\n",
        "    X = df.drop('target', axis=1).values\n",
        "    y = df['target'].values\n",
        "    return X, {'original_shape': df.shape, 'cleaned_shape': X.shape}, y\n",
        "\n",
        "def print_results(results, y_true_available=False):\n",
        "    print(\"Benchmark Results:\")\n",
        "    for method in results['times']:\n",
        "        times = results['times'][method]\n",
        "        print(f\"{method}: Avg Time = {np.mean(times):.3f}s, Avg Iterations = {np.mean(results['iterations'][method]):.1f}\")\n",
        "        metrics = results['metrics'][method]\n",
        "        print(\"Metrics:\")\n",
        "        for m in metrics:\n",
        "            print(m)\n",
        "\n",
        "def visualize_results(X, results, optim_k_results, preprocessing_info, y_true=None):\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        from sklearn.decomposition import PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=results['labels'], cmap='viridis', alpha=0.6)\n",
        "        plt.title(\"Cluster Visualization\")\n",
        "        plt.xlabel(\"PCA 1\")\n",
        "        plt.ylabel(\"PCA 2\")\n",
        "        plt.show()\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib not installed. Skipping visualization.\")\n",
        "\n",
        "def run_bench_evaluation():\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BENCHMARK EVALUATION: HybridBottomUpKMeans vs SklearnKMeans\")\n",
        "    print(\"=\" * 80)\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "    results = {'times': {'hybrid': [], 'sklearn': []},\n",
        "               'metrics': {'hybrid': [], 'sklearn': []},\n",
        "               'iterations': {'hybrid': [], 'sklearn': []},\n",
        "               'labels': None}\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}/{n_runs}\")\n",
        "        start_time = time.time()\n",
        "        hybrid_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42 + run, verbose=True)\n",
        "        hybrid_kmeans.fit(X)\n",
        "        hybrid_time = time.time() - start_time\n",
        "        results['times']['hybrid'].append(hybrid_time)\n",
        "        hybrid_metrics = {\n",
        "            'silhouette': silhouette_score(X, hybrid_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, hybrid_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, hybrid_kmeans.labels_),\n",
        "            'inertia': hybrid_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            hybrid_metrics['adjusted_rand'] = adjusted_rand_score(y_true, hybrid_kmeans.labels_)\n",
        "            hybrid_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, hybrid_kmeans.labels_)\n",
        "        results['metrics']['hybrid'].append(hybrid_metrics)\n",
        "        results['iterations']['hybrid'].append(hybrid_kmeans.n_iter_)\n",
        "        if run == 0:\n",
        "            print(\"\\nIteration table for first run:\")\n",
        "            print(hybrid_kmeans.print_iteration_table())\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42 + run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "    results['labels'] = hybrid_kmeans.labels_\n",
        "    print(\"\\nFinal Benchmark Results:\")\n",
        "    print(f\"HybridBottomUpKMeans Avg Time: {np.mean(results['times']['hybrid']):.3f}s\")\n",
        "    print(f\"SklearnKMeans Avg Time: {np.mean(results['times']['sklearn']):.3f}s\")\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Main entry point\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the benchmark evaluation; additional evaluations (e.g. on wine data) can be added similarly.\n",
        "    bench_results = run_bench_evaluation()\n"
      ],
      "metadata": {
        "id": "o9Vsn5pgC2ng",
        "outputId": "42492858-dc00-4a74-eec0-1ec941a69a25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: HybridBottomUpKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Iteration table for first run:\n",
            "   iteration      phase  active_points  coverage  active_changed  \\\n",
            "0          1  Bottom-up           2249     7.497           392.0   \n",
            "1          2  Bottom-up           5249    17.497           401.0   \n",
            "2          3  Bottom-up          11249    37.497           210.0   \n",
            "3          4   Standard          30000   100.000             NaN   \n",
            "4          5   Standard          30000   100.000             NaN   \n",
            "5          6   Standard          30000   100.000             NaN   \n",
            "6          7   Standard          30000   100.000             NaN   \n",
            "\n",
            "   active_changed_pct  centroid_shift  new_points_added        inertia   time  \\\n",
            "0              17.430           2.128              3000    5981.881836  0.027   \n",
            "1               7.640           3.352              6000   64804.453125  0.032   \n",
            "2               1.867           1.190             12000  169974.281250  0.042   \n",
            "3                 NaN           0.535                 0  718310.625000  0.006   \n",
            "4                 NaN           0.086                 0  713561.500000  0.006   \n",
            "5                 NaN           0.056                 0  713501.625000  0.006   \n",
            "6                 NaN           0.038                 0  713474.875000  0.006   \n",
            "\n",
            "    throughput  cumulative_time  \n",
            "0    84545.452            0.027  \n",
            "1   163789.294            0.059  \n",
            "2   264725.301            0.101  \n",
            "3  4868982.703            0.107  \n",
            "4  5008522.867            0.113  \n",
            "5  5356026.050            0.119  \n",
            "6  5218526.875            0.125  \n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Final Benchmark Results:\n",
            "HybridBottomUpKMeans Avg Time: 7.135s\n",
            "SklearnKMeans Avg Time: 0.070s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "from numba import njit, prange, float64, int32\n",
        "from numba import njit, prange, cuda\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "try:\n",
        "    import cupy as cp\n",
        "    from cupyx.scipy import sparse as cp_sparse\n",
        "    HAS_GPU = True\n",
        "except ImportError:\n",
        "    HAS_GPU = False\n",
        "\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import logging\n",
        "from numba import njit, prange, config, set_num_threads\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configure Numba for optimal performance\n",
        "config.THREADING_LAYER = 'threadsafe'\n",
        "NUMBA_AVAILABLE = True  # Assume Numba is available\n",
        "\n",
        "# GPU optimized functions\n",
        "if HAS_GPU:\n",
        "    # Improved GPU distance calculation with memory optimization\n",
        "    def _fast_distances_gpu(X, centroids):\n",
        "        \"\"\"Optimized Euclidean distance computation with CuPy.\"\"\"\n",
        "        # Transfer data to GPU if needed (only once)\n",
        "        X_gpu = cp.asarray(X, dtype=np.float32)  # Use float32 for better GPU performance\n",
        "        centroids_gpu = cp.asarray(centroids, dtype=np.float32)\n",
        "\n",
        "        # Pre-allocate output array\n",
        "        n_samples = X_gpu.shape[0]\n",
        "        n_clusters = centroids_gpu.shape[0]\n",
        "        distances = cp.empty((n_samples, n_clusters), dtype=np.float32)\n",
        "\n",
        "        # Compute in batches to avoid GPU memory issues on large datasets\n",
        "        batch_size = 10000  # Adjust based on your GPU memory\n",
        "\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            end_idx = min(i + batch_size, n_samples)\n",
        "            batch = X_gpu[i:end_idx]\n",
        "\n",
        "            # Compute squared norms (use inplace operations where possible)\n",
        "            X_norm = cp.sum(batch * batch, axis=1, keepdims=True)\n",
        "            centroids_norm = cp.sum(centroids_gpu * centroids_gpu, axis=1, keepdims=True).T\n",
        "\n",
        "            # Use matrix multiplication for dot product\n",
        "            dot_product = cp.dot(batch, centroids_gpu.T)\n",
        "\n",
        "            # Compute distances using ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
        "            distances[i:end_idx] = X_norm + centroids_norm - 2.0 * dot_product\n",
        "\n",
        "        return distances\n",
        "\n",
        "    # Optimized label assignment on GPU\n",
        "    def assign_labels(distances):\n",
        "        \"\"\"Optimized label assignment using GPU.\"\"\"\n",
        "        distances_gpu = cp.asarray(distances, dtype=np.float32)\n",
        "        min_distances = cp.min(distances_gpu, axis=1)\n",
        "        labels = cp.argmin(distances_gpu, axis=1)\n",
        "\n",
        "        # Transfer results back to CPU at once to minimize transfers\n",
        "        return cp.asnumpy(labels).astype(np.int32), cp.asnumpy(min_distances)\n",
        "\n",
        "    # Streamlined centroid update on GPU\n",
        "    def _update_centroids_gpu(X, labels, n_clusters):\n",
        "        \"\"\"Optimized GPU centroid update using atomics and parallel reduction.\"\"\"\n",
        "        X_gpu = cp.asarray(X, dtype=np.float32)\n",
        "        labels_gpu = cp.asarray(labels, dtype=np.int32)\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Use one-hot encoding for efficient parallel reduction\n",
        "        # This is faster for large datasets than iterating through clusters\n",
        "        centroids = cp.zeros((n_clusters, n_features), dtype=np.float32)\n",
        "        counts = cp.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "        # Custom CUDA kernel for parallel reduction would be ideal here\n",
        "        # For now, use a simpler approach with existing CuPy functions\n",
        "        for k in range(n_clusters):\n",
        "            mask = (labels_gpu == k)\n",
        "            cluster_points = X_gpu[mask]\n",
        "            cluster_size = cp.sum(mask)\n",
        "\n",
        "            if cluster_size > 0:\n",
        "                # Use parallel reduction for sum\n",
        "                centroids[k] = cp.sum(cluster_points, axis=0) / cluster_size\n",
        "                counts[k] = cluster_size\n",
        "\n",
        "        # Transfer results back to CPU at once\n",
        "        return cp.asnumpy(centroids), cp.asnumpy(counts)\n",
        "\n",
        "    # GPU sparse matrix support\n",
        "    def _handle_sparse_gpu(X, centroids=None):\n",
        "        \"\"\"Convert sparse matrices to GPU format efficiently.\"\"\"\n",
        "        if not sparse.issparse(X):\n",
        "            return cp.asarray(X, dtype=np.float32)\n",
        "\n",
        "        # Convert to CSR for efficient GPU transfer\n",
        "        X_csr = X.tocsr() if not isinstance(X, sparse.csr_matrix) else X\n",
        "        X_gpu = cp_sparse.csr_matrix(X_csr)\n",
        "\n",
        "        return X_gpu\n",
        "\n",
        "# Optimized CPU functions with Numba\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _fast_distances_cpu(X, centroids):\n",
        "    \"\"\"Highly optimized squared Euclidean distances with Numba.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float32)\n",
        "\n",
        "    # Pre-compute squared norms with better vectorization\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float32)\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float32)\n",
        "\n",
        "    # Vectorized norm computation\n",
        "    for i in prange(n_samples):\n",
        "        x_norms[i] = np.sum(X[i] * X[i])\n",
        "\n",
        "    for j in prange(n_clusters):\n",
        "        centroid_norms[j] = np.sum(centroids[j] * centroids[j])\n",
        "\n",
        "    # Optimize cache locality by computing all distances for one sample at a time\n",
        "    for i in prange(n_samples):\n",
        "        x = X[i]  # Cache the row for better memory access\n",
        "        for j in range(n_clusters):\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += x[k] * centroids[j, k]\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j] - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _assign_labels_cpu(distances):\n",
        "    \"\"\"Optimized label assignment with Numba.\"\"\"\n",
        "    n_samples = distances.shape[0]\n",
        "    n_clusters = distances.shape[1]\n",
        "    labels = np.zeros(n_samples, dtype=np.int32)\n",
        "    min_distances = np.zeros(n_samples, dtype=np.float32)\n",
        "\n",
        "    # Use parallelization with better memory access\n",
        "    for i in prange(n_samples):\n",
        "        min_dist = np.inf\n",
        "        min_idx = 0\n",
        "        for j in range(n_clusters):\n",
        "            if distances[i, j] < min_dist:\n",
        "                min_dist = distances[i, j]\n",
        "                min_idx = j\n",
        "        labels[i] = min_idx\n",
        "        min_distances[i] = min_dist\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _update_centroids_cpu(X, labels, n_clusters):\n",
        "    \"\"\"Optimized centroid update with Numba.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    # Pre-allocate arrays for better memory management\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count cluster members and sum values - parallelize by cluster\n",
        "    for i in range(n_samples):\n",
        "        cluster_id = labels[i]\n",
        "        counts[cluster_id] += 1\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Calculate means - parallelize by cluster\n",
        "    for i in prange(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            inv_count = 1.0 / counts[i]\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] *= inv_count\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "# Optimized block processing for parallel distance computation\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def _fast_distances_block(X, centroids, start, end):\n",
        "    \"\"\"Process a block of data for parallel distance computation.\"\"\"\n",
        "    X_block = X[start:end]\n",
        "    return _fast_distances_cpu(X_block, centroids)\n",
        "\n",
        "# Unified optimized interface\n",
        "def _fast_distances(X, centroids):\n",
        "    \"\"\"Optimized distance computation with automatic CPU/GPU selection.\"\"\"\n",
        "    if HAS_GPU:\n",
        "        # Use batched GPU implementation for very large datasets\n",
        "        if X.shape[0] > 100000:\n",
        "            return compute_distances(X, centroids)\n",
        "        else:\n",
        "            # For smaller datasets, transfer everything at once\n",
        "            distances_gpu = _fast_distances_gpu(X, centroids)\n",
        "            return cp.asnumpy(distances_gpu)\n",
        "    else:\n",
        "        return _fast_distances_cpu(X, centroids)\n",
        "\n",
        "def _assign_labels_numba(distances):\n",
        "    \"\"\"Optimized label assignment with GPU/CPU selection.\"\"\"\n",
        "    if HAS_GPU and distances.shape[0] > 10000:  # Only use GPU for larger datasets\n",
        "        return assign_labels(distances)\n",
        "    else:\n",
        "        return _assign_labels_cpu(distances)\n",
        "\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    \"\"\"Optimized centroid update with GPU/CPU selection.\"\"\"\n",
        "    if HAS_GPU and X.shape[0] > 10000:  # Only use GPU for larger datasets\n",
        "        return _update_centroids_gpu(X, labels, n_clusters)\n",
        "    else:\n",
        "        return _update_centroids_cpu(X, labels, n_clusters)\n",
        "\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation with hybrid bottom-up approach.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "        # Set Numba threading if n_jobs is specified\n",
        "        if n_jobs is not None and n_jobs > 0:\n",
        "            set_num_threads(n_jobs)\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Optimized parameter selection based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Fast analysis based on size and simple sampling\n",
        "        is_large = n_samples > 50000\n",
        "        is_high_dim = n_features > 100\n",
        "\n",
        "        # Calculate sparsity without full matrix computation\n",
        "        if sparse.issparse(X):\n",
        "            sparsity = 1.0 - (X.count_nonzero() / (n_samples * n_features))\n",
        "        else:\n",
        "            # Sample a subset for large datasets\n",
        "            if n_samples > 10000:\n",
        "                random_idx = np.random.choice(n_samples, 1000, replace=False)\n",
        "                sample = X[random_idx]\n",
        "                sparsity = np.sum(sample == 0) / sample.size\n",
        "            else:\n",
        "                sparsity = np.sum(X == 0) / X.size\n",
        "\n",
        "        is_sparse = sparsity > 0.5\n",
        "\n",
        "        # Optimize parameters based on data characteristics\n",
        "        if is_large:\n",
        "            self._batch_size_factor = 0.02 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "        elif is_high_dim:\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "        else:\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "\n",
        "        self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for very small datasets\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data characteristics: samples={n_samples}, features={n_features}, \"\n",
        "                      f\"sparsity={sparsity:.2f}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                      f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                      f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Optimized centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ with sampling for large datasets\n",
        "            is_large = n_samples > 10000\n",
        "\n",
        "            # Initialize array for centroids\n",
        "            if sparse.issparse(X):\n",
        "                dtype = X.dtype\n",
        "            else:\n",
        "                dtype = X.dtype\n",
        "\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, use sampling with lower memory footprint\n",
        "            if is_large:\n",
        "                # Use a smaller sample for initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample with optimized distance computation\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate squared distances to closest centroid efficiently\n",
        "                    if HAS_GPU and X_sample.shape[0] > 5000:\n",
        "                        # Use GPU for larger samples\n",
        "                        X_gpu = cp.asarray(X_sample)\n",
        "                        centroids_gpu = cp.asarray(centroids[:c])\n",
        "\n",
        "                        # Compute distances efficiently\n",
        "                        distances = cp.zeros((X_sample.shape[0], c))\n",
        "                        for k in range(c):\n",
        "                            diff = X_gpu - centroids_gpu[k]\n",
        "                            distances[:, k] = cp.sum(diff * diff, axis=1)\n",
        "\n",
        "                        min_dists = cp.min(distances, axis=1).get()\n",
        "                    else:\n",
        "                        # Use vectorized CPU computation\n",
        "                        min_dists = np.zeros(X_sample.shape[0])\n",
        "                        for i in range(X_sample.shape[0]):\n",
        "                            min_dist = float('inf')\n",
        "                            for k in range(c):\n",
        "                                dist = np.sum((X_sample[i] - centroids[k]) ** 2)\n",
        "                                if dist < min_dist:\n",
        "                                    min_dist = dist\n",
        "                            min_dists[i] = min_dist\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    sum_dists = min_dists.sum()\n",
        "                    if sum_dists > 0:\n",
        "                        probs = min_dists / sum_dists\n",
        "                        next_idx = random_state.choice(sample_size, p=probs)\n",
        "                        centroids[c] = X_sample[next_idx].copy()\n",
        "                    else:\n",
        "                        # Fallback for numerical issues\n",
        "                        next_idx = random_state.randint(sample_size)\n",
        "                        centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        # Use efficient vectorized distance calculation\n",
        "                        min_dists = np.zeros(n_samples)\n",
        "                        for i in range(n_samples):\n",
        "                            min_dist = float('inf')\n",
        "                            for k in range(c):\n",
        "                                dist = np.sum((X_dense[i] - centroids[k]) ** 2)\n",
        "                                if dist < min_dist:\n",
        "                                    min_dist = dist\n",
        "                            min_dists[i] = min_dist\n",
        "\n",
        "                        sum_dists = min_dists.sum()\n",
        "                        if sum_dists > 0:\n",
        "                            probs = min_dists / sum_dists\n",
        "                            next_idx = random_state.choice(n_samples, p=probs)\n",
        "                            centroids[c] = X_dense[next_idx].copy()\n",
        "                        else:\n",
        "                            next_idx = random_state.randint(n_samples)\n",
        "                            centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        # Use efficient vectorized distance calculation\n",
        "                        if c == 1:\n",
        "                            # First iteration - simple vector operations\n",
        "                            diff = X - centroids[0]\n",
        "                            min_dists = np.sum(diff * diff, axis=1)\n",
        "                        else:\n",
        "                            # Compute only new distances\n",
        "                            new_dists = np.sum((X - centroids[c-1]) ** 2, axis=1)\n",
        "                            min_dists = np.minimum(min_dists, new_dists)\n",
        "\n",
        "                        sum_dists = min_dists.sum()\n",
        "                        if sum_dists > 0:\n",
        "                            probs = min_dists / sum_dists\n",
        "                            next_idx = random_state.choice(n_samples, p=probs)\n",
        "                            centroids[c] = X[next_idx].copy()\n",
        "                        else:\n",
        "                            next_idx = random_state.randint(n_samples)\n",
        "                            centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Highly optimized parallel distance computation.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        # For sparse matrices, use specialized handling\n",
        "        if sparse.issparse(X):\n",
        "            if HAS_GPU:\n",
        "                # Use GPU for sparse computation if available\n",
        "                try:\n",
        "                    X_gpu = _handle_sparse_gpu(X)\n",
        "                    centroids_gpu = cp.asarray(centroids)\n",
        "\n",
        "                    # Use specialized sparse GPU functions\n",
        "                    # For large sparse matrices, process in batches\n",
        "                    if X.shape[0] > 50000:\n",
        "                        batch_size = 10000\n",
        "                        distances = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "\n",
        "                        for i in range(0, X.shape[0], batch_size):\n",
        "                            end = min(i + batch_size, X.shape[0])\n",
        "                            X_batch = X_gpu[i:end]\n",
        "\n",
        "                            # Convert to dense for distance computation (if needed)\n",
        "                            if isinstance(X_batch, cp_sparse.spmatrix):\n",
        "                                X_batch = X_batch.toarray()\n",
        "\n",
        "                            # Compute distances efficiently\n",
        "                            batch_distances = _fast_distances_gpu(X_batch, centroids_gpu)\n",
        "                            distances[i:end] = cp.asnumpy(batch_distances)\n",
        "\n",
        "                        return distances\n",
        "                    else:\n",
        "                        # Convert to dense for smaller matrices\n",
        "                        X_dense = X_gpu.toarray() if isinstance(X_gpu, cp_sparse.spmatrix) else X_gpu\n",
        "                        distances = _fast_distances_gpu(X_dense, centroids_gpu)\n",
        "                        return cp.asnumpy(distances)\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Fallback to CPU if GPU fails\n",
        "                    logger.warning(f\"GPU sparse computation failed, falling back to CPU: {e}\")\n",
        "                    return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "            else:\n",
        "                # CPU sparse computation\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        # For dense matrices, choose optimal implementation based on size\n",
        "        if n_samples < 1000 or n_jobs <= 1:\n",
        "            # Use optimized Numba for small datasets or single-threaded\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        if NUMBA_AVAILABLE:\n",
        "            # Use Numba with parallel processing\n",
        "            # Determine optimal block size based on data size\n",
        "            if n_samples > 100000:\n",
        "                block_size = max(1000, n_samples // (n_jobs * 2))\n",
        "            else:\n",
        "                block_size = max(100, n_samples // n_jobs)\n",
        "\n",
        "            n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "            # Prepare blocks for better load balancing\n",
        "            blocks = []\n",
        "            for i in range(n_blocks):\n",
        "                start_idx = i * block_size\n",
        "                end_idx = min(start_idx + block_size, n_samples)\n",
        "                blocks.append((start_idx, end_idx))\n",
        "\n",
        "            # Process blocks in parallel with joblib\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results efficiently\n",
        "            return np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized strategic batch selection algorithm.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Fast path for edge cases\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask more efficiently using boolean operations\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Optimized batch selection using vectorized operations\n",
        "        batch_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids using vectorized operations\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (inactive_labels == k)\n",
        "            cluster_indices = inactive_indices[cluster_mask]\n",
        "\n",
        "            if len(cluster_indices) > 0:\n",
        "                # Get distances to centroid k\n",
        "                cluster_dists = distances[cluster_indices][:, k]\n",
        "\n",
        "                # Take closest points using argsort (more efficient)\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest_idx = np.argsort(cluster_dists)[:num_to_take]\n",
        "                batch_indices.extend(cluster_indices[closest_idx])\n",
        "\n",
        "        # 2. Take boundary points (vectorized)\n",
        "        if self.n_clusters > 1:\n",
        "            # Get margins between closest and second closest centroids\n",
        "            sorted_idx = np.argsort(inactive_distances, axis=1)\n",
        "            closest_dists = np.take_along_axis(inactive_distances, sorted_idx[:, 0:1], axis=1)\n",
        "            second_closest_dists = np.take_along_axis(inactive_distances, sorted_idx[:, 1:2], axis=1)\n",
        "            margins = second_closest_dists.ravel() - closest_dists.ravel()\n",
        "\n",
        "            # Get points with smallest margins (near boundaries)\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_idx = np.argsort(margins)[:num_boundary]\n",
        "            batch_indices.extend(inactive_indices[boundary_idx])\n",
        "\n",
        "        # 3. Add outliers (vectorized)\n",
        "        min_dists = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_idx = np.argsort(-min_dists)[:num_outliers]\n",
        "        batch_indices.extend(inactive_indices[outlier_idx])\n",
        "\n",
        "        # Ensure uniqueness efficiently using sets\n",
        "        batch_indices = list(set(batch_indices))\n",
        "\n",
        "        # If we have too many points, use the most important ones\n",
        "        if len(batch_indices) > batch_size:\n",
        "            # Prioritize by distance\n",
        "            batch_distances = np.min(distances[batch_indices], axis=1)\n",
        "            priority_idx = np.argsort(batch_distances)[:batch_size]\n",
        "            batch_indices = [batch_indices[i] for i in priority_idx]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(batch_indices) < batch_size:\n",
        "            remaining = batch_size - len(batch_indices)\n",
        "            # Use set difference for efficiency\n",
        "            available = list(set(inactive_indices) - set(batch_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                                size=min(remaining, len(available)),\n",
        "                                                replace=False)\n",
        "                batch_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(batch_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Optimized standard k-means iteration.\"\"\"\n",
        "        # Compute distances efficiently\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "\n",
        "        # Assign labels using optimized function\n",
        "        if NUMBA_AVAILABLE:\n",
        "            labels, min_distances = _assign_labels_numba(distances)\n",
        "        else:\n",
        "            labels = np.argmin(distances, axis=1)\n",
        "            min_distances = np.min(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "\n",
        "        if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "            # Use optimized Numba function for dense data\n",
        "            new_centroids, counts = _update_centroids_numba(X, labels, self.n_clusters)\n",
        "\n",
        "            # Handle empty clusters\n",
        "            for k in range(self.n_clusters):\n",
        "                if counts[k] == 0:\n",
        "                    new_centroids[k] = centroids[k]\n",
        "        else:\n",
        "            # Standard update logic\n",
        "            for k in range(self.n_clusters):\n",
        "                cluster_mask = (labels == k)\n",
        "                if np.any(cluster_mask):\n",
        "                    if sparse.issparse(X):\n",
        "                        new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                    else:\n",
        "                        new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "                else:\n",
        "                    new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(min_distances)\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Optimized hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "\n",
        "        # Initial setup\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels efficiently\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        if NUMBA_AVAILABLE:\n",
        "            labels, _ = _assign_labels_numba(distances)\n",
        "        else:\n",
        "            labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        # Use vectorized operations for efficiency\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points closest to centroids\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            cluster_points = np.where(cluster_mask)[0]\n",
        "\n",
        "            if len(cluster_points) > 0:\n",
        "                # Use vectorized operations\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest_idx = np.argsort(cluster_distances)[:num_to_take]\n",
        "                initial_indices.extend(cluster_points[closest_idx])\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            # Vectorized boundary computation\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_idx = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_idx)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Convergence tracking\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop - optimized\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data efficiently\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using optimized functions\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        # Standard update with vectorized operations\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points efficiently\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "\n",
        "                # Assign labels efficiently\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    new_labels, min_distances = _assign_labels_numba(distances)\n",
        "                else:\n",
        "                    new_labels = np.argmin(distances, axis=1)\n",
        "                    min_distances = np.min(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia efficiently\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth based on stability\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size with bounds\n",
        "                next_batch_size = min(\n",
        "                    int(self._initial_batch_size * (growth_factor ** iteration)),\n",
        "                    n_samples - len(active_indices)\n",
        "                )\n",
        "\n",
        "                # Select next batch strategically\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration stats\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping optimizations\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing when it should\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability with counter\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration with optimizations\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration stats\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration if needed\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Optimized model fitting with multiple initializations for Google Colab T4 environment.\n",
        "        \"\"\"\n",
        "        # Start timing for performance monitoring\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Input validation with optimal data type for T4 GPU\n",
        "        if HAS_GPU:\n",
        "            # For T4 GPU, float32 offers better performance\n",
        "            X = check_array(X, dtype=np.float32, accept_sparse=True)\n",
        "        else:\n",
        "            X = check_array(X, accept_sparse=True)\n",
        "\n",
        "        best_inertia = np.inf\n",
        "        best_centroids = None\n",
        "        best_labels = None\n",
        "        best_n_iter = 0\n",
        "\n",
        "        # Generate seeds efficiently\n",
        "        seeds = self._get_seeds()\n",
        "\n",
        "        # For Colab's 2-core system, limit parallelism and optimize single runs\n",
        "        if self.n_jobs is None or self.n_jobs > 2:\n",
        "            # Override n_jobs for Colab environment\n",
        "            self.n_jobs = 2\n",
        "\n",
        "        # Progress tracking\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Starting K-means with {len(seeds)} initializations on \"\n",
        "                    f\"{'GPU (T4)' if HAS_GPU else 'CPU (2 cores)'}\")\n",
        "\n",
        "        # Run initializations\n",
        "        for seed_idx, seed in enumerate(seeds):\n",
        "            if self.verbose and len(seeds) > 1:\n",
        "                logger.info(f\"K-means initialization {seed_idx + 1}/{len(seeds)}\")\n",
        "\n",
        "            # Run single K-means efficiently\n",
        "            try:\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                # Check if this is the best run\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    self.iteration_table_ = iter_table\n",
        "\n",
        "            except Exception as e:\n",
        "                # Robust error handling\n",
        "                logger.warning(f\"Initialization {seed_idx + 1} failed: {str(e)}. Continuing with next seed.\")\n",
        "                continue\n",
        "\n",
        "        # Store results\n",
        "        self.cluster_centers_ = best_centroids\n",
        "        self.labels_ = best_labels\n",
        "        self.inertia_ = best_inertia\n",
        "        self.n_iter_ = best_n_iter\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"HybridBottomUpKMeans converged in {total_time:.2f}s after {self.n_iter_} iterations. \"\n",
        "                    f\"Final inertia: {self.inertia_:.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _get_seeds(self):\n",
        "        \"\"\"Generate optimized random seeds for multiple initializations.\"\"\"\n",
        "        # If only one initialization is needed, don't generate extra random numbers\n",
        "        if self.n_init == 1:\n",
        "            return [self.random_state if isinstance(self.random_state, int) else None]\n",
        "\n",
        "        # Generate seeds efficiently\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        # For T4 GPU in Colab, optimal n_init is usually 3-5 due to time constraints\n",
        "        # Adjust n_init if it's too large for interactive use\n",
        "        effective_n_init = min(self.n_init, 5) if HAS_GPU else self.n_init\n",
        "\n",
        "        # Generate all seeds at once (more efficient than one at a time)\n",
        "        if effective_n_init < self.n_init and self.verbose:\n",
        "            logger.info(f\"Reducing initializations from {self.n_init} to {effective_n_init} for optimal GPU usage\")\n",
        "\n",
        "        return random_state.randint(0, 2**31 - 1, size=effective_n_init).tolist()\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Optimized prediction for T4 GPU and dual-core CPU.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "\n",
        "        # Convert input to appropriate format\n",
        "        if HAS_GPU:\n",
        "            X = check_array(X, dtype=np.float32, accept_sparse=True)\n",
        "        else:\n",
        "            X = check_array(X, accept_sparse=True)\n",
        "\n",
        "        # For large datasets, process in batches to optimize GPU memory\n",
        "        if HAS_GPU and X.shape[0] > 10000:\n",
        "            batch_size = 10000  # Optimal batch size for T4 GPU\n",
        "            n_samples = X.shape[0]\n",
        "            labels = np.empty(n_samples, dtype=np.int32)\n",
        "\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                end = min(i + batch_size, n_samples)\n",
        "                X_batch = X[i:end]\n",
        "\n",
        "                # Use optimized GPU distance calculation\n",
        "                distances = _fast_distances(X_batch, self.cluster_centers_)\n",
        "                batch_labels, _ = _assign_labels_numba(distances)\n",
        "                labels[i:end] = batch_labels\n",
        "\n",
        "            return labels\n",
        "        else:\n",
        "            # For smaller datasets, process all at once\n",
        "            if NUMBA_AVAILABLE:\n",
        "                distances = _fast_distances(X, self.cluster_centers_)\n",
        "                labels, _ = _assign_labels_numba(distances)\n",
        "                return labels\n",
        "            else:\n",
        "                distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "                return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Optimized fit and predict for T4 GPU environment.\"\"\"\n",
        "        # For Colab/T4 environment, do this in one pass to minimize memory transfers\n",
        "        self.fit(X)\n",
        "        return self.labels_  # Use stored labels rather than recomputing\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Enhanced iteration table with performance insights.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "\n",
        "        try:\n",
        "            import pandas as pd\n",
        "\n",
        "            # Create DataFrame\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "\n",
        "            # Add more useful columns for performance analysis\n",
        "            if 'time' in df.columns:\n",
        "                # Calculate throughput (points processed per second)\n",
        "                if 'active_points' in df.columns:\n",
        "                    df['throughput'] = df['active_points'] / df['time']\n",
        "\n",
        "                # Calculate cumulative time\n",
        "                df['cumulative_time'] = df['time'].cumsum()\n",
        "\n",
        "            # Add hardware info\n",
        "            if HAS_GPU:\n",
        "                df.name = \"Iteration History (GPU: T4, CPU: 2-core Xeon)\"\n",
        "            else:\n",
        "                df.name = \"Iteration History (CPU: 2-core Xeon)\"\n",
        "\n",
        "            # Format for better readability\n",
        "            if 'phase' in df.columns:\n",
        "                df['phase'] = df['phase'].str.capitalize()\n",
        "\n",
        "            # Round numeric columns\n",
        "            for col in df.select_dtypes(include=['float']).columns:\n",
        "                df[col] = df[col].round(3)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except ImportError:\n",
        "            # Fallback to simple text output\n",
        "            print(\"Iteration History:\")\n",
        "            for i, info in enumerate(self.iteration_table_):\n",
        "                print(f\"Iteration {i+1}: \" + \", \".join([f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\"\n",
        "                                                    for k, v in info.items()]))\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to cluster-distance space (optimized for T4 GPU).\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "\n",
        "        # Optimize data type for GPU\n",
        "        if HAS_GPU:\n",
        "            X = check_array(X, dtype=np.float32, accept_sparse=True)\n",
        "        else:\n",
        "            X = check_array(X, accept_sparse=True)\n",
        "\n",
        "        # For large datasets on T4, process in batches\n",
        "        if HAS_GPU and X.shape[0] > 10000:\n",
        "            batch_size = 10000\n",
        "            n_samples = X.shape[0]\n",
        "            n_clusters = self.cluster_centers_.shape[0]\n",
        "            distances = np.empty((n_samples, n_clusters), dtype=np.float32)\n",
        "\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                end = min(i + batch_size, n_samples)\n",
        "                X_batch = X[i:end]\n",
        "\n",
        "                # Use GPU accelerated distance computation\n",
        "                distances[i:end] = _fast_distances(X_batch, self.cluster_centers_)\n",
        "\n",
        "            return distances\n",
        "        else:\n",
        "            # For smaller datasets, compute all distances at once\n",
        "            return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=300000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    #scalene_profiler.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M4KKH25dRvhZ",
        "outputId": "fe4364cf-bb22-48ac-ae26-b6580e145e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5112399303c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1859\u001b[0m     \u001b[0;31m# To run the benchmark evaluation that includes BottomUpKMeans,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m     \u001b[0;31m# simply call run_bench_evaluation(). You can also run the full evaluation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0mrun_bench_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m     \u001b[0;31m# Alternatively, uncomment the following line to run all evaluations:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m     \u001b[0;31m#run_full_evaluation()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5112399303c8>\u001b[0m in \u001b[0;36mrun_bench_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1508\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'times'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bottomup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbu_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         bu_metrics = {\n\u001b[0;32m-> 1510\u001b[0;31m             \u001b[0;34m'silhouette'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbu_kmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m             \u001b[0;34m'calinski_harabasz'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcalinski_harabasz_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbu_kmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0;34m'davies_bouldin'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdavies_bouldin_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbu_kmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0m_silhouette_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_freqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_freqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     )\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpairwise_distances_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m             \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m             \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2262\u001b[0m             \u001b[0m_check_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36m_silhouette_reduce\u001b[0;34m(D_chunk, start, labels, label_freqs)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0msample_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             cluster_distances[i] += np.bincount(\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0msample_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_freqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "import numpy as np\n",
        "from numba import njit, prange, float64, int32\n",
        "from numba import njit, prange, cuda\n",
        "import numpy as np\n",
        "from numba import njit, prange\n",
        "import numpy as np\n",
        "\n",
        "# Check if CuPy is available\n",
        "try:\n",
        "    import cupy as cp\n",
        "    HAS_GPU = True\n",
        "except ImportError:\n",
        "    HAS_GPU = False\n",
        "\n",
        "# GPU version using CuPy\n",
        "if HAS_GPU:\n",
        "    def _fast_distances_gpu(X, centroids):\n",
        "        \"\"\"Compute squared Euclidean distances with CuPy GPU acceleration.\"\"\"\n",
        "        # Transfer data to GPU if needed\n",
        "        X_gpu = cp.asarray(X)\n",
        "        centroids_gpu = cp.asarray(centroids)\n",
        "\n",
        "        # Compute squared norms\n",
        "        X_norm = cp.sum(X_gpu**2, axis=1, keepdims=True)\n",
        "        centroids_norm = cp.sum(centroids_gpu**2, axis=1, keepdims=True).T\n",
        "\n",
        "        # Use matrix multiplication for dot product\n",
        "        dot_product = cp.dot(X_gpu, centroids_gpu.T)\n",
        "\n",
        "        # Compute distances using ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
        "        distances = X_norm + centroids_norm - 2.0 * dot_product\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def compute_distances(X, centroids):\n",
        "        \"\"\"Wrapper to handle GPU distance computation.\"\"\"\n",
        "        distances_gpu = _fast_distances_gpu(X, centroids)\n",
        "        return cp.asnumpy(distances_gpu)\n",
        "\n",
        "    def assign_labels(distances):\n",
        "        \"\"\"Assign labels based on distances using GPU.\"\"\"\n",
        "        distances_gpu = cp.asarray(distances)\n",
        "        min_distances = cp.min(distances_gpu, axis=1)\n",
        "        labels = cp.argmin(distances_gpu, axis=1)\n",
        "\n",
        "        return cp.asnumpy(labels).astype(np.int32), cp.asnumpy(min_distances)\n",
        "\n",
        "    def _update_centroids_gpu(X, labels, n_clusters):\n",
        "        \"\"\"Update centroids using GPU acceleration.\"\"\"\n",
        "        X_gpu = cp.asarray(X)\n",
        "        labels_gpu = cp.asarray(labels)\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        centroids = cp.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "        counts = cp.zeros(n_clusters, dtype=cp.int32)\n",
        "\n",
        "        # Process each cluster\n",
        "        for k in range(n_clusters):\n",
        "            mask = (labels_gpu == k)\n",
        "            cluster_points = X_gpu[mask]\n",
        "            if len(cluster_points) > 0:\n",
        "                centroids[k] = cp.mean(cluster_points, axis=0)\n",
        "                counts[k] = len(cluster_points)\n",
        "\n",
        "        return cp.asnumpy(centroids), cp.asnumpy(counts)\n",
        "\n",
        "# CPU optimized version - unchanged from original\n",
        "@njit(parallel=True,fastmath=True,cache=True)\n",
        "def _fast_distances_cpu(X, centroids):\n",
        "    \"\"\"Compute squared Euclidean distances with Numba - vectorized version.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float64)\n",
        "\n",
        "    # Pre-compute squared norms - parallelized\n",
        "    x_norms = np.zeros(n_samples, dtype=np.float64)\n",
        "    centroid_norms = np.zeros(n_clusters, dtype=np.float64)\n",
        "\n",
        "    for i in prange(n_samples):\n",
        "        for k in range(n_features):\n",
        "            x_norms[i] += X[i, k] * X[i, k]\n",
        "\n",
        "    for j in prange(n_clusters):\n",
        "        for k in range(n_features):\n",
        "            centroid_norms[j] += centroids[j, k] * centroids[j, k]\n",
        "\n",
        "    # Compute distances in parallel\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            dot_product = 0.0\n",
        "            for k in range(n_features):\n",
        "                dot_product += X[i, k] * centroids[j, k]\n",
        "            distances[i, j] = x_norms[i] + centroid_norms[j] - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True,cache=True)\n",
        "def _assign_labels_cpu(distances):\n",
        "    \"\"\"Assign labels to data points based on nearest centroid.\"\"\"\n",
        "    n_samples = distances.shape[0]\n",
        "    labels = np.zeros(n_samples, dtype=np.int32)\n",
        "    min_distances = np.full(n_samples, np.inf)\n",
        "\n",
        "    for i in prange(n_samples):\n",
        "        for j in range(distances.shape[1]):\n",
        "            if distances[i, j] < min_distances[i]:\n",
        "                min_distances[i] = distances[i, j]\n",
        "                labels[i] = j\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "@njit(parallel=True,cache=True)\n",
        "def _update_centroids_cpu(X, labels, n_clusters):\n",
        "    \"\"\"Update centroids based on assigned labels.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=X.dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Count cluster members and sum values\n",
        "    for i in range(n_samples):\n",
        "        cluster_id = labels[i]\n",
        "        counts[cluster_id] += 1\n",
        "        for j in range(n_features):\n",
        "            centroids[cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Calculate means\n",
        "    for i in prange(n_clusters):\n",
        "        if counts[i] > 0:\n",
        "            for j in range(n_features):\n",
        "                centroids[i, j] /= counts[i]\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "# Unified interface\n",
        "def _fast_distances(X, centroids):\n",
        "    if HAS_GPU:\n",
        "        return compute_distances(X, centroids)\n",
        "    else:\n",
        "        return _fast_distances_cpu(X, centroids)\n",
        "\n",
        "def _assign_labels_numba(distances):\n",
        "    if HAS_GPU:\n",
        "        return assign_labels(distances)\n",
        "    else:\n",
        "        return _assign_labels_cpu(distances)\n",
        "\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    if HAS_GPU:\n",
        "        return _update_centroids_gpu(X, labels, n_clusters)\n",
        "    else:\n",
        "        return _update_centroids_cpu(X, labels, n_clusters)\n",
        "\n",
        "# Main kmeans function\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    #scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "SnVkMw5whLFv",
        "outputId": "80d8627c-c937-4fce-a68c-fb14c1634c6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 0.434 seconds\n",
            "Average execution time (OptimizedKMeans): 0.025 seconds\n",
            "Average execution time (SklearnKMeans): 0.019 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.144\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.577\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "import numpy as np\n",
        "from numba import njit, prange, float64, int32\n",
        "from numba import njit, prange, cuda\n",
        "import numpy as np\n",
        "from numba import njit, prange\n",
        "import numpy as np\n",
        "\n",
        "# Check if CuPy is available\n",
        "import numpy as np\n",
        "import numba as nb\n",
        "from numba import prange, cuda\n",
        "import math\n",
        "\n",
        "# Check if CuPy is available\n",
        "try:\n",
        "    import cupy as cp\n",
        "    HAS_GPU = True\n",
        "except ImportError:\n",
        "    HAS_GPU = False\n",
        "\n",
        "# GPU version using CuPy - Highly optimized\n",
        "if HAS_GPU:\n",
        "    def _fast_distances_gpu(X, centroids):\n",
        "        \"\"\"Optimized squared Euclidean distances with CuPy GPU acceleration.\"\"\"\n",
        "        # Transfer data to GPU if needed (detect if already on GPU)\n",
        "        X_gpu = cp.asarray(X)\n",
        "        centroids_gpu = cp.asarray(centroids)\n",
        "\n",
        "        # Use more efficient matrix operations and broadcast\n",
        "        # ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
        "        # Use fp32 precision for better performance when appropriate\n",
        "        if X_gpu.dtype == np.float64 and X_gpu.shape[1] < 100:\n",
        "            X_gpu = X_gpu.astype(cp.float32)\n",
        "            centroids_gpu = centroids_gpu.astype(cp.float32)\n",
        "\n",
        "        # Efficient squared norm computation\n",
        "        X_norm = cp.sum(X_gpu**2, axis=1, keepdims=True)\n",
        "        centroids_norm = cp.sum(centroids_gpu**2, axis=1)\n",
        "\n",
        "        # Use optimized BLAS for matrix multiplication\n",
        "        # cuBLAS is automatically used by CuPy\n",
        "        distances = X_norm + centroids_norm - 2 * cp.dot(X_gpu, centroids_gpu.T)\n",
        "\n",
        "        # Ensure non-negative distances (numerical stability)\n",
        "        cp.maximum(distances, 0, out=distances)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def compute_distances(X, centroids):\n",
        "        \"\"\"Wrapper with improved memory management.\"\"\"\n",
        "        # Avoid unnecessary transfers for large datasets\n",
        "        if X.shape[0] > 100000:\n",
        "            # Process in batches to avoid OOM\n",
        "            batch_size = 50000\n",
        "            n_batches = (X.shape[0] + batch_size - 1) // batch_size\n",
        "            distances = np.empty((X.shape[0], centroids.shape[0]), dtype=np.float32)\n",
        "\n",
        "            for i in range(n_batches):\n",
        "                start, end = i * batch_size, min((i + 1) * batch_size, X.shape[0])\n",
        "                batch_distances = _fast_distances_gpu(X[start:end], centroids)\n",
        "                distances[start:end] = cp.asnumpy(batch_distances)\n",
        "\n",
        "            return distances\n",
        "        else:\n",
        "            # Process in one go for smaller datasets\n",
        "            distances_gpu = _fast_distances_gpu(X, centroids)\n",
        "            return cp.asnumpy(distances_gpu)\n",
        "\n",
        "    def assign_labels(distances):\n",
        "        \"\"\"Optimized label assignment using GPU.\"\"\"\n",
        "        # Keep data on GPU if already there\n",
        "        if isinstance(distances, cp.ndarray):\n",
        "            distances_gpu = distances\n",
        "        else:\n",
        "            distances_gpu = cp.asarray(distances)\n",
        "\n",
        "        # Use specialized kernel for argmin operation\n",
        "        labels = cp.argmin(distances_gpu, axis=1)\n",
        "        min_distances = cp.take_along_axis(distances_gpu,\n",
        "                                          cp.expand_dims(labels, axis=1),\n",
        "                                          axis=1).squeeze()\n",
        "\n",
        "        return cp.asnumpy(labels).astype(np.int32), cp.asnumpy(min_distances)\n",
        "\n",
        "    def _update_centroids_gpu(X, labels, n_clusters):\n",
        "        \"\"\"Optimized centroid update using reduction operations.\"\"\"\n",
        "        X_gpu = cp.asarray(X)\n",
        "        labels_gpu = cp.asarray(labels)\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Use specialized CuPy functions for performance\n",
        "        new_centroids = cp.zeros((n_clusters, n_features), dtype=X_gpu.dtype)\n",
        "        counts = cp.zeros(n_clusters, dtype=cp.int32)\n",
        "\n",
        "        # Use one-hot encoding and matrix multiplication for faster reduction\n",
        "        if n_clusters < 100:  # For reasonable number of clusters\n",
        "            one_hot = cp.eye(n_clusters)[labels_gpu]\n",
        "            counts = cp.sum(one_hot, axis=0)\n",
        "            new_centroids = cp.dot(one_hot.T, X_gpu)\n",
        "\n",
        "            # Handle non-empty clusters\n",
        "            mask = counts > 0\n",
        "            new_centroids[mask] = new_centroids[mask] / counts[mask, cp.newaxis]\n",
        "        else:\n",
        "            # Fall back to loop for many clusters to save memory\n",
        "            for k in range(n_clusters):\n",
        "                mask = (labels_gpu == k)\n",
        "                cluster_size = cp.sum(mask)\n",
        "                counts[k] = cluster_size\n",
        "\n",
        "                if cluster_size > 0:\n",
        "                    new_centroids[k] = cp.sum(X_gpu[mask], axis=0) / cluster_size\n",
        "\n",
        "        # Handle empty clusters on GPU\n",
        "        empty_clusters = cp.where(counts == 0)[0]\n",
        "        if len(empty_clusters) > 0:\n",
        "            # Find points farthest from their centroids\n",
        "            non_empty_mask = cp.where(counts > 0)[0]\n",
        "            if len(non_empty_mask) > 0:\n",
        "                random_centroid = new_centroids[non_empty_mask[0]]\n",
        "\n",
        "                for k in empty_clusters:\n",
        "                    # Add random perturbation to an existing centroid\n",
        "                    noise = cp.random.randn(n_features).astype(X_gpu.dtype) * 0.1\n",
        "                    new_centroids[k] = random_centroid + noise\n",
        "                    counts[k] = 1\n",
        "\n",
        "        return cp.asnumpy(new_centroids), cp.asnumpy(counts)\n",
        "\n",
        "# CPU optimized version - significantly improved\n",
        "@nb.njit(parallel=True, fastmath=True, cache=True)\n",
        "def _fast_distances_cpu(X, centroids):\n",
        "    \"\"\"Highly optimized squared Euclidean distances with Numba.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float32)\n",
        "\n",
        "    # Pre-compute squared norms - optimized for cache locality\n",
        "    x_norms = np.empty(n_samples, dtype=np.float32)\n",
        "    centroid_norms = np.empty(n_clusters, dtype=np.float32)\n",
        "\n",
        "    # Compute norms with better vectorization\n",
        "    for i in prange(n_samples):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            val = X[i, k]\n",
        "            norm += val * val\n",
        "        x_norms[i] = norm\n",
        "\n",
        "    for j in prange(n_clusters):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            val = centroids[j, k]\n",
        "            norm += val * val\n",
        "        centroid_norms[j] = norm\n",
        "\n",
        "    # Compute distances with better memory access pattern\n",
        "    # Process in blocks for better cache utilization\n",
        "    block_size = min(32, n_samples)\n",
        "\n",
        "    for block_start in prange(0, n_samples, block_size):\n",
        "        block_end = min(block_start + block_size, n_samples)\n",
        "\n",
        "        # Prefetch centroid data for this block\n",
        "        for j in range(n_clusters):\n",
        "            centroid_norm_j = centroid_norms[j]\n",
        "\n",
        "            for i in range(block_start, block_end):\n",
        "                x_norm_i = x_norms[i]\n",
        "                dot_product = 0.0\n",
        "\n",
        "                # Manual loop unrolling for better instruction-level parallelism\n",
        "                for k in range(0, n_features - 3, 4):\n",
        "                    dot_product += (X[i, k] * centroids[j, k] +\n",
        "                                  X[i, k+1] * centroids[j, k+1] +\n",
        "                                  X[i, k+2] * centroids[j, k+2] +\n",
        "                                  X[i, k+3] * centroids[j, k+3])\n",
        "\n",
        "                # Handle remaining elements\n",
        "                for k in range((n_features // 4) * 4, n_features):\n",
        "                    dot_product += X[i, k] * centroids[j, k]\n",
        "\n",
        "                # ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
        "                distances[i, j] = x_norm_i + centroid_norm_j - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@nb.njit(parallel=True, cache=True, fastmath=True)\n",
        "def _assign_labels_cpu(distances):\n",
        "    \"\"\"Optimized label assignment for CPU.\"\"\"\n",
        "    n_samples = distances.shape[0]\n",
        "    n_clusters = distances.shape[1]\n",
        "    labels = np.empty(n_samples, dtype=np.int32)\n",
        "    min_distances = np.empty(n_samples, dtype=np.float32)\n",
        "\n",
        "    # Process in parallel with better vectorization\n",
        "    for i in prange(n_samples):\n",
        "        min_idx = 0\n",
        "        min_dist = distances[i, 0]\n",
        "\n",
        "        # Manual vectorization helps Numba generate better code\n",
        "        for j in range(1, n_clusters):\n",
        "            dist = distances[i, j]\n",
        "            # Branchless min operation\n",
        "            is_smaller = dist < min_dist\n",
        "            min_idx = j if is_smaller else min_idx\n",
        "            min_dist = dist if is_smaller else min_dist\n",
        "\n",
        "        labels[i] = min_idx\n",
        "        min_distances[i] = min_dist\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "@nb.njit(parallel=True, cache=True, fastmath=True)\n",
        "def _update_centroids_cpu(X, labels, n_clusters):\n",
        "    \"\"\"Optimized centroid update with improved memory patterns.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    # Use float32 for better SIMD utilization when appropriate\n",
        "    dtype = np.float32 if X.dtype == np.float32 else np.float64\n",
        "\n",
        "    # Initialize centroids and counts\n",
        "    new_centroids = np.zeros((n_clusters, n_features), dtype=dtype)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    # Process in thread-local manner to avoid synchronization\n",
        "    n_threads = nb.get_num_threads()\n",
        "    local_centroids = np.zeros((n_threads, n_clusters, n_features), dtype=dtype)\n",
        "    local_counts = np.zeros((n_threads, n_clusters), dtype=np.int32)\n",
        "\n",
        "    # Accumulate in thread-local storage\n",
        "    for i in prange(n_samples):\n",
        "        thread_id = nb.get_thread_id()\n",
        "        cluster_id = labels[i]\n",
        "        local_counts[thread_id, cluster_id] += 1\n",
        "\n",
        "        # Accumulate values with better cache pattern\n",
        "        for j in range(n_features):\n",
        "            local_centroids[thread_id, cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Combine thread-local results\n",
        "    for t in range(n_threads):\n",
        "        for k in range(n_clusters):\n",
        "            counts[k] += local_counts[t, k]\n",
        "            for j in range(n_features):\n",
        "                new_centroids[k, j] += local_centroids[t, k, j]\n",
        "\n",
        "    # Calculate means\n",
        "    for k in prange(n_clusters):\n",
        "        if counts[k] > 0:\n",
        "            inv_count = 1.0 / counts[k]\n",
        "            for j in range(n_features):\n",
        "                new_centroids[k, j] *= inv_count\n",
        "\n",
        "    return new_centroids, counts\n",
        "# Unified interface\n",
        "def _fast_distances(X, centroids):\n",
        "    if HAS_GPU:\n",
        "        return compute_distances(X, centroids)\n",
        "    else:\n",
        "        return _fast_distances_cpu(X, centroids)\n",
        "\n",
        "def _assign_labels_numba(distances):\n",
        "    if HAS_GPU:\n",
        "        return assign_labels(distances)\n",
        "    else:\n",
        "        return _assign_labels_cpu(distances)\n",
        "\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    if HAS_GPU:\n",
        "        return _update_centroids_gpu(X, labels, n_clusters)\n",
        "    else:\n",
        "        return _update_centroids_cpu(X, labels, n_clusters)\n",
        "\n",
        "# Main kmeans function\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    #scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "YnjU5fzZlx9H",
        "outputId": "0eea159b-fd32-48af-fe51-fb2241c66b55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 1.313 seconds\n",
            "Average execution time (OptimizedKMeans): 0.019 seconds\n",
            "Average execution time (SklearnKMeans): 0.016 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.062\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.594\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "multiprocessing cpu"
      ],
      "metadata": {
        "id": "pZiTEIGUng4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "import numpy as np\n",
        "from numba import njit, prange, float64, int32\n",
        "from numba import njit, prange, cuda\n",
        "import numpy as np\n",
        "from numba import njit, prange\n",
        "import numpy as np\n",
        "\n",
        "# Check if CuPy is available\n",
        "import numpy as np\n",
        "import numba as nb\n",
        "from numba import prange, cuda\n",
        "import math\n",
        "\n",
        "# Check if CuPy is available\n",
        "try:\n",
        "    import cupy as cp\n",
        "    HAS_GPU = True\n",
        "except ImportError:\n",
        "    HAS_GPU = False\n",
        "\n",
        "# GPU version using CuPy - Highly optimized\n",
        "if HAS_GPU:\n",
        "    def _fast_distances_gpu(X, centroids):\n",
        "        \"\"\"Optimized squared Euclidean distances with CuPy GPU acceleration.\"\"\"\n",
        "        # Transfer data to GPU if needed (detect if already on GPU)\n",
        "        X_gpu = cp.asarray(X)\n",
        "        centroids_gpu = cp.asarray(centroids)\n",
        "\n",
        "        # Use more efficient matrix operations and broadcast\n",
        "        # ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
        "        # Use fp32 precision for better performance when appropriate\n",
        "        if X_gpu.dtype == np.float64 and X_gpu.shape[1] < 100:\n",
        "            X_gpu = X_gpu.astype(cp.float32)\n",
        "            centroids_gpu = centroids_gpu.astype(cp.float32)\n",
        "\n",
        "        # Efficient squared norm computation\n",
        "        X_norm = cp.sum(X_gpu**2, axis=1, keepdims=True)\n",
        "        centroids_norm = cp.sum(centroids_gpu**2, axis=1)\n",
        "\n",
        "        # Use optimized BLAS for matrix multiplication\n",
        "        # cuBLAS is automatically used by CuPy\n",
        "        distances = X_norm + centroids_norm - 2 * cp.dot(X_gpu, centroids_gpu.T)\n",
        "\n",
        "        # Ensure non-negative distances (numerical stability)\n",
        "        cp.maximum(distances, 0, out=distances)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def compute_distances(X, centroids):\n",
        "        \"\"\"Wrapper with improved memory management.\"\"\"\n",
        "        # Avoid unnecessary transfers for large datasets\n",
        "        if X.shape[0] > 100000:\n",
        "            # Process in batches to avoid OOM\n",
        "            batch_size = 50000\n",
        "            n_batches = (X.shape[0] + batch_size - 1) // batch_size\n",
        "            distances = np.empty((X.shape[0], centroids.shape[0]), dtype=np.float32)\n",
        "\n",
        "            for i in range(n_batches):\n",
        "                start, end = i * batch_size, min((i + 1) * batch_size, X.shape[0])\n",
        "                batch_distances = _fast_distances_gpu(X[start:end], centroids)\n",
        "                distances[start:end] = cp.asnumpy(batch_distances)\n",
        "\n",
        "            return distances\n",
        "        else:\n",
        "            # Process in one go for smaller datasets\n",
        "            distances_gpu = _fast_distances_gpu(X, centroids)\n",
        "            return cp.asnumpy(distances_gpu)\n",
        "\n",
        "    def assign_labels(distances):\n",
        "        \"\"\"Optimized label assignment using GPU.\"\"\"\n",
        "        # Keep data on GPU if already there\n",
        "        if isinstance(distances, cp.ndarray):\n",
        "            distances_gpu = distances\n",
        "        else:\n",
        "            distances_gpu = cp.asarray(distances)\n",
        "\n",
        "        # Use specialized kernel for argmin operation\n",
        "        labels = cp.argmin(distances_gpu, axis=1)\n",
        "        min_distances = cp.take_along_axis(distances_gpu,\n",
        "                                          cp.expand_dims(labels, axis=1),\n",
        "                                          axis=1).squeeze()\n",
        "\n",
        "        return cp.asnumpy(labels).astype(np.int32), cp.asnumpy(min_distances)\n",
        "\n",
        "    def _update_centroids_gpu(X, labels, n_clusters):\n",
        "        \"\"\"Optimized centroid update using reduction operations.\"\"\"\n",
        "        X_gpu = cp.asarray(X)\n",
        "        labels_gpu = cp.asarray(labels)\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Use specialized CuPy functions for performance\n",
        "        new_centroids = cp.zeros((n_clusters, n_features), dtype=X_gpu.dtype)\n",
        "        counts = cp.zeros(n_clusters, dtype=cp.int32)\n",
        "\n",
        "        # Use one-hot encoding and matrix multiplication for faster reduction\n",
        "        if n_clusters < 100:  # For reasonable number of clusters\n",
        "            one_hot = cp.eye(n_clusters)[labels_gpu]\n",
        "            counts = cp.sum(one_hot, axis=0)\n",
        "            new_centroids = cp.dot(one_hot.T, X_gpu)\n",
        "\n",
        "            # Handle non-empty clusters\n",
        "            mask = counts > 0\n",
        "            new_centroids[mask] = new_centroids[mask] / counts[mask, cp.newaxis]\n",
        "        else:\n",
        "            # Fall back to loop for many clusters to save memory\n",
        "            for k in range(n_clusters):\n",
        "                mask = (labels_gpu == k)\n",
        "                cluster_size = cp.sum(mask)\n",
        "                counts[k] = cluster_size\n",
        "\n",
        "                if cluster_size > 0:\n",
        "                    new_centroids[k] = cp.sum(X_gpu[mask], axis=0) / cluster_size\n",
        "\n",
        "        # Handle empty clusters on GPU\n",
        "        empty_clusters = cp.where(counts == 0)[0]\n",
        "        if len(empty_clusters) > 0:\n",
        "            # Find points farthest from their centroids\n",
        "            non_empty_mask = cp.where(counts > 0)[0]\n",
        "            if len(non_empty_mask) > 0:\n",
        "                random_centroid = new_centroids[non_empty_mask[0]]\n",
        "\n",
        "                for k in empty_clusters:\n",
        "                    # Add random perturbation to an existing centroid\n",
        "                    noise = cp.random.randn(n_features).astype(X_gpu.dtype) * 0.1\n",
        "                    new_centroids[k] = random_centroid + noise\n",
        "                    counts[k] = 1\n",
        "\n",
        "        return cp.asnumpy(new_centroids), cp.asnumpy(counts)\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _fast_distances_cpu(X, centroids):\n",
        "    \"\"\"Highly optimized squared Euclidean distances with Numba.\n",
        "    Specifically tuned for 2-core CPU on Google Colab.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float32)\n",
        "\n",
        "    # Precompute centroids squared norms (small array, fast)\n",
        "    centroid_norms = np.empty(n_clusters, dtype=np.float32)\n",
        "    for j in range(n_clusters):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            val = centroids[j, k]\n",
        "            norm += val * val\n",
        "        centroid_norms[j] = norm\n",
        "\n",
        "    # Process samples in parallel chunks optimized for 2 cores\n",
        "    chunk_size = max(1, n_samples // 2)  # Split work evenly for 2 cores\n",
        "\n",
        "    for chunk_idx in prange(2):  # Explicitly use 2 cores\n",
        "        start = chunk_idx * chunk_size\n",
        "        end = min(n_samples, (chunk_idx + 1) * chunk_size)\n",
        "\n",
        "        # Process samples in this chunk\n",
        "        for i in range(start, end):\n",
        "            # Compute sample norm once\n",
        "            x_norm = 0.0\n",
        "            for k in range(n_features):\n",
        "                val = X[i, k]\n",
        "                x_norm += val * val\n",
        "\n",
        "            # Compute distances to all centroids\n",
        "            for j in range(n_clusters):\n",
        "                # Start with precomputed norms\n",
        "                distance = x_norm + centroid_norms[j]\n",
        "\n",
        "                # Subtract 2 * dot product with manual unrolling for better vectorization\n",
        "                dot_product = 0.0\n",
        "                # Process 4 elements at a time when possible (SIMD friendly)\n",
        "                for k in range(0, n_features - 3, 4):\n",
        "                    dot_product += X[i, k] * centroids[j, k]\n",
        "                    dot_product += X[i, k+1] * centroids[j, k+1]\n",
        "                    dot_product += X[i, k+2] * centroids[j, k+2]\n",
        "                    dot_product += X[i, k+3] * centroids[j, k+3]\n",
        "\n",
        "                # Handle remaining elements\n",
        "                for k in range((n_features // 4) * 4, n_features):\n",
        "                    dot_product += X[i, k] * centroids[j, k]\n",
        "\n",
        "                distances[i, j] = distance - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _assign_labels_cpu(distances):\n",
        "    \"\"\"Optimized label assignment with Numba.\n",
        "    Tuned for 2-core processing.\"\"\"\n",
        "    n_samples = distances.shape[0]\n",
        "    n_clusters = distances.shape[1]\n",
        "    labels = np.empty(n_samples, dtype=np.int32)\n",
        "    min_distances = np.empty(n_samples, dtype=np.float32)\n",
        "\n",
        "    # Divide work for 2 cores\n",
        "    chunk_size = max(1, n_samples // 2)\n",
        "\n",
        "    for chunk_idx in prange(2):  # Explicitly use 2 cores\n",
        "        start = chunk_idx * chunk_size\n",
        "        end = min(n_samples, (chunk_idx + 1) * chunk_size)\n",
        "\n",
        "        for i in range(start, end):\n",
        "            min_idx = 0\n",
        "            min_dist = distances[i, 0]\n",
        "\n",
        "            # Manual unrolling to help compiler vectorize\n",
        "            j = 1\n",
        "            while j < n_clusters - 1:  # Process 2 at a time when possible\n",
        "                dist1 = distances[i, j]\n",
        "                dist2 = distances[i, j+1]\n",
        "\n",
        "                # Update if first distance is smaller\n",
        "                if dist1 < min_dist:\n",
        "                    min_dist = dist1\n",
        "                    min_idx = j\n",
        "\n",
        "                # Update if second distance is smaller\n",
        "                if dist2 < min_dist:\n",
        "                    min_dist = dist2\n",
        "                    min_idx = j+1\n",
        "\n",
        "                j += 2\n",
        "\n",
        "            # Handle last element if n_clusters is odd\n",
        "            if j < n_clusters:\n",
        "                dist = distances[i, j]\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    min_idx = j\n",
        "\n",
        "            labels[i] = min_idx\n",
        "            min_distances[i] = min_dist\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _update_centroids_cpu(X, labels, n_clusters):\n",
        "    \"\"\"Optimized centroid update for 2-core CPU.\n",
        "    Uses thread-local storage to avoid synchronization.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    # Use thread-local storage to avoid locks (optimized for 2 cores)\n",
        "    local_sums = np.zeros((2, n_clusters, n_features), dtype=np.float64)\n",
        "    local_counts = np.zeros((2, n_clusters), dtype=np.int32)\n",
        "\n",
        "    # Chunk size for 2 threads\n",
        "    chunk_size = max(1, n_samples // 2)\n",
        "\n",
        "    # Accumulate in thread-local storage\n",
        "    for thread_id in prange(2):  # Explicitly use 2 cores\n",
        "        start = thread_id * chunk_size\n",
        "        end = min(n_samples, (thread_id + 1) * chunk_size)\n",
        "\n",
        "        # Process points assigned to this thread\n",
        "        for i in range(start, end):\n",
        "            cluster_id = labels[i]\n",
        "            local_counts[thread_id, cluster_id] += 1\n",
        "\n",
        "            # Accumulate sum of points in thread-local storage\n",
        "            for j in range(n_features):\n",
        "                local_sums[thread_id, cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Combine thread-local results\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=np.float64)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    for t in range(2):  # Combine from 2 threads\n",
        "        for k in range(n_clusters):\n",
        "            counts[k] += local_counts[t, k]\n",
        "            for j in range(n_features):\n",
        "                centroids[k, j] += local_sums[t, k, j]\n",
        "\n",
        "    # Calculate means - avoid division by zero\n",
        "    for k in range(n_clusters):\n",
        "        if counts[k] > 0:\n",
        "            inv_count = 1.0 / counts[k]\n",
        "            for j in range(n_features):\n",
        "                centroids[k, j] *= inv_count\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "# Unified interface\n",
        "def _fast_distances(X, centroids):\n",
        "    if HAS_GPU:\n",
        "        return compute_distances(X, centroids)\n",
        "    else:\n",
        "        return _fast_distances_cpu(X, centroids)\n",
        "\n",
        "def _assign_labels_numba(distances):\n",
        "    if HAS_GPU:\n",
        "        return assign_labels(distances)\n",
        "    else:\n",
        "        return _assign_labels_cpu(distances)\n",
        "\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    if HAS_GPU:\n",
        "        return _update_centroids_gpu(X, labels, n_clusters)\n",
        "    else:\n",
        "        return _update_centroids_cpu(X, labels, n_clusters)\n",
        "\n",
        "# Main kmeans function\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    #scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "1vrhAiKloVZK",
        "outputId": "e4577e63-0632-4759-91b7-13bdbd0a428c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 0.492 seconds\n",
            "Average execution time (OptimizedKMeans): 0.027 seconds\n",
            "Average execution time (SklearnKMeans): 0.016 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.062\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.594\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trying gpu optimisation"
      ],
      "metadata": {
        "id": "72u9TZTopqzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import warnings\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.cluster import KMeans as SklearnKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Optional Numba acceleration\n",
        "try:\n",
        "    from numba import njit\n",
        "    NUMBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMBA_AVAILABLE = False\n",
        "    def njit(func):\n",
        "        return func\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, boolean, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available since we're using its decorators\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import logging\n",
        "from numba import njit, prange, float64, int32\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if Numba is available\n",
        "NUMBA_AVAILABLE = True  # We'll assume Numba is available\n",
        "import numpy as np\n",
        "from numba import njit, prange, float64, int32\n",
        "from numba import njit, prange, cuda\n",
        "import numpy as np\n",
        "from numba import njit, prange\n",
        "import numpy as np\n",
        "\n",
        "# Check if CuPy is available\n",
        "import numpy as np\n",
        "import numba as nb\n",
        "from numba import prange, cuda\n",
        "import math\n",
        "\n",
        "# Check if CuPy is available\n",
        "try:\n",
        "    import cupy as cp\n",
        "    HAS_GPU = True\n",
        "except ImportError:\n",
        "    HAS_GPU = False\n",
        "\n",
        "# GPU version using CuPy - Highly optimized\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import math\n",
        "import time\n",
        "from cupy import prof\n",
        "\n",
        "# Custom CUDA kernel for efficient distance calculation\n",
        "_distance_kernel = cp.RawKernel(r'''\n",
        "extern \"C\" __global__\n",
        "void compute_distances(const float* X, const float* centroids,\n",
        "                      float* distances, int n_samples, int n_features, int n_clusters) {\n",
        "    // Get global thread ID\n",
        "    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "\n",
        "    // Check if this thread should compute a distance\n",
        "    if (tid < n_samples * n_clusters) {\n",
        "        int sample_idx = tid / n_clusters;\n",
        "        int centroid_idx = tid % n_clusters;\n",
        "\n",
        "        // Compute squared distance\n",
        "        float dist = 0.0f;\n",
        "        for (int j = 0; j < n_features; j++) {\n",
        "            float diff = X[sample_idx * n_features + j] - centroids[centroid_idx * n_features + j];\n",
        "            dist += diff * diff;\n",
        "        }\n",
        "\n",
        "        // Store result\n",
        "        distances[sample_idx * n_clusters + centroid_idx] = dist;\n",
        "    }\n",
        "}\n",
        "''', 'compute_distances')\n",
        "\n",
        "# Custom CUDA kernel for label assignment\n",
        "_label_kernel = cp.RawKernel(r'''\n",
        "extern \"C\" __global__\n",
        "void assign_labels(const float* distances, int* labels, float* min_distances,\n",
        "                 int n_samples, int n_clusters) {\n",
        "    // Get global thread ID\n",
        "    int sample_idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "\n",
        "    // Check if this thread should process a sample\n",
        "    if (sample_idx < n_samples) {\n",
        "        int best_cluster = 0;\n",
        "        float min_dist = distances[sample_idx * n_clusters];\n",
        "\n",
        "        // Find minimum distance and corresponding cluster\n",
        "        for (int j = 1; j < n_clusters; j++) {\n",
        "            float dist = distances[sample_idx * n_clusters + j];\n",
        "            if (dist < min_dist) {\n",
        "                min_dist = dist;\n",
        "                best_cluster = j;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Store results\n",
        "        labels[sample_idx] = best_cluster;\n",
        "        min_distances[sample_idx] = min_dist;\n",
        "    }\n",
        "}\n",
        "''', 'assign_labels')\n",
        "\n",
        "# Custom CUDA kernel for centroid update (reduction)\n",
        "_centroid_update_kernel = cp.RawKernel(r'''\n",
        "extern \"C\" __global__\n",
        "void update_centroids(const float* X, const int* labels, float* centroids,\n",
        "                     int* counts, int n_samples, int n_features, int n_clusters) {\n",
        "    // Get feature and cluster for this thread\n",
        "    int feature_idx = blockIdx.x;\n",
        "    int cluster_idx = blockIdx.y;\n",
        "\n",
        "    if (feature_idx < n_features && cluster_idx < n_clusters) {\n",
        "        float sum = 0.0f;\n",
        "        int count = 0;\n",
        "\n",
        "        // Sum all points in this cluster (for this feature)\n",
        "        for (int i = 0; i < n_samples; i++) {\n",
        "            if (labels[i] == cluster_idx) {\n",
        "                sum += X[i * n_features + feature_idx];\n",
        "                count++;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Store results - atomic updates not needed since each thread handles a unique cluster-feature pair\n",
        "        centroids[cluster_idx * n_features + feature_idx] = (count > 0) ? (sum / count) : 0.0f;\n",
        "        if (feature_idx == 0) {  // Only update count once per cluster\n",
        "            counts[cluster_idx] = count;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "''', 'update_centroids')\n",
        "\n",
        "def _fast_distances_gpu(X, centroids, use_raw_kernel=True):\n",
        "    \"\"\"Highly optimized distance calculation using CuPy.\n",
        "\n",
        "    Args:\n",
        "        X: Input data array (n_samples, n_features)\n",
        "        centroids: Centroid array (n_clusters, n_features)\n",
        "        use_raw_kernel: Whether to use custom CUDA kernel (faster for smaller datasets)\n",
        "\n",
        "    Returns:\n",
        "        distances: Distance matrix (n_samples, n_clusters)\n",
        "    \"\"\"\n",
        "    # Transfer to GPU if needed (avoid copies if already on GPU)\n",
        "    X_gpu = cp.asarray(X, dtype=cp.float32)\n",
        "    centroids_gpu = cp.asarray(centroids, dtype=cp.float32)\n",
        "\n",
        "    # Get dimensions\n",
        "    n_samples = X_gpu.shape[0]\n",
        "    n_features = X_gpu.shape[1]\n",
        "    n_clusters = centroids_gpu.shape[0]\n",
        "\n",
        "    # For small dimensions, use raw CUDA kernel\n",
        "    if use_raw_kernel and n_features <= 128:\n",
        "        # Allocate output memory\n",
        "        distances = cp.empty((n_samples, n_clusters), dtype=cp.float32)\n",
        "\n",
        "        # Configure kernel launch parameters\n",
        "        threads_per_block = 256\n",
        "        blocks_per_grid = (n_samples * n_clusters + threads_per_block - 1) // threads_per_block\n",
        "\n",
        "        # Launch kernel\n",
        "        _distance_kernel(\n",
        "            (blocks_per_grid,), (threads_per_block,),\n",
        "            (X_gpu, centroids_gpu, distances, n_samples, n_features, n_clusters)\n",
        "        )\n",
        "\n",
        "        return distances\n",
        "\n",
        "    # For larger dimensions, use optimized matrix operations\n",
        "    else:\n",
        "        # Use the ||x-y||² = ||x||² + ||y||² - 2<x,y> identity\n",
        "        # Pre-compute squared norms\n",
        "        X_squared_norms = cp.sum(X_gpu**2, axis=1, keepdims=True)\n",
        "        centroids_squared_norms = cp.sum(centroids_gpu**2, axis=1, keepdims=True).T\n",
        "\n",
        "        # Compute dot products efficiently using BLAS\n",
        "        # Use batching for very large matrices to avoid memory issues\n",
        "        if n_samples > 50000 and n_clusters > 100:\n",
        "            batch_size = 10000\n",
        "            distances = cp.empty((n_samples, n_clusters), dtype=cp.float32)\n",
        "\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                end_idx = min(i + batch_size, n_samples)\n",
        "                batch_dots = cp.dot(X_gpu[i:end_idx], centroids_gpu.T)\n",
        "                batch_dists = X_squared_norms[i:end_idx] + centroids_squared_norms - 2.0 * batch_dots\n",
        "                distances[i:end_idx] = cp.maximum(batch_dists, 0)  # Ensure non-negative\n",
        "        else:\n",
        "            # Compute all at once\n",
        "            dot_products = cp.dot(X_gpu, centroids_gpu.T)\n",
        "            distances = X_squared_norms + centroids_squared_norms - 2.0 * dot_products\n",
        "            # Ensure non-negative distances (numerical stability)\n",
        "            cp.maximum(distances, 0, out=distances)\n",
        "\n",
        "        return distances\n",
        "\n",
        "def assign_labels_gpu(distances, use_raw_kernel=True):\n",
        "    \"\"\"Optimized label assignment using CuPy.\n",
        "\n",
        "    Args:\n",
        "        distances: Distance matrix (n_samples, n_clusters)\n",
        "        use_raw_kernel: Whether to use custom CUDA kernel\n",
        "\n",
        "    Returns:\n",
        "        labels: Cluster assignments for each sample\n",
        "        min_distances: Minimum distance for each sample\n",
        "    \"\"\"\n",
        "    # Ensure data is on GPU\n",
        "    distances_gpu = cp.asarray(distances, dtype=cp.float32)\n",
        "    n_samples, n_clusters = distances_gpu.shape\n",
        "\n",
        "    # For smaller datasets, use custom kernel\n",
        "    if use_raw_kernel and n_clusters <= 256:\n",
        "        # Allocate output memory\n",
        "        labels = cp.empty(n_samples, dtype=cp.int32)\n",
        "        min_distances = cp.empty(n_samples, dtype=cp.float32)\n",
        "\n",
        "        # Configure kernel launch parameters - 1 thread per sample\n",
        "        threads_per_block = 256\n",
        "        blocks_per_grid = (n_samples + threads_per_block - 1) // threads_per_block\n",
        "\n",
        "        # Launch kernel\n",
        "        _label_kernel(\n",
        "            (blocks_per_grid,), (threads_per_block,),\n",
        "            (distances_gpu, labels, min_distances, n_samples, n_clusters)\n",
        "        )\n",
        "    else:\n",
        "        # For larger datasets, use optimized CuPy operations\n",
        "        labels = cp.argmin(distances_gpu, axis=1).astype(cp.int32)\n",
        "        # Extract minimum distances efficiently\n",
        "        min_distances = cp.take_along_axis(\n",
        "            distances_gpu,\n",
        "            cp.expand_dims(labels, axis=1),\n",
        "            axis=1\n",
        "        ).squeeze()\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "def update_centroids_gpu(X, labels, n_clusters, use_raw_kernel=False):\n",
        "    \"\"\"Optimized centroid update using CuPy.\n",
        "\n",
        "    Args:\n",
        "        X: Input data array (n_samples, n_features)\n",
        "        labels: Cluster assignments (n_samples,)\n",
        "        n_clusters: Number of clusters\n",
        "        use_raw_kernel: Whether to use custom CUDA kernel\n",
        "\n",
        "    Returns:\n",
        "        centroids: Updated cluster centroids\n",
        "        counts: Number of points in each cluster\n",
        "    \"\"\"\n",
        "    # Transfer to GPU if needed\n",
        "    X_gpu = cp.asarray(X, dtype=cp.float32)\n",
        "    labels_gpu = cp.asarray(labels, dtype=cp.int32)\n",
        "\n",
        "    n_samples, n_features = X_gpu.shape\n",
        "\n",
        "    # For small datasets with many features, use raw kernel\n",
        "    if use_raw_kernel and n_features > 32 and n_clusters < 64:\n",
        "        # Allocate output memory\n",
        "        centroids = cp.zeros((n_clusters, n_features), dtype=cp.float32)\n",
        "        counts = cp.zeros(n_clusters, dtype=cp.int32)\n",
        "\n",
        "        # Configure kernel - one thread per feature per cluster\n",
        "        # Each thread computes one element of the result\n",
        "        blocks_per_grid = (n_features, n_clusters)\n",
        "        threads_per_block = 1  # Simple reduction\n",
        "\n",
        "        # Launch kernel\n",
        "        _centroid_update_kernel(\n",
        "            blocks_per_grid, (threads_per_block,),\n",
        "            (X_gpu, labels_gpu, centroids, counts, n_samples, n_features, n_clusters)\n",
        "        )\n",
        "    else:\n",
        "        # Use optimized matrix operations for larger datasets\n",
        "        # This method is much faster for typical datasets\n",
        "\n",
        "        # Method 1: One-hot encoding (fastest for moderate n_clusters)\n",
        "        if n_clusters <= 256:\n",
        "            # Create one-hot encoding of labels\n",
        "            one_hot = cp.zeros((n_samples, n_clusters), dtype=cp.float32)\n",
        "            cp.scatter_add(one_hot, 1, labels_gpu.reshape(-1, 1), 1)\n",
        "\n",
        "            # Count points per cluster\n",
        "            counts = cp.sum(one_hot, axis=0, dtype=cp.int32)\n",
        "\n",
        "            # Compute new centroids (matrix multiplication)\n",
        "            centroids = cp.dot(one_hot.T, X_gpu)\n",
        "\n",
        "            # Normalize by counts (avoiding division by zero)\n",
        "            valid_mask = counts > 0\n",
        "            if cp.any(valid_mask):\n",
        "                centroids[valid_mask] = centroids[valid_mask] / counts[valid_mask, cp.newaxis]\n",
        "\n",
        "        # Method 2: For very large number of clusters, use scatter_add\n",
        "        else:\n",
        "            centroids = cp.zeros((n_clusters, n_features), dtype=cp.float32)\n",
        "            counts = cp.zeros(n_clusters, dtype=cp.int32)\n",
        "\n",
        "            # Count points per cluster\n",
        "            cp.scatter_add(counts, labels_gpu, cp.ones(n_samples, dtype=cp.int32))\n",
        "\n",
        "            # For each feature, sum values for each cluster\n",
        "            for j in range(n_features):\n",
        "                feature_values = X_gpu[:, j]\n",
        "                cp.scatter_add(centroids[:, j], labels_gpu, feature_values)\n",
        "\n",
        "            # Normalize\n",
        "            valid_mask = counts > 0\n",
        "            if cp.any(valid_mask):\n",
        "                centroids[valid_mask] = centroids[valid_mask] / counts[valid_mask, cp.newaxis]\n",
        "\n",
        "    # Handle empty clusters by setting to random point\n",
        "    empty_clusters = cp.where(counts == 0)[0]\n",
        "    if len(empty_clusters) > 0:\n",
        "        # Get points furthest from their centroids\n",
        "        if len(centroids) > 0:  # Ensure there's at least one valid centroid\n",
        "            # Calculate distances to assigned centroids\n",
        "            X_reshaped = X_gpu.reshape(n_samples, 1, n_features)\n",
        "            valid_centroids = centroids.reshape(1, n_clusters, n_features)\n",
        "\n",
        "            # Get assigned centroid for each point\n",
        "            assigned_centroids = valid_centroids[:, labels_gpu, :]\n",
        "\n",
        "            # Calculate distances\n",
        "            dists = cp.sum((X_reshaped - assigned_centroids) ** 2, axis=2).squeeze()\n",
        "\n",
        "            # Find furthest points\n",
        "            furthest_indices = cp.argsort(dists)[-len(empty_clusters):]\n",
        "\n",
        "            # Assign these points to empty clusters\n",
        "            empty_centroids = X_gpu[furthest_indices]\n",
        "            centroids[empty_clusters] = empty_centroids\n",
        "            counts[empty_clusters] = 1\n",
        "\n",
        "    return centroids, counts\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _fast_distances_cpu(X, centroids):\n",
        "    \"\"\"Highly optimized squared Euclidean distances with Numba.\n",
        "    Specifically tuned for 2-core CPU on Google Colab.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    distances = np.empty((n_samples, n_clusters), dtype=np.float32)\n",
        "\n",
        "    # Precompute centroids squared norms (small array, fast)\n",
        "    centroid_norms = np.empty(n_clusters, dtype=np.float32)\n",
        "    for j in range(n_clusters):\n",
        "        norm = 0.0\n",
        "        for k in range(n_features):\n",
        "            val = centroids[j, k]\n",
        "            norm += val * val\n",
        "        centroid_norms[j] = norm\n",
        "\n",
        "    # Process samples in parallel chunks optimized for 2 cores\n",
        "    chunk_size = max(1, n_samples // 2)  # Split work evenly for 2 cores\n",
        "\n",
        "    for chunk_idx in prange(2):  # Explicitly use 2 cores\n",
        "        start = chunk_idx * chunk_size\n",
        "        end = min(n_samples, (chunk_idx + 1) * chunk_size)\n",
        "\n",
        "        # Process samples in this chunk\n",
        "        for i in range(start, end):\n",
        "            # Compute sample norm once\n",
        "            x_norm = 0.0\n",
        "            for k in range(n_features):\n",
        "                val = X[i, k]\n",
        "                x_norm += val * val\n",
        "\n",
        "            # Compute distances to all centroids\n",
        "            for j in range(n_clusters):\n",
        "                # Start with precomputed norms\n",
        "                distance = x_norm + centroid_norms[j]\n",
        "\n",
        "                # Subtract 2 * dot product with manual unrolling for better vectorization\n",
        "                dot_product = 0.0\n",
        "                # Process 4 elements at a time when possible (SIMD friendly)\n",
        "                for k in range(0, n_features - 3, 4):\n",
        "                    dot_product += X[i, k] * centroids[j, k]\n",
        "                    dot_product += X[i, k+1] * centroids[j, k+1]\n",
        "                    dot_product += X[i, k+2] * centroids[j, k+2]\n",
        "                    dot_product += X[i, k+3] * centroids[j, k+3]\n",
        "\n",
        "                # Handle remaining elements\n",
        "                for k in range((n_features // 4) * 4, n_features):\n",
        "                    dot_product += X[i, k] * centroids[j, k]\n",
        "\n",
        "                distances[i, j] = distance - 2.0 * dot_product\n",
        "\n",
        "    return distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _assign_labels_cpu(distances):\n",
        "    \"\"\"Optimized label assignment with Numba.\n",
        "    Tuned for 2-core processing.\"\"\"\n",
        "    n_samples = distances.shape[0]\n",
        "    n_clusters = distances.shape[1]\n",
        "    labels = np.empty(n_samples, dtype=np.int32)\n",
        "    min_distances = np.empty(n_samples, dtype=np.float32)\n",
        "\n",
        "    # Divide work for 2 cores\n",
        "    chunk_size = max(1, n_samples // 2)\n",
        "\n",
        "    for chunk_idx in prange(2):  # Explicitly use 2 cores\n",
        "        start = chunk_idx * chunk_size\n",
        "        end = min(n_samples, (chunk_idx + 1) * chunk_size)\n",
        "\n",
        "        for i in range(start, end):\n",
        "            min_idx = 0\n",
        "            min_dist = distances[i, 0]\n",
        "\n",
        "            # Manual unrolling to help compiler vectorize\n",
        "            j = 1\n",
        "            while j < n_clusters - 1:  # Process 2 at a time when possible\n",
        "                dist1 = distances[i, j]\n",
        "                dist2 = distances[i, j+1]\n",
        "\n",
        "                # Update if first distance is smaller\n",
        "                if dist1 < min_dist:\n",
        "                    min_dist = dist1\n",
        "                    min_idx = j\n",
        "\n",
        "                # Update if second distance is smaller\n",
        "                if dist2 < min_dist:\n",
        "                    min_dist = dist2\n",
        "                    min_idx = j+1\n",
        "\n",
        "                j += 2\n",
        "\n",
        "            # Handle last element if n_clusters is odd\n",
        "            if j < n_clusters:\n",
        "                dist = distances[i, j]\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    min_idx = j\n",
        "\n",
        "            labels[i] = min_idx\n",
        "            min_distances[i] = min_dist\n",
        "\n",
        "    return labels, min_distances\n",
        "\n",
        "@njit(parallel=True, fastmath=True, cache=True)\n",
        "def _update_centroids_cpu(X, labels, n_clusters):\n",
        "    \"\"\"Optimized centroid update for 2-core CPU.\n",
        "    Uses thread-local storage to avoid synchronization.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    # Use thread-local storage to avoid locks (optimized for 2 cores)\n",
        "    local_sums = np.zeros((2, n_clusters, n_features), dtype=np.float64)\n",
        "    local_counts = np.zeros((2, n_clusters), dtype=np.int32)\n",
        "\n",
        "    # Chunk size for 2 threads\n",
        "    chunk_size = max(1, n_samples // 2)\n",
        "\n",
        "    # Accumulate in thread-local storage\n",
        "    for thread_id in prange(2):  # Explicitly use 2 cores\n",
        "        start = thread_id * chunk_size\n",
        "        end = min(n_samples, (thread_id + 1) * chunk_size)\n",
        "\n",
        "        # Process points assigned to this thread\n",
        "        for i in range(start, end):\n",
        "            cluster_id = labels[i]\n",
        "            local_counts[thread_id, cluster_id] += 1\n",
        "\n",
        "            # Accumulate sum of points in thread-local storage\n",
        "            for j in range(n_features):\n",
        "                local_sums[thread_id, cluster_id, j] += X[i, j]\n",
        "\n",
        "    # Combine thread-local results\n",
        "    centroids = np.zeros((n_clusters, n_features), dtype=np.float64)\n",
        "    counts = np.zeros(n_clusters, dtype=np.int32)\n",
        "\n",
        "    for t in range(2):  # Combine from 2 threads\n",
        "        for k in range(n_clusters):\n",
        "            counts[k] += local_counts[t, k]\n",
        "            for j in range(n_features):\n",
        "                centroids[k, j] += local_sums[t, k, j]\n",
        "\n",
        "    # Calculate means - avoid division by zero\n",
        "    for k in range(n_clusters):\n",
        "        if counts[k] > 0:\n",
        "            inv_count = 1.0 / counts[k]\n",
        "            for j in range(n_features):\n",
        "                centroids[k, j] *= inv_count\n",
        "\n",
        "    return centroids, counts\n",
        "\n",
        "# Unified interface\n",
        "def _fast_distances(X, centroids):\n",
        "    if HAS_GPU:\n",
        "        return compute_distances(X, centroids)\n",
        "    else:\n",
        "        return _fast_distances_cpu(X, centroids)\n",
        "\n",
        "def _assign_labels_numba(distances):\n",
        "    if HAS_GPU:\n",
        "        return assign_labels(distances)\n",
        "    else:\n",
        "        return _assign_labels_cpu(distances)\n",
        "\n",
        "def _update_centroids_numba(X, labels, n_clusters):\n",
        "    if HAS_GPU:\n",
        "        return _update_centroids_gpu(X, labels, n_clusters)\n",
        "    else:\n",
        "        return _update_centroids_cpu(X, labels, n_clusters)\n",
        "\n",
        "# Main kmeans function\n",
        "class HybridBottomUpKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Improved K-means implementation with hybrid bottom-up approach and optimized performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, default=8\n",
        "        The number of clusters to form.\n",
        "\n",
        "    max_iterations : int, default=300\n",
        "        Maximum number of iterations of the k-means algorithm.\n",
        "\n",
        "    tolerance : float, default=1e-4\n",
        "        Relative tolerance for convergence.\n",
        "\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls randomness.\n",
        "\n",
        "    batch_size_factor : float or str, default='auto'\n",
        "        Initial batch size as percentage of data.\n",
        "\n",
        "    batch_growth_factor : float or str, default='auto'\n",
        "        Factor to grow the batch size in each iteration.\n",
        "\n",
        "    verbose : bool, default=False\n",
        "        Verbosity mode.\n",
        "\n",
        "    init : {'k-means++', 'random'}, default='k-means++'\n",
        "        Method for initialization.\n",
        "\n",
        "    early_stopping_factor : float, default=0.001\n",
        "        Fraction of inertia change to trigger early stopping.\n",
        "\n",
        "    n_init : int, default=3\n",
        "        Number of times to run with different seeds.\n",
        "\n",
        "    hybrid_threshold : float, default=0.5\n",
        "        Threshold to switch from bottom-up to standard k-means.\n",
        "\n",
        "    n_jobs : int, default=None\n",
        "        Number of parallel jobs for computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, batch_size_factor='auto',\n",
        "                 batch_growth_factor='auto', verbose=False, init='k-means++',\n",
        "                 early_stopping_factor=0.001, n_init=3,\n",
        "                 hybrid_threshold=0.5, n_jobs=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.batch_size_factor = batch_size_factor\n",
        "        self.batch_growth_factor = batch_growth_factor\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.early_stopping_factor = early_stopping_factor\n",
        "        self.n_init = n_init\n",
        "        self.hybrid_threshold = hybrid_threshold\n",
        "        self.n_jobs = n_jobs\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _analyze_data_characteristics(self, X):\n",
        "        \"\"\"Quick analysis of data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Detect if data is sparse\n",
        "        is_sparse_matrix = sparse.issparse(X)\n",
        "\n",
        "        # For large datasets, use sampling\n",
        "        sample_size = min(1000, n_samples)\n",
        "        if is_sparse_matrix:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            sparsity = 1.0 - (X[sample_indices].count_nonzero() / (sample_size * n_features))\n",
        "        else:\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=False)\n",
        "            X_sample = X[sample_indices]\n",
        "            sparsity = np.sum(X_sample == 0) / (sample_size * n_features)\n",
        "\n",
        "        # High dimensionality check\n",
        "        high_dimensionality = n_features > 100 or n_features > np.sqrt(n_samples)\n",
        "\n",
        "        # Size category\n",
        "        if n_samples < 10000:\n",
        "            size_category = 'small'\n",
        "        elif n_samples < 100000:\n",
        "            size_category = 'medium'\n",
        "        else:\n",
        "            size_category = 'large'\n",
        "\n",
        "        # Determine data type\n",
        "        if sparsity > 0.8:\n",
        "            data_type = 'sparse'\n",
        "        elif high_dimensionality:\n",
        "            data_type = 'high_dim'\n",
        "        else:\n",
        "            if size_category == 'small':\n",
        "                # For small datasets, try to detect tight clusters\n",
        "                try:\n",
        "                    subsample = X[sample_indices][:100] if not is_sparse_matrix else X[sample_indices][:100].toarray()\n",
        "                    distances = pairwise_distances(subsample, metric='euclidean')\n",
        "                    distances_flat = distances[np.triu_indices(distances.shape[0], k=1)]\n",
        "                    cv = np.std(distances_flat) / np.mean(distances_flat) if len(distances_flat) > 0 and np.mean(distances_flat) > 0 else 0\n",
        "                    tight_clusters = cv > 1.0\n",
        "                    data_type = 'dense_tight' if tight_clusters else 'standard'\n",
        "                except:\n",
        "                    data_type = 'standard'\n",
        "            else:\n",
        "                data_type = 'standard'\n",
        "\n",
        "        return {\n",
        "            'data_type': data_type,\n",
        "            'size_category': size_category,\n",
        "            'sparsity': sparsity,\n",
        "            'high_dimensionality': high_dimensionality\n",
        "        }\n",
        "\n",
        "    def _set_dynamic_parameters(self, X):\n",
        "        \"\"\"Set optimized parameters based on data characteristics.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Analyze data characteristics\n",
        "        chars = self._analyze_data_characteristics(X)\n",
        "        data_type = chars['data_type']\n",
        "        size_category = chars['size_category']\n",
        "\n",
        "        # Store for reference\n",
        "        self.data_characteristics_ = chars\n",
        "\n",
        "        # Set parameters based on data type and size\n",
        "        if data_type == 'sparse':\n",
        "            # For sparse data: larger initial batch, faster growth\n",
        "            self._batch_size_factor = 0.15 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 3.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.4  # Switch earlier for sparse data\n",
        "\n",
        "        elif data_type == 'dense_tight':\n",
        "            # For tight clusters: smaller initial batch, moderate growth\n",
        "            self._batch_size_factor = 0.05 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.6  # Need more coverage before switching\n",
        "\n",
        "        elif data_type == 'high_dim':\n",
        "            # For high-dimensional: moderate batch, higher growth\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.5 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = 0.5\n",
        "\n",
        "        else:  # standard\n",
        "            self._batch_size_factor = 0.1 if self.batch_size_factor == 'auto' else self.batch_size_factor\n",
        "            self._batch_growth_factor = 2.0 if self.batch_growth_factor == 'auto' else self.batch_growth_factor\n",
        "            self._hybrid_threshold = self.hybrid_threshold\n",
        "\n",
        "        # Adjust for dataset size\n",
        "        if size_category == 'large':\n",
        "            # For very large datasets, start smaller and grow faster\n",
        "            self._batch_size_factor = min(0.01, self._batch_size_factor)\n",
        "            self._batch_growth_factor = max(self._batch_growth_factor, 3.0)\n",
        "\n",
        "        # For very small datasets, just use all points\n",
        "        if n_samples < 500:\n",
        "            self._batch_size_factor = 1.0\n",
        "            self._hybrid_threshold = 0.99\n",
        "\n",
        "        # Set initial batch size (at least 3 * n_clusters)\n",
        "        self._initial_batch_size = max(int(n_samples * self._batch_size_factor), self.n_clusters * 3)\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"Data type: {data_type}, size: {size_category}\")\n",
        "            logger.info(f\"Parameters: initial_batch={self._initial_batch_size}, \"\n",
        "                       f\"growth_factor={self._batch_growth_factor:.1f}, \"\n",
        "                       f\"hybrid_threshold={self._hybrid_threshold:.2f}\")\n",
        "\n",
        "    def _initialize_centroids(self, X, seed=None):\n",
        "        \"\"\"Fast centroid initialization.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            # Simple stratified random selection\n",
        "            indices = random_state.choice(n_samples, size=self.n_clusters, replace=False)\n",
        "            if sparse.issparse(X):\n",
        "                return X[indices].toarray()\n",
        "            else:\n",
        "                return X[indices].copy()\n",
        "\n",
        "        elif self.init == 'k-means++':\n",
        "            # Optimized k-means++ implementation\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "\n",
        "            # Choose first centroid randomly\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            if sparse.issparse(X):\n",
        "                centroids[0] = X[first_idx].toarray().flatten()\n",
        "            else:\n",
        "                centroids[0] = X[first_idx].copy()\n",
        "\n",
        "            # For large datasets, implement k-means++ with sampling\n",
        "            if n_samples > 10000:\n",
        "                # Use a sample for faster initialization\n",
        "                sample_size = min(10000, n_samples)\n",
        "                sample_indices = random_state.choice(n_samples, size=sample_size, replace=False)\n",
        "\n",
        "                if sparse.issparse(X):\n",
        "                    X_sample = X[sample_indices].toarray()\n",
        "                else:\n",
        "                    X_sample = X[sample_indices]\n",
        "\n",
        "                # Execute k-means++ on the sample\n",
        "                for c in range(1, self.n_clusters):\n",
        "                    # Calculate distances to closest centroid\n",
        "                    min_dists = pairwise_distances(X_sample, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "\n",
        "                    # Select next centroid with probability proportional to squared distance\n",
        "                    probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                    next_idx = random_state.choice(sample_size, p=probs)\n",
        "                    centroids[c] = X_sample[next_idx].copy()\n",
        "\n",
        "            else:\n",
        "                # Standard k-means++ for smaller datasets\n",
        "                if sparse.issparse(X):\n",
        "                    X_dense = X.toarray()\n",
        "\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X_dense, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X_dense[next_idx].copy()\n",
        "                else:\n",
        "                    for c in range(1, self.n_clusters):\n",
        "                        min_dists = pairwise_distances(X, centroids[:c], metric='euclidean', squared=True).min(axis=1)\n",
        "                        probs = min_dists / min_dists.sum() if min_dists.sum() > 0 else None\n",
        "                        next_idx = random_state.choice(n_samples, p=probs)\n",
        "                        centroids[c] = X[next_idx].copy()\n",
        "\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances_parallel(self, X, centroids):\n",
        "        \"\"\"Compute distances in parallel for better performance.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_jobs = self.n_jobs or 1\n",
        "\n",
        "        if sparse.issparse(X):\n",
        "            # For sparse matrices, use specialized handling\n",
        "            return pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        if n_jobs <= 1 or n_samples < 1000:\n",
        "            # Use optimized Numba implementation for single-threaded or small datasets\n",
        "            if NUMBA_AVAILABLE:\n",
        "                return _fast_distances(X, centroids)\n",
        "            else:\n",
        "                return pairwise_distances(X, centroids, metric='euclidean', squared=True)\n",
        "\n",
        "        # For multi-threaded computation on larger datasets\n",
        "        # Process in blocks for better cache locality\n",
        "        block_size = max(100, n_samples // n_jobs)\n",
        "        n_blocks = (n_samples + block_size - 1) // block_size\n",
        "\n",
        "        # Prepare blocks\n",
        "        blocks = []\n",
        "        for i in range(n_blocks):\n",
        "            start_idx = i * block_size\n",
        "            end_idx = min(start_idx + block_size, n_samples)\n",
        "            blocks.append((start_idx, end_idx))\n",
        "\n",
        "        # Process blocks in parallel with joblib\n",
        "        if NUMBA_AVAILABLE:\n",
        "            results = Parallel(n_jobs=n_jobs)(\n",
        "                delayed(_fast_distances_block)(X, centroids, start, end)\n",
        "                for start, end in blocks\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            distances = np.vstack(results)\n",
        "        else:\n",
        "            # Fall back to sklearn's implementation\n",
        "            distances = pairwise_distances(X, centroids, metric='euclidean', squared=True, n_jobs=n_jobs)\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _select_next_batch(self, X, current_active, distances, batch_size, labels):\n",
        "        \"\"\"Optimized batch selection focusing on informative points.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Quick return if all points are active or batch_size is 0\n",
        "        if len(current_active) >= n_samples or batch_size <= 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Create inactive mask efficiently\n",
        "        active_mask = np.zeros(n_samples, dtype=bool)\n",
        "        active_mask[current_active] = True\n",
        "        inactive_indices = np.where(~active_mask)[0]\n",
        "\n",
        "        # Quick return if no inactive points\n",
        "        if len(inactive_indices) == 0:\n",
        "            return np.array([], dtype=np.int32)\n",
        "\n",
        "        # Get distances for inactive points\n",
        "        inactive_distances = distances[inactive_indices]\n",
        "        inactive_labels = labels[inactive_indices]\n",
        "\n",
        "        # Quick heuristic for batch selection - prioritize high-impact points\n",
        "        selected_indices = []\n",
        "\n",
        "        # 1. Take points closest to centroids (to improve centroid positions)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = inactive_indices[inactive_labels == k]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                # Get closest points for this cluster\n",
        "                num_to_take = max(1, batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                selected_indices.extend(closest)\n",
        "\n",
        "        # 2. Take some boundary points (helps with cluster separation)\n",
        "        if self.n_clusters > 1:\n",
        "            # Margins between closest and second closest centroid\n",
        "            sorted_distances = np.sort(inactive_distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            # Small margins indicate points near boundaries\n",
        "            num_boundary = max(1, batch_size // 4)\n",
        "            boundary_points = inactive_indices[np.argsort(margins)[:num_boundary]]\n",
        "            selected_indices.extend(boundary_points)\n",
        "\n",
        "        # 3. Add some outliers (for exploration)\n",
        "        min_distances = np.min(inactive_distances, axis=1)\n",
        "        num_outliers = max(1, batch_size // 10)\n",
        "        outlier_points = inactive_indices[np.argsort(-min_distances)[:num_outliers]]\n",
        "        selected_indices.extend(outlier_points)\n",
        "\n",
        "        # Ensure uniqueness and limit to batch_size\n",
        "        selected_indices = list(set(selected_indices))\n",
        "        if len(selected_indices) > batch_size:\n",
        "            selected_indices = selected_indices[:batch_size]\n",
        "\n",
        "        # If we need more points, add random ones\n",
        "        if len(selected_indices) < batch_size:\n",
        "            remaining = batch_size - len(selected_indices)\n",
        "            available = list(set(inactive_indices) - set(selected_indices))\n",
        "            if available:\n",
        "                random_indices = np.random.choice(available,\n",
        "                                               size=min(remaining, len(available)),\n",
        "                                               replace=False)\n",
        "                selected_indices.extend(random_indices)\n",
        "\n",
        "        return np.array(selected_indices, dtype=np.int32)\n",
        "\n",
        "    def _standard_kmeans_iteration(self, X, centroids):\n",
        "        \"\"\"Run a single iteration of standard k-means on full dataset.\"\"\"\n",
        "        # Compute distances and assign labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.zeros_like(centroids)\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_mask = (labels == k)\n",
        "            if np.any(cluster_mask):\n",
        "                if sparse.issparse(X):\n",
        "                    new_centroids[k] = X[cluster_mask].mean(axis=0).A1\n",
        "                else:\n",
        "                    new_centroids[k] = np.mean(X[cluster_mask], axis=0)\n",
        "            else:\n",
        "                new_centroids[k] = centroids[k]\n",
        "\n",
        "        # Calculate inertia and centroid shift\n",
        "        inertia = np.sum(np.min(distances, axis=1))\n",
        "        centroid_shift = np.sqrt(np.sum((centroids - new_centroids)**2))\n",
        "\n",
        "        return new_centroids, labels, distances, inertia, centroid_shift\n",
        "\n",
        "    def _run_kmeans(self, X, seed=None):\n",
        "        \"\"\"Run hybrid k-means algorithm.\"\"\"\n",
        "        start_time = time.time()\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(seed if seed is not None else self.random_state)\n",
        "\n",
        "        # Set dynamic parameters based on data characteristics\n",
        "        self._set_dynamic_parameters(X)\n",
        "\n",
        "        # Initialize centroids\n",
        "        centroids = self._initialize_centroids(X, seed)\n",
        "        labels = np.zeros(n_samples, dtype=np.int32)\n",
        "        iteration_table = []\n",
        "        hybrid_switch_iter = -1\n",
        "\n",
        "        # Compute initial distances and labels\n",
        "        distances = self._compute_distances_parallel(X, centroids)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Initialize active set with strategic selection\n",
        "        initial_indices = []\n",
        "\n",
        "        # Include points close to each centroid\n",
        "        for k in range(self.n_clusters):\n",
        "            cluster_points = np.where(labels == k)[0]\n",
        "            if len(cluster_points) > 0:\n",
        "                cluster_distances = distances[cluster_points][:, k]\n",
        "                num_to_take = max(1, self._initial_batch_size // (2 * self.n_clusters))\n",
        "                closest = cluster_points[np.argsort(cluster_distances)[:num_to_take]]\n",
        "                initial_indices.extend(closest)\n",
        "\n",
        "        # Include boundary points for better separation\n",
        "        if self.n_clusters > 1:\n",
        "            sorted_distances = np.sort(distances, axis=1)\n",
        "            margins = sorted_distances[:, 1] - sorted_distances[:, 0]\n",
        "            num_boundary = max(1, self._initial_batch_size // 4)\n",
        "            boundary_points = np.argsort(margins)[:num_boundary]\n",
        "            initial_indices.extend(boundary_points)\n",
        "\n",
        "        # Ensure unique indices and limit to initial_batch_size\n",
        "        active_indices = np.array(list(set(initial_indices)), dtype=np.int32)\n",
        "        if len(active_indices) > self._initial_batch_size:\n",
        "            active_indices = active_indices[:self._initial_batch_size]\n",
        "\n",
        "        # For very small datasets, use all points\n",
        "        if n_samples <= self._initial_batch_size:\n",
        "            active_indices = np.arange(n_samples)\n",
        "\n",
        "        # Track convergence\n",
        "        prev_inertia = float('inf')\n",
        "        stability_counter = 0\n",
        "        prev_active_size = len(active_indices)\n",
        "\n",
        "        # Main iteration loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "            iter_start = time.time()\n",
        "            n_iter = iteration + 1\n",
        "            old_centroids = centroids.copy()\n",
        "\n",
        "            # Calculate coverage ratio\n",
        "            coverage_ratio = len(active_indices) / n_samples\n",
        "\n",
        "            if coverage_ratio < self._hybrid_threshold:\n",
        "                # PHASE 1: BOTTOM-UP APPROACH\n",
        "                if len(active_indices) > 0:\n",
        "                    # Extract active data\n",
        "                    if sparse.issparse(X):\n",
        "                        X_active = X[active_indices].toarray()\n",
        "                    else:\n",
        "                        X_active = X[active_indices]\n",
        "                    active_labels = labels[active_indices]\n",
        "\n",
        "                    # Update centroids using active points\n",
        "                    if NUMBA_AVAILABLE and not sparse.issparse(X):\n",
        "                        new_centroids, counts = _update_centroids_numba(X_active, active_labels, self.n_clusters)\n",
        "                        # Handle empty clusters\n",
        "                        for k in range(self.n_clusters):\n",
        "                            if counts[k] == 0:\n",
        "                                # Use old centroid or a random point\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "                    else:\n",
        "                        new_centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "                        for k in range(self.n_clusters):\n",
        "                            cluster_mask = (active_labels == k)\n",
        "                            if np.any(cluster_mask):\n",
        "                                new_centroids[k] = np.mean(X_active[cluster_mask], axis=0)\n",
        "                            else:\n",
        "                                # Handle empty clusters\n",
        "                                if iteration > 0:\n",
        "                                    new_centroids[k] = old_centroids[k]\n",
        "                                else:\n",
        "                                    idx = random_state.randint(len(X_active))\n",
        "                                    new_centroids[k] = X_active[idx]\n",
        "\n",
        "                    centroids = new_centroids\n",
        "\n",
        "                # Compute distances for all points\n",
        "                distances = self._compute_distances_parallel(X, centroids)\n",
        "                new_labels = np.argmin(distances, axis=1)\n",
        "\n",
        "                # Track label changes in active set\n",
        "                active_changed = np.sum(new_labels[active_indices] != labels[active_indices])\n",
        "                active_changed_pct = active_changed / len(active_indices) if len(active_indices) > 0 else 0\n",
        "                labels = new_labels\n",
        "\n",
        "                # Calculate partial inertia\n",
        "                min_distances = np.min(distances, axis=1)\n",
        "                active_inertia = np.sum(min_distances[active_indices])\n",
        "                total_inertia = np.sum(min_distances)\n",
        "\n",
        "                # Adaptive batch size growth\n",
        "                growth_factor = self._batch_growth_factor\n",
        "                if active_changed_pct > 0.1:\n",
        "                    growth_factor *= 0.8  # Slow down if unstable\n",
        "                elif active_changed_pct < 0.01 and iteration > 1:\n",
        "                    growth_factor *= 1.5  # Speed up if stable\n",
        "\n",
        "                # Calculate next batch size\n",
        "                next_batch_size = int(self._initial_batch_size * (growth_factor ** iteration))\n",
        "                next_batch_size = min(next_batch_size, n_samples - len(active_indices))\n",
        "\n",
        "                # Select next batch of points\n",
        "                new_batch = self._select_next_batch(X, active_indices, distances, next_batch_size, labels)\n",
        "\n",
        "                # Calculate centroid shift\n",
        "                centroid_shift = np.sqrt(np.sum((old_centroids - centroids)**2))\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'bottom-up',\n",
        "                    'active_points': len(active_indices),\n",
        "                    'coverage': len(active_indices) / n_samples * 100,\n",
        "                    'active_changed': active_changed,\n",
        "                    'active_changed_pct': active_changed_pct * 100,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': len(new_batch),\n",
        "                    'inertia': active_inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (bottom-up): {len(active_indices)/n_samples*100:.1f}% points, \"\n",
        "                              f\"{active_changed_pct*100:.1f}% changed, {iter_time:.3f}s\")\n",
        "\n",
        "                # Add new batch to active set\n",
        "                if len(new_batch) > 0:\n",
        "                    active_indices = np.append(active_indices, new_batch)\n",
        "\n",
        "                # Early stopping conditions\n",
        "                # 1. Centroid stability\n",
        "                if centroid_shift < self.tolerance and len(active_indices) == n_samples:\n",
        "                    break\n",
        "\n",
        "                # 2. Active set not growing but should be\n",
        "                if len(active_indices) == prev_active_size and next_batch_size > 0:\n",
        "                    # No growth when expected - might be converged\n",
        "                    break\n",
        "\n",
        "                # 3. Inertia stability\n",
        "                if prev_inertia > 0 and abs(active_inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 3:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = active_inertia\n",
        "                prev_active_size = len(active_indices)\n",
        "\n",
        "            else:\n",
        "                # PHASE 2: STANDARD K-MEANS ON FULL DATASET\n",
        "                if hybrid_switch_iter == -1:\n",
        "                    hybrid_switch_iter = iteration\n",
        "                    if self.verbose:\n",
        "                        logger.info(f\"Switching to standard k-means at iteration {n_iter} \"\n",
        "                                  f\"with {len(active_indices)/n_samples*100:.1f}% coverage\")\n",
        "\n",
        "                # Run standard k-means iteration\n",
        "                centroids, labels, distances, inertia, centroid_shift = self._standard_kmeans_iteration(X, centroids)\n",
        "\n",
        "                # Record iteration information\n",
        "                iter_time = time.time() - iter_start\n",
        "                iteration_info = {\n",
        "                    'iteration': n_iter,\n",
        "                    'phase': 'standard',\n",
        "                    'active_points': n_samples,\n",
        "                    'coverage': 100.0,\n",
        "                    'active_changed': np.nan,\n",
        "                    'active_changed_pct': np.nan,\n",
        "                    'centroid_shift': centroid_shift,\n",
        "                    'new_points_added': 0,\n",
        "                    'inertia': inertia,\n",
        "                    'time': iter_time\n",
        "                }\n",
        "                iteration_table.append(iteration_info)\n",
        "\n",
        "                if self.verbose and n_iter % 5 == 0:\n",
        "                    logger.info(f\"Iter {n_iter} (standard): inertia={inertia:.1f}, \"\n",
        "                              f\"shift={centroid_shift:.6f}, {iter_time:.3f}s\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if centroid_shift < self.tolerance:\n",
        "                    break\n",
        "\n",
        "                # Check for inertia stability\n",
        "                if prev_inertia > 0 and abs(inertia - prev_inertia) / prev_inertia < self.early_stopping_factor:\n",
        "                    stability_counter += 1\n",
        "                else:\n",
        "                    stability_counter = 0\n",
        "\n",
        "                if stability_counter >= 2:\n",
        "                    break\n",
        "\n",
        "                prev_inertia = inertia\n",
        "\n",
        "        # Final full iteration to ensure all points are used\n",
        "        if coverage_ratio < 1.0:\n",
        "            centroids, labels, distances, inertia, _ = self._standard_kmeans_iteration(X, centroids)\n",
        "        else:\n",
        "            inertia = np.sum(np.min(distances, axis=1))\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means completed in {n_iter} iterations, {total_time:.3f}s. Inertia: {inertia:.1f}\")\n",
        "            if hybrid_switch_iter > 0:\n",
        "                logger.info(f\"Switched to standard K-means at iteration {hybrid_switch_iter+1}\")\n",
        "\n",
        "        return centroids, labels, inertia, n_iter, iteration_table, hybrid_switch_iter\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit k-means clustering.\"\"\"\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        # Run multiple initializations if requested\n",
        "        if self.n_init > 1:\n",
        "            best_inertia = float('inf')\n",
        "            best_centroids = None\n",
        "            best_labels = None\n",
        "            best_n_iter = 0\n",
        "            best_iteration_table = []\n",
        "\n",
        "            seeds = [check_random_state(self.random_state).randint(0, 2**31 - 1) for _ in range(self.n_init)]\n",
        "\n",
        "            for i, seed in enumerate(seeds):\n",
        "                if self.verbose:\n",
        "                    logger.info(f\"K-means initialization {i+1}/{len(seeds)}\")\n",
        "\n",
        "                centroids, labels, inertia, n_iter, iter_table, hybrid_switch = self._run_kmeans(X, seed)\n",
        "\n",
        "                if inertia < best_inertia:\n",
        "                    best_centroids = centroids.copy()\n",
        "                    best_labels = labels.copy()\n",
        "                    best_inertia = inertia\n",
        "                    best_n_iter = n_iter\n",
        "                    best_iteration_table = iter_table\n",
        "\n",
        "            self.cluster_centers_ = best_centroids\n",
        "            self.labels_ = best_labels\n",
        "            self.inertia_ = best_inertia\n",
        "            self.n_iter_ = best_n_iter\n",
        "            self.iteration_table_ = best_iteration_table\n",
        "        else:\n",
        "            # Single run\n",
        "            centroids, labels, inertia, n_iter, iteration_table, hybrid_switch = self._run_kmeans(X)\n",
        "\n",
        "            self.cluster_centers_ = centroids\n",
        "            self.labels_ = labels\n",
        "            self.inertia_ = inertia\n",
        "            self.n_iter_ = n_iter\n",
        "            self.iteration_table_ = iteration_table\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "\n",
        "        distances = self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform X to a cluster-distance space.\"\"\"\n",
        "        check_is_fitted(self, ['cluster_centers_'])\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
        "        return self._compute_distances_parallel(X, self.cluster_centers_)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        \"\"\"Returns a table of iteration details.\"\"\"\n",
        "        check_is_fitted(self, ['iteration_table_'])\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            return pd.DataFrame(self.iteration_table_)\n",
        "        except ImportError:\n",
        "            result = \"\"\n",
        "            for info in self.iteration_table_:\n",
        "                result += \", \".join([f\"{k}: {v}\" for k, v in info.items()]) + \"\\n\"\n",
        "            return result\n",
        "# =============================================================================\n",
        "# Existing OptimizedKMeans Implementation\n",
        "# =============================================================================\n",
        "class OptimizedKMeans(BaseEstimator, ClusterMixin):\n",
        "    \"\"\"\n",
        "    Optimized K-means implementation focused on practical performance gains.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=8, max_iterations=300, tolerance=1e-4,\n",
        "                 random_state=None, stable_point_check_interval=5,\n",
        "                 verbose=False, init='random'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.random_state = random_state\n",
        "        self.stable_point_check_interval = stable_point_check_interval\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.iteration_table_ = []\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if self.init == 'random':\n",
        "            centroid_indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
        "            return X[centroid_indices].copy()\n",
        "        elif self.init == 'k-means++':\n",
        "            centroids = np.zeros((self.n_clusters, n_features), dtype=X.dtype)\n",
        "            first_idx = random_state.randint(n_samples)\n",
        "            centroids[0] = X[first_idx].copy()\n",
        "            closest_dist_sq = np.zeros(n_samples)\n",
        "            for c in range(1, self.n_clusters):\n",
        "                if NUMBA_AVAILABLE:\n",
        "                    distances = _fast_distances(X, centroids[:c])\n",
        "                    closest_dist_sq = np.min(distances, axis=1)\n",
        "                else:\n",
        "                    for i in range(n_samples):\n",
        "                        min_dist = float('inf')\n",
        "                        for j in range(c):\n",
        "                            dist = np.sum((X[i] - centroids[j])**2)\n",
        "                            min_dist = min(min_dist, dist)\n",
        "                        closest_dist_sq[i] = min_dist\n",
        "                sum_distances = closest_dist_sq.sum()\n",
        "                if sum_distances > 0:\n",
        "                    probs = closest_dist_sq / sum_distances\n",
        "                    cumprobs = np.cumsum(probs)\n",
        "                    r = random_state.rand()\n",
        "                    next_centroid_idx = np.searchsorted(cumprobs, r)\n",
        "                    if next_centroid_idx >= n_samples:\n",
        "                        next_centroid_idx = random_state.randint(n_samples)\n",
        "                else:\n",
        "                    next_centroid_idx = random_state.randint(n_samples)\n",
        "                centroids[c] = X[next_centroid_idx].copy()\n",
        "            return centroids\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
        "\n",
        "    def _compute_distances(self, X, centroids):\n",
        "        if NUMBA_AVAILABLE:\n",
        "            return _fast_distances(X, centroids)\n",
        "        else:\n",
        "            result = np.zeros((X.shape[0], centroids.shape[0]))\n",
        "            for i, centroid in enumerate(centroids):\n",
        "                result[:, i] = np.sum((X - centroid)**2, axis=1)\n",
        "            return result\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = np.zeros((self.n_clusters, X.shape[1]), dtype=X.dtype)\n",
        "        counts = np.bincount(labels, minlength=self.n_clusters)\n",
        "        for k in range(self.n_clusters):\n",
        "            if counts[k] > 0:\n",
        "                mask = (labels == k)\n",
        "                new_centroids[k] = np.sum(X[mask], axis=0) / counts[k]\n",
        "            else:\n",
        "                new_centroids[k] = self.centroids_[k]\n",
        "        return new_centroids\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.centroids_ = self._initialize_centroids(X)\n",
        "        self.labels_ = np.zeros(n_samples, dtype=np.int32)\n",
        "        self.inertia_ = 0.0\n",
        "        self.n_iter_ = 0\n",
        "        self.iteration_table_ = []\n",
        "        stable_points = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "        for iteration in range(self.max_iterations):\n",
        "            self.n_iter_ = iteration + 1\n",
        "            old_centroids = self.centroids_.copy()\n",
        "\n",
        "            if iteration % self.stable_point_check_interval == 0:\n",
        "                stable_points[:] = False\n",
        "\n",
        "            active_indices = np.where(~stable_points)[0]\n",
        "            if len(active_indices) == 0:\n",
        "                break\n",
        "\n",
        "            active_X = X[active_indices]\n",
        "            old_labels = self.labels_[active_indices].copy()\n",
        "            distances = self._compute_distances(active_X, self.centroids_)\n",
        "            new_labels = np.argmin(distances, axis=1)\n",
        "            self.labels_[active_indices] = new_labels\n",
        "            changed_indices = np.where(new_labels != old_labels)[0]\n",
        "            newly_stable = active_indices[np.isin(np.arange(len(active_indices)), changed_indices, invert=True)]\n",
        "            stable_points[newly_stable] = True\n",
        "\n",
        "            iteration_info = {\n",
        "                'iteration': iteration + 1,\n",
        "                'total_points': n_samples,\n",
        "                'active_points': len(active_indices),\n",
        "                'points_changed': len(changed_indices),\n",
        "                'new_stable_points': len(newly_stable),\n",
        "                'cumulative_stable_points': np.sum(stable_points)\n",
        "            }\n",
        "            self.iteration_table_.append(iteration_info)\n",
        "\n",
        "            self.centroids_ = self._update_centroids(X, self.labels_)\n",
        "            centroid_shift = np.max(np.sqrt(np.sum((old_centroids - self.centroids_)**2, axis=1)))\n",
        "            if centroid_shift < self.tolerance:\n",
        "                break\n",
        "\n",
        "            if self.verbose and (iteration + 1) % 10 == 0:\n",
        "                logger.info(f\"Iteration {iteration + 1}: {len(changed_indices)} points changed clusters\")\n",
        "\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        self.inertia_ = np.sum(distances[np.arange(n_samples), self.labels_])\n",
        "        if self.verbose:\n",
        "            logger.info(f\"K-means converged after {self.n_iter_} iterations. Inertia: {self.inertia_:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        check_is_fitted(self, ['centroids_'])\n",
        "        X = check_array(X)\n",
        "        distances = self._compute_distances(X, self.centroids_)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def print_iteration_table(self):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.DataFrame(self.iteration_table_)\n",
        "            print(\"\\nIteration Table:\")\n",
        "            print(df)\n",
        "        except ImportError:\n",
        "            print(\"\\nIteration Table:\")\n",
        "            for info in self.iteration_table_:\n",
        "                print(info)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions for Data Loading, Preprocessing, and Evaluation\n",
        "# =============================================================================\n",
        "def load_wine_data() -> pd.DataFrame:\n",
        "    \"\"\"Load the Wine Quality dataset.\"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        df = pd.read_csv(io.StringIO(data), sep=';')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'fixed acidity': [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "            'volatile acidity': [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "            'citric acid': [0, 0, 0.04, 0.56, 0],\n",
        "            'residual sugar': [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "            'chlorides': [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "            'free sulfur dioxide': [11, 25, 15, 17, 11],\n",
        "            'total sulfur dioxide': [34, 67, 54, 60, 34],\n",
        "            'density': [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "            'pH': [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "            'sulphates': [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "            'alcohol': [9.4, 9.8, 9.8, 9.8, 9.4],\n",
        "            'quality': [5, 5, 5, 6, 5]\n",
        "        })\n",
        "\n",
        "def load_synthetic_data(n_samples=1000, n_features=2, n_clusters=3,\n",
        "                        random_state=42) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate synthetic dataset with known clusters.\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    X = np.concatenate([\n",
        "        np.random.normal(0, 1, (n_samples//3, n_features)),\n",
        "        np.random.normal(4, 1.5, (n_samples//3, n_features)),\n",
        "        np.random.normal(-4, 0.5, (n_samples//3, n_features))\n",
        "    ])\n",
        "    y_true = np.concatenate([\n",
        "        np.zeros(n_samples//3, dtype=int),\n",
        "        np.ones(n_samples//3, dtype=int),\n",
        "        np.full(n_samples//3, 2, dtype=int)\n",
        "    ])\n",
        "    return X, y_true\n",
        "\n",
        "def load_pendigits_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the Pendigits dataset from the UCI repository.\"\"\"\n",
        "    base_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\"\n",
        "    train_url = base_url + \"pendigits.tra\"\n",
        "    test_url = base_url + \"pendigits.tes\"\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_url, header=None)\n",
        "        test_data = pd.read_csv(test_url, header=None)\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "        X = data.iloc[:, :-1].values\n",
        "        y = data.iloc[:, -1].values\n",
        "        return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading pendigits data:\", e)\n",
        "        X_sample = np.random.rand(100, 16)\n",
        "        y_sample = np.random.randint(0, 10, size=100)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def load_mnist_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load the MNIST dataset (8M version).\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\"\n",
        "    try:\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(response.read())) as f:\n",
        "                X = np.load(f)\n",
        "                y = np.load(f)\n",
        "                subset_size = 10000\n",
        "                X = X[:subset_size].reshape(subset_size, -1)\n",
        "                y = y[:subset_size]\n",
        "                return X, y\n",
        "    except Exception as e:\n",
        "        print(\"Error loading MNIST data:\", e)\n",
        "        X_sample = np.random.rand(1000, 784)\n",
        "        y_sample = np.random.randint(0, 10, size=1000)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> Tuple[np.ndarray, Dict, Optional[np.ndarray]]:\n",
        "    \"\"\"Preprocess the wine dataset and return preprocessed features and info.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    df_clean = df_clean.fillna(df_clean.mean())\n",
        "\n",
        "    if 'quality' in df_clean.columns:\n",
        "        quality = df_clean['quality'].values\n",
        "        quality_median = np.median(quality)\n",
        "        y_true = (quality > quality_median).astype(int)\n",
        "        df_clean = df_clean.drop('quality', axis=1)\n",
        "    else:\n",
        "        y_true = None\n",
        "\n",
        "    Q1 = df_clean.quantile(0.25)\n",
        "    Q3 = df_clean.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df_clean = df_clean[~((df_clean < (Q1 - 1.5 * IQR)) |\n",
        "                          (df_clean > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "    if y_true is not None:\n",
        "        y_true = y_true[:len(df_clean)]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df_clean)\n",
        "\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    preprocessing_info = {\n",
        "        'original_shape': df.shape,\n",
        "        'cleaned_shape': df_clean.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
        "        'features': df.columns.tolist() if hasattr(df, 'columns') else None\n",
        "    }\n",
        "\n",
        "    return X_pca, preprocessing_info, y_true\n",
        "\n",
        "def determine_optimal_k(X: np.ndarray, max_k: int = 10) -> int:\n",
        "    \"\"\"Determine optimal number of clusters using multiple methods.\"\"\"\n",
        "    results = {\n",
        "        'k': list(range(2, max_k + 1)),\n",
        "        'inertia': [],\n",
        "        'silhouette': [],\n",
        "        'calinski_harabasz': [],\n",
        "        'davies_bouldin': []\n",
        "    }\n",
        "\n",
        "    for k in range(2, max_k + 1):\n",
        "        kmeans = SklearnKMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        results['inertia'].append(kmeans.inertia_)\n",
        "        results['silhouette'].append(silhouette_score(X, labels))\n",
        "        results['calinski_harabasz'].append(calinski_harabasz_score(X, labels))\n",
        "        results['davies_bouldin'].append(davies_bouldin_score(X, labels))\n",
        "\n",
        "    inertias = np.array(results['inertia'])\n",
        "    if len(inertias) > 2:\n",
        "        diffs = np.diff(inertias)\n",
        "        second_diffs = np.diff(diffs)\n",
        "        elbow_idx = np.argmax(second_diffs) + 2\n",
        "    else:\n",
        "        elbow_idx = 2\n",
        "\n",
        "    silhouette_idx = np.argmax(results['silhouette']) + 2\n",
        "    ch_idx = np.argmax(results['calinski_harabasz']) + 2\n",
        "    db_idx = np.argmin(results['davies_bouldin']) + 2\n",
        "\n",
        "    votes = {k: 0 for k in range(2, max_k + 1)}\n",
        "    votes[elbow_idx] += 1\n",
        "    votes[silhouette_idx] += 1\n",
        "    votes[ch_idx] += 1\n",
        "    votes[db_idx] += 1\n",
        "\n",
        "    optimal_k = max(votes.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    logging.info(f\"Optimal k votes: Elbow={elbow_idx}, Silhouette={silhouette_idx}, \"\n",
        "                 f\"Calinski-Harabasz={ch_idx}, Davies-Bouldin={db_idx}\")\n",
        "    logging.info(f\"Selected optimal k={optimal_k}\")\n",
        "\n",
        "    return optimal_k, results\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark and Evaluation Functions\n",
        "# =============================================================================\n",
        "def run_bench_evaluation():\n",
        "    \"\"\"\n",
        "    Benchmark evaluation comparing BottomUpKMeans, OptimizedKMeans,\n",
        "    and SklearnKMeans on synthetic data.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use synthetic data for benchmarking\n",
        "    X, y_true = load_synthetic_data(n_samples=30000, n_features=5, n_clusters=3)\n",
        "    n_clusters = 3\n",
        "    n_runs = 3\n",
        "\n",
        "    results = {\n",
        "        'times': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'metrics': {'bottomup': [], 'optimized': [], 'sklearn': []},\n",
        "        'iterations': {'bottomup': [], 'optimized': [], 'sklearn': []}\n",
        "    }\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run+1}/{n_runs}\")\n",
        "\n",
        "        # BottomUpKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        bu_kmeans = HybridBottomUpKMeans(n_clusters=n_clusters, random_state=42+run, verbose=True)\n",
        "        bu_kmeans.fit(X)\n",
        "        bu_time = time.time() - start_time\n",
        "        results['times']['bottomup'].append(bu_time)\n",
        "        bu_metrics = {\n",
        "            'silhouette': silhouette_score(X, bu_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, bu_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, bu_kmeans.labels_),\n",
        "            'inertia': bu_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            bu_metrics['adjusted_rand'] = adjusted_rand_score(y_true, bu_kmeans.labels_)\n",
        "            bu_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, bu_kmeans.labels_)\n",
        "        results['metrics']['bottomup'].append(bu_metrics)\n",
        "        results['iterations']['bottomup'].append(bu_kmeans.n_iter_)\n",
        "\n",
        "        # Optionally print the iteration table for the first run\n",
        "        if run == 0:\n",
        "            bu_kmeans.print_iteration_table()\n",
        "\n",
        "        # OptimizedKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run)\n",
        "        opt_kmeans.fit(X)\n",
        "        opt_time = time.time() - start_time\n",
        "        results['times']['optimized'].append(opt_time)\n",
        "        opt_metrics = {\n",
        "            'silhouette': silhouette_score(X, opt_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, opt_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, opt_kmeans.labels_),\n",
        "            'inertia': opt_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            opt_metrics['adjusted_rand'] = adjusted_rand_score(y_true, opt_kmeans.labels_)\n",
        "            opt_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, opt_kmeans.labels_)\n",
        "        results['metrics']['optimized'].append(opt_metrics)\n",
        "        results['iterations']['optimized'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "        # SklearnKMeans evaluation\n",
        "        start_time = time.time()\n",
        "        sk_kmeans = SklearnKMeans(n_clusters=n_clusters, random_state=42+run, init='k-means++')\n",
        "        sk_kmeans.fit(X)\n",
        "        sk_time = time.time() - start_time\n",
        "        results['times']['sklearn'].append(sk_time)\n",
        "        sk_metrics = {\n",
        "            'silhouette': silhouette_score(X, sk_kmeans.labels_),\n",
        "            'calinski_harabasz': calinski_harabasz_score(X, sk_kmeans.labels_),\n",
        "            'davies_bouldin': davies_bouldin_score(X, sk_kmeans.labels_),\n",
        "            'inertia': sk_kmeans.inertia_\n",
        "        }\n",
        "        if y_true is not None:\n",
        "            sk_metrics['adjusted_rand'] = adjusted_rand_score(y_true, sk_kmeans.labels_)\n",
        "            sk_metrics['adjusted_mutual_info'] = adjusted_mutual_info_score(y_true, sk_kmeans.labels_)\n",
        "        results['metrics']['sklearn'].append(sk_metrics)\n",
        "        results['iterations']['sklearn'].append(sk_kmeans.n_iter_)\n",
        "\n",
        "    # Print aggregated benchmark results\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"==================\")\n",
        "    print(f\"Average execution time (BottomUpKMeans): {np.mean(results['times']['bottomup']):.3f} seconds\")\n",
        "    print(f\"Average execution time (OptimizedKMeans): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (SklearnKMeans): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "\n",
        "    print(\"\\nClustering Quality Metrics (averages):\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        m = results['metrics'][method]\n",
        "        avg_silhouette = np.mean([mm['silhouette'] for mm in m])\n",
        "        avg_ch = np.mean([mm['calinski_harabasz'] for mm in m])\n",
        "        avg_db = np.mean([mm['davies_bouldin'] for mm in m])\n",
        "        avg_inertia = np.mean([mm['inertia'] for mm in m])\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"  Silhouette Score: {avg_silhouette:.3f}\")\n",
        "        print(f\"  Calinski-Harabasz: {avg_ch:.3f}\")\n",
        "        print(f\"  Davies-Bouldin: {avg_db:.3f}\")\n",
        "        print(f\"  Inertia: {avg_inertia:.3f}\")\n",
        "        if y_true is not None:\n",
        "            avg_adj_rand = np.mean([mm['adjusted_rand'] for mm in m])\n",
        "            avg_adj_mutual = np.mean([mm['adjusted_mutual_info'] for mm in m])\n",
        "            print(f\"  Adjusted Rand: {avg_adj_rand:.3f}\")\n",
        "            print(f\"  Adjusted Mutual Info: {avg_adj_mutual:.3f}\")\n",
        "\n",
        "    print(\"\\nAverage Iterations:\")\n",
        "    for method in ['bottomup', 'optimized', 'sklearn']:\n",
        "        avg_iter = np.mean(results['iterations'][method])\n",
        "        print(f\"  {method.upper()}: {avg_iter:.1f} iterations\")\n",
        "\n",
        "def print_results(results: Dict, y_true_available: bool = False) -> None:\n",
        "    \"\"\"Print formatted benchmark results.\"\"\"\n",
        "    print(\"\\nBenchmark Results:\")\n",
        "    print(\"=================\")\n",
        "    print(f\"Average execution time (Optimized): {np.mean(results['times']['optimized']):.3f} seconds\")\n",
        "    print(f\"Average execution time (Sklearn): {np.mean(results['times']['sklearn']):.3f} seconds\")\n",
        "    speedup = np.mean(results['times']['sklearn']) / np.mean(results['times']['optimized'])\n",
        "    print(f\"Speedup factor: {speedup:.2f}x\")\n",
        "\n",
        "    print(\"\\nInternal Clustering Quality Metrics:\")\n",
        "    print(f\"Average Silhouette Score (Optimized): {np.mean([m['silhouette'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Silhouette Score (Sklearn): {np.mean([m['silhouette'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Calinski-Harabasz (Optimized): {np.mean([m['calinski_harabasz'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Calinski-Harabasz (Sklearn): {np.mean([m['calinski_harabasz'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Davies-Bouldin (Optimized): {np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Davies-Bouldin (Sklearn): {np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(f\"Average Inertia (Optimized): {np.mean([m['inertia'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "    print(f\"Average Inertia (Sklearn): {np.mean([m['inertia'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    if y_true_available:\n",
        "        print(\"\\nExternal Clustering Quality Metrics (against ground truth):\")\n",
        "        print(f\"Average Adjusted Rand (Optimized): {np.mean([m['adjusted_rand'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Rand (Sklearn): {np.mean([m['adjusted_rand'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "        print(f\"Average Adjusted Mutual Info (Optimized): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['optimized']]):.3f}\")\n",
        "        print(f\"Average Adjusted Mutual Info (Sklearn): {np.mean([m['adjusted_mutual_info'] for m in results['metrics']['sklearn']]):.3f}\")\n",
        "\n",
        "    print(\"\\nConvergence:\")\n",
        "    print(f\"Average iterations (Optimized): {np.mean(results['iterations']['optimized']):.1f}\")\n",
        "    print(f\"Average iterations (Sklearn): {np.mean(results['iterations']['sklearn']):.1f}\")\n",
        "\n",
        "def visualize_results(X: np.ndarray, results: Dict, optim_k_results: Dict = None,\n",
        "                      preprocessing_info: Dict = None, y_true: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"Create visualizations for clustering results.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Execution Time\n",
        "        plt.subplot(3, 2, 1)\n",
        "        times_data = {\n",
        "            'Optimized': np.mean(results['times']['optimized']),\n",
        "            'Sklearn': np.mean(results['times']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(times_data.keys()), y=list(times_data.values()))\n",
        "        for i, v in enumerate(list(times_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}s\", ha='center')\n",
        "        plt.title('Average Execution Time (seconds)', fontsize=14)\n",
        "        plt.ylabel('Time (seconds)')\n",
        "\n",
        "        # 2. Iterations\n",
        "        plt.subplot(3, 2, 2)\n",
        "        iterations_data = {\n",
        "            'Optimized': np.mean(results['iterations']['optimized']),\n",
        "            'Sklearn': np.mean(results['iterations']['sklearn'])\n",
        "        }\n",
        "        ax = sns.barplot(x=list(iterations_data.keys()), y=list(iterations_data.values()))\n",
        "        for i, v in enumerate(list(iterations_data.values())):\n",
        "            ax.text(i, v * 1.01, f\"{v:.1f}\", ha='center')\n",
        "        plt.title('Average Number of Iterations', fontsize=14)\n",
        "        plt.ylabel('Iterations')\n",
        "\n",
        "        # 3. Silhouette Score\n",
        "        plt.subplot(3, 2, 3)\n",
        "        metrics_opt = np.mean([m['silhouette'] for m in results['metrics']['optimized']])\n",
        "        metrics_sk = np.mean([m['silhouette'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[metrics_opt, metrics_sk])\n",
        "        for i, v in enumerate([metrics_opt, metrics_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Silhouette Score (higher is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 4. Davies-Bouldin Score\n",
        "        plt.subplot(3, 2, 4)\n",
        "        db_opt = np.mean([m['davies_bouldin'] for m in results['metrics']['optimized']])\n",
        "        db_sk = np.mean([m['davies_bouldin'] for m in results['metrics']['sklearn']])\n",
        "        ax = sns.barplot(x=['Optimized', 'Sklearn'], y=[db_opt, db_sk])\n",
        "        for i, v in enumerate([db_opt, db_sk]):\n",
        "            ax.text(i, v * 1.01, f\"{v:.3f}\", ha='center')\n",
        "        plt.title('Average Davies-Bouldin Score (lower is better)', fontsize=14)\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # 5. Elbow Method (if provided)\n",
        "        if optim_k_results:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            k_values = optim_k_results['k']\n",
        "            plt.plot(k_values, optim_k_results['inertia'], 'o-', label='Inertia')\n",
        "            plt.title('Elbow Method for Optimal k', fontsize=14)\n",
        "            plt.xlabel('Number of clusters (k)')\n",
        "            plt.ylabel('Inertia')\n",
        "            plt.xticks(k_values)\n",
        "            plt.grid(True)\n",
        "            ax2 = plt.twinx()\n",
        "            sil_scores = np.array(optim_k_results['silhouette'])\n",
        "            norm_sil = (sil_scores - sil_scores.min()) / (sil_scores.max() - sil_scores.min())\n",
        "            ax2.plot(k_values, norm_sil, 'x-', color='red', label='Silhouette (normalized)')\n",
        "            ax2.set_ylabel('Normalized Silhouette Score')\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "        # 6. PCA Visualization of Clusters\n",
        "        if X.shape[1] >= 2:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=len(np.unique(y_true)) if y_true is not None else 3,\n",
        "                                         random_state=42)\n",
        "            opt_labels = opt_kmeans.fit_predict(X)\n",
        "            scatter = plt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.6)\n",
        "            plt.scatter(opt_kmeans.centroids_[:, 0], opt_kmeans.centroids_[:, 1],\n",
        "                        marker='X', s=200, c='red', label='Centroids')\n",
        "            plt.colorbar(scatter)\n",
        "            plt.title('Cluster Visualization (First 2 PCA Components)', fontsize=14)\n",
        "            plt.xlabel('PCA 1')\n",
        "            plt.ylabel('PCA 2')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('kmeans_benchmark_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib or seaborn not available for visualization. Install with: pip install matplotlib seaborn\")\n",
        "\n",
        "def compare_initialization_methods(X: np.ndarray, n_clusters: int, n_runs: int = 5) -> Dict:\n",
        "    \"\"\"Compare random vs k-means++ initialization.\"\"\"\n",
        "    init_methods = ['random', 'k-means++']\n",
        "    results = {method: {'time': [], 'inertia': [], 'iterations': []} for method in init_methods}\n",
        "\n",
        "    for method in init_methods:\n",
        "        for run in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            opt_kmeans = OptimizedKMeans(n_clusters=n_clusters, random_state=42+run, init=method)\n",
        "            opt_kmeans.fit(X)\n",
        "            end_time = time.time()\n",
        "            results[method]['time'].append(end_time - start_time)\n",
        "            results[method]['inertia'].append(opt_kmeans.inertia_)\n",
        "            results[method]['iterations'].append(opt_kmeans.n_iter_)\n",
        "\n",
        "    print(\"\\nInitialization Method Comparison:\")\n",
        "    print(\"================================\")\n",
        "    for method in init_methods:\n",
        "        print(f\"\\n{method.upper()}:\")\n",
        "        print(f\"Average Time: {np.mean(results[method]['time']):.3f} seconds\")\n",
        "        print(f\"Average Inertia: {np.mean(results[method]['inertia']):.3f}\")\n",
        "        print(f\"Average Iterations: {np.mean(results[method]['iterations']):.1f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_pendigits_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the Pendigits dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PENDIGITS DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_pend, y_pend = load_pendigits_data()\n",
        "    print(f\"Pendigits data shape: {X_pend.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_pend))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_pend_scaled = scaler.fit_transform(X_pend)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_pend_pca = pca.fit_transform(X_pend_scaled)\n",
        "    print(f\"Preprocessed Pendigits data shape: {X_pend_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_pend = {\n",
        "        'original_shape': X_pend.shape,\n",
        "        'cleaned_shape': X_pend.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_pend))\n",
        "    print(f\"Running benchmark on Pendigits data with {n_clusters} clusters...\")\n",
        "    pendigits_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "def run_mnist_evaluation():\n",
        "    \"\"\"Evaluate clustering performance on the MNIST dataset.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MNIST DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_mnist, y_mnist = load_mnist_data()\n",
        "    print(f\"MNIST data shape: {X_mnist.shape}\")\n",
        "    print(f\"Unique labels (digits): {len(np.unique(y_mnist))}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
        "    pca = PCA(n_components=0.95)\n",
        "    X_mnist_pca = pca.fit_transform(X_mnist_scaled)\n",
        "    print(f\"Preprocessed MNIST data shape: {X_mnist_pca.shape}\")\n",
        "\n",
        "    preprocessing_info_mnist = {\n",
        "        'original_shape': X_mnist.shape,\n",
        "        'cleaned_shape': X_mnist.shape,\n",
        "        'pca_components': pca.n_components_,\n",
        "        'explained_variance_ratio': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "    n_clusters = len(np.unique(y_mnist))\n",
        "    print(f\"Running benchmark on MNIST data with {n_clusters} clusters...\")\n",
        "    mnist_results = run_bench_evaluation()  # Or call run_benchmark-like function if desired\n",
        "\n",
        "    print_results(mnist_results, y_true_available=True)\n",
        "    visualize_results(X_mnist_pca, mnist_results, optim_k_results=None,\n",
        "                     preprocessing_info=preprocessing_info_mnist, y_true=y_mnist)\n",
        "\n",
        "def run_full_evaluation():\n",
        "    \"\"\"Run complete evaluation on synthetic, wine, and pendigits datasets.\"\"\"\n",
        "    # 1. Synthetic Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SYNTHETIC DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synth, y_synth = load_synthetic_data(n_samples=100000, n_features=10, n_clusters=20)\n",
        "    print(f\"Synthetic data shape: {X_synth.shape}\")\n",
        "    print(f\"Known clusters: 3\")\n",
        "\n",
        "    preprocessing_info_synth = {\"original_shape\": X_synth.shape, \"cleaned_shape\": X_synth.shape}\n",
        "\n",
        "    print(\"\\nRunning benchmark on synthetic data...\")\n",
        "    synth_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(synth_results, y_true_available=True)\n",
        "    visualize_results(X_synth, synth_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_synth, y_true=y_synth)\n",
        "\n",
        "    # 2. Wine Data Test\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"WINE DATA EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    wine_df = load_wine_data()\n",
        "    print(f\"Wine data shape: {wine_df.shape}\")\n",
        "\n",
        "    X_wine, preprocessing_info_wine, y_wine = preprocess_data(wine_df)\n",
        "    print(f\"Preprocessed wine data shape: {X_wine.shape}\")\n",
        "\n",
        "    print(\"\\nRunning benchmark on wine data...\")\n",
        "    wine_results = run_bench_evaluation()  # Or call run_benchmark if preferred\n",
        "    print_results(wine_results, y_true_available=True)\n",
        "    visualize_results(X_wine, wine_results, optim_k_results=None,\n",
        "                      preprocessing_info=preprocessing_info_wine, y_true=y_wine)\n",
        "\n",
        "    # 3. Compare Initialization Methods\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARING INITIALIZATION METHODS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on synthetic data...\")\n",
        "    init_comparison_synth = compare_initialization_methods(X_synth, n_clusters=3, n_runs=5)\n",
        "\n",
        "    print(\"\\nComparing initialization methods on wine data...\")\n",
        "    init_comparison_wine = compare_initialization_methods(X_wine, n_clusters=2, n_runs=5)\n",
        "\n",
        "    # 4. Pendigits and MNIST Data Test\n",
        "    run_pendigits_evaluation()\n",
        "    run_mnist_evaluation()\n",
        "    print(\"\\nComplete evaluation finished.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Main Entry Point\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # To run the benchmark evaluation that includes BottomUpKMeans,\n",
        "    # simply call run_bench_evaluation(). You can also run the full evaluation.\n",
        "    run_bench_evaluation()\n",
        "    # Alternatively, uncomment the following line to run all evaluations:\n",
        "    #run_full_evaluation()\n",
        "    #scalene_profiler.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "0WlpVDRIptyG",
        "outputId": "9138615d-457f-4790-d3d5-e34e2b613431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BENCHMARK EVALUATION: BottomUpKMeans vs OptimizedKMeans vs SklearnKMeans\n",
            "================================================================================\n",
            "\n",
            "Run 1/3\n",
            "\n",
            "Run 2/3\n",
            "\n",
            "Run 3/3\n",
            "\n",
            "Benchmark Results:\n",
            "==================\n",
            "Average execution time (BottomUpKMeans): 0.392 seconds\n",
            "Average execution time (OptimizedKMeans): 0.021 seconds\n",
            "Average execution time (SklearnKMeans): 0.016 seconds\n",
            "\n",
            "Clustering Quality Metrics (averages):\n",
            "\n",
            "BOTTOMUP:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.141\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "OPTIMIZED:\n",
            "  Silhouette Score: 0.451\n",
            "  Calinski-Harabasz: 59195.143\n",
            "  Davies-Bouldin: 1.405\n",
            "  Inertia: 446862.594\n",
            "  Adjusted Rand: 0.601\n",
            "  Adjusted Mutual Info: 0.725\n",
            "\n",
            "SKLEARN:\n",
            "  Silhouette Score: 0.670\n",
            "  Calinski-Harabasz: 136986.333\n",
            "  Davies-Bouldin: 0.516\n",
            "  Inertia: 175270.145\n",
            "  Adjusted Rand: 0.999\n",
            "  Adjusted Mutual Info: 0.997\n",
            "\n",
            "Average Iterations:\n",
            "  BOTTOMUP: 6.0 iterations\n",
            "  OPTIMIZED: 3.3 iterations\n",
            "  SKLEARN: 2.3 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**potential**"
      ],
      "metadata": {
        "id": "hNeMAzj_rxOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c numba icc_rt\n"
      ],
      "metadata": {
        "id": "50SOapLsiCRu",
        "outputId": "62154d39-7228-43b5-eee0-6096554b6606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5c9b2837baa2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conda install -c numba icc_rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    }
  ]
}